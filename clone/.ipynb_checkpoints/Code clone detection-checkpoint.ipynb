{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T08:10:26.825080Z",
     "start_time": "2020-08-05T08:10:26.820093Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {'a': 13,\n",
    "               'b': {'datadir': '/tmp/minst',\n",
    "                     'batch_size': 1024,\n",
    "                     'num_workers': 4,\n",
    "                     'learning_rate': 0.01,\n",
    "                     'momentum': 0.5,\n",
    "                     'num_epochs': 20,\n",
    "                     'num_cores': 8,\n",
    "                     'log_steps': 20,\n",
    "                     'metrics_debug': False},\n",
    "               'c': [1, 2, 4]}\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4, separators=(',', ': '))\n",
    "#with open('result.json', 'r') as f:\n",
    "#    d = json.load(f)\n",
    "\n",
    "'''\n",
    "json.dumps: dict -> str\n",
    "json.dump: dict -> file\n",
    "json.loads: str -> dict\n",
    "json.load: file -> dict\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:19:26.407701Z",
     "start_time": "2020-08-05T11:19:26.401718Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
      "'{\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5}'\n",
      "{\n",
      "    \"a\": \"Runoob\",\n",
      "    \"b\": 7\n",
      "}\n",
      "{\n",
      "      \"a\":\"Runoob\",\n",
      "      \"b\":7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "data = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}\n",
    "print(data)\n",
    "pprint(data)\n",
    "data2 = json.dumps(data)\n",
    "pprint(data2)\n",
    "\n",
    "data2 = json.dumps({'a':'Runoob', 'b':7}, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "print(data2)\n",
    "data2 = json.dumps({'a':'Runoob', 'b':7}, sort_keys=True, indent=6, separators=(',', ':'))\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:33.306095Z",
     "start_time": "2020-09-21T06:38:29.576069Z"
    }
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramCC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.get_device_name(0) == 'GeForce GT 730':\n",
    "            device = 'cpu'\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return torch.device(device)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    x1, x2, labels = [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        x2.append(item['code_y'])\n",
    "        labels.append([item['label']])\n",
    "    return x1, x2, torch.FloatTensor(labels)\n",
    "\n",
    "def train(model, train_data, batch_size, device, epochs, epoch, optimizer):\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    while i < len(train_data):\n",
    "        train1_inputs, train2_inputs, train_labels = get_batch(train_data, i, batch_size)\n",
    "        i += len(train_labels)\n",
    "        train_labels = train_labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train1_inputs, train2_inputs)\n",
    "        #print(output.shape)\n",
    "\n",
    "        loss = loss_function(output, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # acc\n",
    "        total_loss += loss.item() * len(train_labels)\n",
    "        predicted = np.where(output.data.cpu().numpy()<0.5, 1, 0)\n",
    "        total_trues += (predicted==train_labels.cpu().numpy()).sum()\n",
    "        total_acc = total_trues / i\n",
    "\n",
    "        print('[Epoch:%3d/%3d] [data: %d/%d] Training Loss: %.4f Training Acc: %.4f%%'\n",
    "              % (epoch + 1, epochs, i, len(train_data), loss, total_acc*100))\n",
    "    return total_loss/len(train_data), total_acc\n",
    "\n",
    "def validation(model, validation_data, batch_size, device, epochs, epoch):\n",
    "    trues = []\n",
    "    total_trues = 0\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    with torch.no_grad():\n",
    "        while i < len(validation_data):\n",
    "            validation1_inputs, validation2_inputs, validation_labels = get_batch(validation_data, i, batch_size)\n",
    "            i += len(validation_labels)\n",
    "            test_labels = validation_labels.to(device)\n",
    "\n",
    "            model.batch_size = len(validation_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(validation1_inputs, validation2_inputs)\n",
    "\n",
    "            loss = loss_function(output, validation_labels)\n",
    "\n",
    "            # calc testing acc\n",
    "            predicted = (output.data < 0.5).cpu().numpy()\n",
    "            trues_ex = test_labels.cpu().numpy()\n",
    "            trues.extend(trues_ex)\n",
    "            total_trues += np.where(predicted==trues_ex, 1, 0).sum()\n",
    "            total_loss += loss.item() * len(validation_labels)\n",
    "    \n",
    "    total_loss /= len(validation_data)\n",
    "    total_acc = total_trues / len(validation_data)\n",
    "    print('[Epoch:%3d/%3d] Validation Loss: %.4f Validation Acc: %.4f%%'\n",
    "              % (epoch + 1, epochs, total_loss, total_acc*100))\n",
    "    return total_loss, total_acc\n",
    "\n",
    "def test(model, test_data, batch_size, device):\n",
    "    global precision, recall, f1\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_trues = 0\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    with torch.no_grad():\n",
    "        while i < len(test_data):\n",
    "            test1_inputs, test2_inputs, test_labels = get_batch(test_data, i, batch_size)\n",
    "            i += len(test_labels)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test1_inputs, test2_inputs)\n",
    "            loss = loss_function(output, test_labels)\n",
    "\n",
    "            # calc testing acc\n",
    "            predicted = (output.data < 0.5).cpu().numpy()\n",
    "            predicts.extend(predicted)\n",
    "            trues_ex = test_labels.cpu().numpy()\n",
    "            trues.extend(trues_ex)\n",
    "            total_trues += np.where(predicted==trues_ex, 1, 0).sum()\n",
    "            total_loss += loss.item() * len(test_labels)\n",
    "\n",
    "    total_acc = total_trues / len(test_data)\n",
    "    \n",
    "    if lang == 'java':\n",
    "        weights = [0, 0.005, 0.001, 0.002, 0.010, 0.982]\n",
    "        p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "        precision += weights[t] * p\n",
    "        recall += weights[t] * r\n",
    "        f1 += weights[t] * f\n",
    "        print(\"Type-\" + str(t) + \": \" + str(p) + \" \" + str(r) + \" \" + str(f))\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "        print(\"Testing results(P,R,F1):%.3f, %.3f, %.3f\" \n",
    "              % (precision, recall, f1))\n",
    "        \n",
    "    return total_loss, total_acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T15:37:12.074999Z",
     "start_time": "2020-08-12T15:37:06.344788Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "word2vec = Word2Vec.load(root+lang+\"/train/embedding/node_w2v_128\").wv\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:44:43.344345Z",
     "start_time": "2020-09-08T13:44:22.055733Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "#model= Word2Vec(min_count=4, size=200, workers=6, max_final_vocab=1000000)\n",
    "#model.load('./word2vec_Model/word2vec')\n",
    "word2vec = Word2Vec.load('./word2vec_Model/word2vec').wv\n",
    "\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:38.862354Z",
     "start_time": "2020-09-21T06:38:33.307090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "word2vec = Word2Vec.load(root+lang+\"/train/embedding/node_w2v_128\").wv\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:38.869245Z",
     "start_time": "2020-09-21T06:38:38.864228Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, enclidean_distance, label):\n",
    "        #enclidean_distance = F.pairwise_distance(output[0], output[1])\n",
    "        loss_contrastive = torch.mean(label*torch.pow(enclidean_distance, 2) +\n",
    "                                     (1-label) * torch.pow(torch.clamp(self.margin - enclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-21T06:38:29.564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[Epoch:  1/  2] [data: 128/21362] Training Loss: 0.1335 Training Acc: 84.3750%\n",
      "[Epoch:  1/  2] [data: 256/21362] Training Loss: 0.0271 Training Acc: 92.1875%\n",
      "[Epoch:  1/  2] [data: 384/21362] Training Loss: 0.0038 Training Acc: 94.7917%\n",
      "[Epoch:  1/  2] [data: 512/21362] Training Loss: 0.0006 Training Acc: 96.0938%\n",
      "[Epoch:  1/  2] [data: 640/21362] Training Loss: 0.0037 Training Acc: 96.8750%\n",
      "[Epoch:  1/  2] [data: 768/21362] Training Loss: 0.0037 Training Acc: 97.3958%\n",
      "[Epoch:  1/  2] [data: 896/21362] Training Loss: 0.0002 Training Acc: 97.7679%\n",
      "[Epoch:  1/  2] [data: 1024/21362] Training Loss: 0.0001 Training Acc: 98.0469%\n",
      "[Epoch:  1/  2] [data: 1152/21362] Training Loss: 0.0001 Training Acc: 98.2639%\n",
      "[Epoch:  1/  2] [data: 1280/21362] Training Loss: 0.0000 Training Acc: 98.4375%\n",
      "[Epoch:  1/  2] [data: 1408/21362] Training Loss: 0.0001 Training Acc: 98.5795%\n",
      "[Epoch:  1/  2] [data: 1536/21362] Training Loss: 0.0027 Training Acc: 98.6979%\n",
      "[Epoch:  1/  2] [data: 1664/21362] Training Loss: 0.0001 Training Acc: 98.7981%\n",
      "[Epoch:  1/  2] [data: 1792/21362] Training Loss: 0.0004 Training Acc: 98.8839%\n",
      "[Epoch:  1/  2] [data: 1920/21362] Training Loss: 0.0002 Training Acc: 98.9583%\n",
      "[Epoch:  1/  2] [data: 2048/21362] Training Loss: 0.0001 Training Acc: 99.0234%\n",
      "[Epoch:  1/  2] [data: 2176/21362] Training Loss: 0.0001 Training Acc: 99.0809%\n",
      "[Epoch:  1/  2] [data: 2304/21362] Training Loss: 0.0000 Training Acc: 99.1319%\n",
      "[Epoch:  1/  2] [data: 2432/21362] Training Loss: 0.0000 Training Acc: 99.1776%\n",
      "[Epoch:  1/  2] [data: 2560/21362] Training Loss: 0.0001 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 2688/21362] Training Loss: 0.0000 Training Acc: 99.2560%\n",
      "[Epoch:  1/  2] [data: 2816/21362] Training Loss: 0.0004 Training Acc: 99.2898%\n",
      "[Epoch:  1/  2] [data: 2944/21362] Training Loss: 0.0000 Training Acc: 99.3207%\n",
      "[Epoch:  1/  2] [data: 3072/21362] Training Loss: 0.0002 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 3200/21362] Training Loss: 0.0005 Training Acc: 99.3750%\n",
      "[Epoch:  1/  2] [data: 3328/21362] Training Loss: 0.0003 Training Acc: 99.3990%\n",
      "[Epoch:  1/  2] [data: 3456/21362] Training Loss: 0.0006 Training Acc: 99.4213%\n",
      "[Epoch:  1/  2] [data: 3584/21362] Training Loss: 0.0000 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 3712/21362] Training Loss: 0.0000 Training Acc: 99.4612%\n",
      "[Epoch:  1/  2] [data: 3840/21362] Training Loss: 0.0001 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 3968/21362] Training Loss: 0.0000 Training Acc: 99.4960%\n",
      "[Epoch:  1/  2] [data: 4096/21362] Training Loss: 0.0000 Training Acc: 99.5117%\n",
      "[Epoch:  1/  2] [data: 4224/21362] Training Loss: 0.0000 Training Acc: 99.5265%\n",
      "[Epoch:  1/  2] [data: 4352/21362] Training Loss: 0.0000 Training Acc: 99.5404%\n",
      "[Epoch:  1/  2] [data: 4480/21362] Training Loss: 0.0003 Training Acc: 99.5536%\n",
      "[Epoch:  1/  2] [data: 4608/21362] Training Loss: 0.0001 Training Acc: 99.5660%\n",
      "[Epoch:  1/  2] [data: 4736/21362] Training Loss: 0.0000 Training Acc: 99.5777%\n",
      "[Epoch:  1/  2] [data: 4864/21362] Training Loss: 0.0000 Training Acc: 99.5888%\n",
      "[Epoch:  1/  2] [data: 4992/21362] Training Loss: 0.0001 Training Acc: 99.5994%\n",
      "[Epoch:  1/  2] [data: 5120/21362] Training Loss: 0.0003 Training Acc: 99.6094%\n",
      "[Epoch:  1/  2] [data: 5248/21362] Training Loss: 0.0001 Training Acc: 99.6189%\n",
      "[Epoch:  1/  2] [data: 5376/21362] Training Loss: 0.0001 Training Acc: 99.6280%\n",
      "[Epoch:  1/  2] [data: 5504/21362] Training Loss: 0.0003 Training Acc: 99.6366%\n",
      "[Epoch:  1/  2] [data: 5632/21362] Training Loss: 0.0001 Training Acc: 99.6449%\n",
      "[Epoch:  1/  2] [data: 5760/21362] Training Loss: 0.0000 Training Acc: 99.6528%\n",
      "[Epoch:  1/  2] [data: 5888/21362] Training Loss: 0.0000 Training Acc: 99.6603%\n",
      "[Epoch:  1/  2] [data: 6016/21362] Training Loss: 0.0000 Training Acc: 99.6676%\n",
      "[Epoch:  1/  2] [data: 6144/21362] Training Loss: 0.0000 Training Acc: 99.6745%\n",
      "[Epoch:  1/  2] [data: 6272/21362] Training Loss: 0.0000 Training Acc: 99.6811%\n",
      "[Epoch:  1/  2] [data: 6400/21362] Training Loss: 0.0000 Training Acc: 99.6875%\n",
      "[Epoch:  1/  2] [data: 6528/21362] Training Loss: 0.0000 Training Acc: 99.6936%\n",
      "[Epoch:  1/  2] [data: 6656/21362] Training Loss: 0.0000 Training Acc: 99.6995%\n",
      "[Epoch:  1/  2] [data: 6784/21362] Training Loss: 0.0000 Training Acc: 99.7052%\n",
      "[Epoch:  1/  2] [data: 6912/21362] Training Loss: 0.0000 Training Acc: 99.7106%\n",
      "[Epoch:  1/  2] [data: 7040/21362] Training Loss: 0.0000 Training Acc: 99.7159%\n",
      "[Epoch:  1/  2] [data: 7168/21362] Training Loss: 0.0000 Training Acc: 99.7210%\n",
      "[Epoch:  1/  2] [data: 7296/21362] Training Loss: 0.0000 Training Acc: 99.7259%\n",
      "[Epoch:  1/  2] [data: 7424/21362] Training Loss: 0.0000 Training Acc: 99.7306%\n",
      "[Epoch:  1/  2] [data: 7552/21362] Training Loss: 0.0000 Training Acc: 99.7352%\n",
      "[Epoch:  1/  2] [data: 7680/21362] Training Loss: 0.0000 Training Acc: 99.7396%\n",
      "[Epoch:  1/  2] [data: 7808/21362] Training Loss: 0.0000 Training Acc: 99.7439%\n",
      "[Epoch:  1/  2] [data: 7936/21362] Training Loss: 0.0001 Training Acc: 99.7480%\n",
      "[Epoch:  1/  2] [data: 8064/21362] Training Loss: 0.0000 Training Acc: 99.7520%\n",
      "[Epoch:  1/  2] [data: 8192/21362] Training Loss: 0.0000 Training Acc: 99.7559%\n",
      "[Epoch:  1/  2] [data: 8320/21362] Training Loss: 0.0000 Training Acc: 99.7596%\n",
      "[Epoch:  1/  2] [data: 8448/21362] Training Loss: 0.0000 Training Acc: 99.7633%\n",
      "[Epoch:  1/  2] [data: 8576/21362] Training Loss: 0.0000 Training Acc: 99.7668%\n",
      "[Epoch:  1/  2] [data: 8704/21362] Training Loss: 0.0000 Training Acc: 99.7702%\n",
      "[Epoch:  1/  2] [data: 8832/21362] Training Loss: 0.0000 Training Acc: 99.7736%\n",
      "[Epoch:  1/  2] [data: 8960/21362] Training Loss: 0.0000 Training Acc: 99.7768%\n",
      "[Epoch:  1/  2] [data: 9088/21362] Training Loss: 0.0000 Training Acc: 99.7799%\n",
      "[Epoch:  1/  2] [data: 9216/21362] Training Loss: 0.0000 Training Acc: 99.7830%\n",
      "[Epoch:  1/  2] [data: 9344/21362] Training Loss: 0.0000 Training Acc: 99.7860%\n",
      "[Epoch:  1/  2] [data: 9472/21362] Training Loss: 0.0000 Training Acc: 99.7889%\n",
      "[Epoch:  1/  2] [data: 9600/21362] Training Loss: 0.0000 Training Acc: 99.7917%\n",
      "[Epoch:  1/  2] [data: 9728/21362] Training Loss: 0.0000 Training Acc: 99.7944%\n",
      "[Epoch:  1/  2] [data: 9856/21362] Training Loss: 0.0000 Training Acc: 99.7971%\n",
      "[Epoch:  1/  2] [data: 9984/21362] Training Loss: 0.0000 Training Acc: 99.7997%\n",
      "[Epoch:  1/  2] [data: 10112/21362] Training Loss: 0.0000 Training Acc: 99.8022%\n",
      "[Epoch:  1/  2] [data: 10240/21362] Training Loss: 0.0000 Training Acc: 99.8047%\n",
      "[Epoch:  1/  2] [data: 10368/21362] Training Loss: 0.0000 Training Acc: 99.8071%\n",
      "[Epoch:  1/  2] [data: 10496/21362] Training Loss: 0.0053 Training Acc: 99.8095%\n",
      "[Epoch:  1/  2] [data: 10624/21362] Training Loss: 0.0000 Training Acc: 99.8117%\n",
      "[Epoch:  1/  2] [data: 10752/21362] Training Loss: 0.0000 Training Acc: 99.8140%\n",
      "[Epoch:  1/  2] [data: 10880/21362] Training Loss: 0.0000 Training Acc: 99.8162%\n",
      "[Epoch:  1/  2] [data: 11008/21362] Training Loss: 0.0000 Training Acc: 99.8183%\n",
      "[Epoch:  1/  2] [data: 11136/21362] Training Loss: 0.0001 Training Acc: 99.8204%\n",
      "[Epoch:  1/  2] [data: 11264/21362] Training Loss: 0.0000 Training Acc: 99.8224%\n",
      "[Epoch:  1/  2] [data: 11392/21362] Training Loss: 0.0000 Training Acc: 99.8244%\n",
      "[Epoch:  1/  2] [data: 11520/21362] Training Loss: 0.0000 Training Acc: 99.8264%\n",
      "[Epoch:  1/  2] [data: 11648/21362] Training Loss: 0.0000 Training Acc: 99.8283%\n",
      "[Epoch:  1/  2] [data: 11776/21362] Training Loss: 0.0000 Training Acc: 99.8302%\n",
      "[Epoch:  1/  2] [data: 11904/21362] Training Loss: 0.0000 Training Acc: 99.8320%\n",
      "[Epoch:  1/  2] [data: 12032/21362] Training Loss: 0.0000 Training Acc: 99.8338%\n",
      "[Epoch:  1/  2] [data: 12160/21362] Training Loss: 0.0000 Training Acc: 99.8355%\n",
      "[Epoch:  1/  2] [data: 12288/21362] Training Loss: 0.0000 Training Acc: 99.8372%\n",
      "[Epoch:  1/  2] [data: 12416/21362] Training Loss: 0.0000 Training Acc: 99.8389%\n",
      "[Epoch:  1/  2] [data: 12544/21362] Training Loss: 0.0000 Training Acc: 99.8406%\n",
      "[Epoch:  1/  2] [data: 12672/21362] Training Loss: 0.0000 Training Acc: 99.8422%\n",
      "[Epoch:  1/  2] [data: 12800/21362] Training Loss: 0.0000 Training Acc: 99.8438%\n",
      "[Epoch:  1/  2] [data: 12928/21362] Training Loss: 0.0002 Training Acc: 99.8453%\n",
      "[Epoch:  1/  2] [data: 13056/21362] Training Loss: 0.0000 Training Acc: 99.8468%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 13184/21362] Training Loss: 0.0000 Training Acc: 99.8483%\n",
      "[Epoch:  1/  2] [data: 13312/21362] Training Loss: 0.0000 Training Acc: 99.8498%\n",
      "[Epoch:  1/  2] [data: 13440/21362] Training Loss: 0.0000 Training Acc: 99.8512%\n",
      "[Epoch:  1/  2] [data: 13568/21362] Training Loss: 0.0000 Training Acc: 99.8526%\n",
      "[Epoch:  1/  2] [data: 13696/21362] Training Loss: 0.0000 Training Acc: 99.8540%\n",
      "[Epoch:  1/  2] [data: 13824/21362] Training Loss: 0.0000 Training Acc: 99.8553%\n",
      "[Epoch:  1/  2] [data: 13952/21362] Training Loss: 0.0000 Training Acc: 99.8567%\n",
      "[Epoch:  1/  2] [data: 14080/21362] Training Loss: 0.0000 Training Acc: 99.8580%\n",
      "[Epoch:  1/  2] [data: 14208/21362] Training Loss: 0.0006 Training Acc: 99.8592%\n",
      "[Epoch:  1/  2] [data: 14336/21362] Training Loss: 0.0000 Training Acc: 99.8605%\n",
      "[Epoch:  1/  2] [data: 14464/21362] Training Loss: 0.0000 Training Acc: 99.8617%\n",
      "[Epoch:  1/  2] [data: 14592/21362] Training Loss: 0.0000 Training Acc: 99.8629%\n",
      "[Epoch:  1/  2] [data: 14720/21362] Training Loss: 0.0000 Training Acc: 99.8641%\n",
      "[Epoch:  1/  2] [data: 14848/21362] Training Loss: 0.0000 Training Acc: 99.8653%\n",
      "[Epoch:  1/  2] [data: 14976/21362] Training Loss: 0.0000 Training Acc: 99.8665%\n",
      "[Epoch:  1/  2] [data: 15104/21362] Training Loss: 0.0000 Training Acc: 99.8676%\n",
      "[Epoch:  1/  2] [data: 15232/21362] Training Loss: 0.0095 Training Acc: 99.8687%\n",
      "[Epoch:  1/  2] [data: 15360/21362] Training Loss: 0.0000 Training Acc: 99.8698%\n",
      "[Epoch:  1/  2] [data: 15488/21362] Training Loss: 0.0007 Training Acc: 99.8709%\n",
      "[Epoch:  1/  2] [data: 15616/21362] Training Loss: 0.0000 Training Acc: 99.8719%\n",
      "[Epoch:  1/  2] [data: 15744/21362] Training Loss: 0.0000 Training Acc: 99.8730%\n",
      "[Epoch:  1/  2] [data: 15872/21362] Training Loss: 0.0040 Training Acc: 99.8740%\n",
      "[Epoch:  1/  2] [data: 16000/21362] Training Loss: 0.0000 Training Acc: 99.8750%\n",
      "[Epoch:  1/  2] [data: 16128/21362] Training Loss: 0.0000 Training Acc: 99.8760%\n",
      "[Epoch:  1/  2] [data: 16256/21362] Training Loss: 0.0000 Training Acc: 99.8770%\n",
      "[Epoch:  1/  2] [data: 16384/21362] Training Loss: 0.0000 Training Acc: 99.8779%\n",
      "[Epoch:  1/  2] [data: 16512/21362] Training Loss: 0.0000 Training Acc: 99.8789%\n",
      "[Epoch:  1/  2] [data: 16640/21362] Training Loss: 0.0000 Training Acc: 99.8798%\n",
      "[Epoch:  1/  2] [data: 16768/21362] Training Loss: 0.0000 Training Acc: 99.8807%\n",
      "[Epoch:  1/  2] [data: 16896/21362] Training Loss: 0.0000 Training Acc: 99.8816%\n",
      "[Epoch:  1/  2] [data: 17024/21362] Training Loss: 0.0000 Training Acc: 99.8825%\n",
      "[Epoch:  1/  2] [data: 17152/21362] Training Loss: 0.0000 Training Acc: 99.8834%\n",
      "[Epoch:  1/  2] [data: 17280/21362] Training Loss: 0.0000 Training Acc: 99.8843%\n",
      "[Epoch:  1/  2] [data: 17408/21362] Training Loss: 0.0000 Training Acc: 99.8851%\n",
      "[Epoch:  1/  2] [data: 17536/21362] Training Loss: 0.0000 Training Acc: 99.8859%\n",
      "[Epoch:  1/  2] [data: 17664/21362] Training Loss: 0.0000 Training Acc: 99.8868%\n",
      "[Epoch:  1/  2] [data: 17792/21362] Training Loss: 0.0000 Training Acc: 99.8876%\n",
      "[Epoch:  1/  2] [data: 17920/21362] Training Loss: 0.0000 Training Acc: 99.8884%\n",
      "[Epoch:  1/  2] [data: 18048/21362] Training Loss: 0.0000 Training Acc: 99.8892%\n",
      "[Epoch:  1/  2] [data: 18176/21362] Training Loss: 0.0002 Training Acc: 99.8900%\n",
      "[Epoch:  1/  2] [data: 18304/21362] Training Loss: 0.0001 Training Acc: 99.8907%\n",
      "[Epoch:  1/  2] [data: 18432/21362] Training Loss: 0.0000 Training Acc: 99.8915%\n",
      "[Epoch:  1/  2] [data: 18560/21362] Training Loss: 0.0000 Training Acc: 99.8922%\n",
      "[Epoch:  1/  2] [data: 18688/21362] Training Loss: 0.0000 Training Acc: 99.8930%\n",
      "[Epoch:  1/  2] [data: 18816/21362] Training Loss: 0.0000 Training Acc: 99.8937%\n",
      "[Epoch:  1/  2] [data: 18944/21362] Training Loss: 0.0000 Training Acc: 99.8944%\n",
      "[Epoch:  1/  2] [data: 19072/21362] Training Loss: 0.0000 Training Acc: 99.8951%\n",
      "[Epoch:  1/  2] [data: 19200/21362] Training Loss: 0.0008 Training Acc: 99.8958%\n",
      "[Epoch:  1/  2] [data: 19328/21362] Training Loss: 0.0000 Training Acc: 99.8965%\n",
      "[Epoch:  1/  2] [data: 19456/21362] Training Loss: 0.0000 Training Acc: 99.8972%\n",
      "[Epoch:  1/  2] [data: 19584/21362] Training Loss: 0.0000 Training Acc: 99.8979%\n",
      "[Epoch:  1/  2] [data: 19712/21362] Training Loss: 0.0000 Training Acc: 99.8985%\n",
      "[Epoch:  1/  2] [data: 19840/21362] Training Loss: 0.0000 Training Acc: 99.8992%\n",
      "[Epoch:  1/  2] [data: 19968/21362] Training Loss: 0.0000 Training Acc: 99.8998%\n",
      "[Epoch:  1/  2] [data: 20096/21362] Training Loss: 0.0000 Training Acc: 99.9005%\n",
      "[Epoch:  1/  2] [data: 20224/21362] Training Loss: 0.0000 Training Acc: 99.9011%\n",
      "[Epoch:  1/  2] [data: 20352/21362] Training Loss: 0.0000 Training Acc: 99.9017%\n",
      "[Epoch:  1/  2] [data: 20480/21362] Training Loss: 0.0000 Training Acc: 99.9023%\n",
      "[Epoch:  1/  2] [data: 20608/21362] Training Loss: 0.0000 Training Acc: 99.9030%\n",
      "[Epoch:  1/  2] [data: 20736/21362] Training Loss: 0.0000 Training Acc: 99.9035%\n",
      "[Epoch:  1/  2] [data: 20864/21362] Training Loss: 0.0000 Training Acc: 99.9041%\n",
      "[Epoch:  1/  2] [data: 20992/21362] Training Loss: 0.0000 Training Acc: 99.9047%\n",
      "[Epoch:  1/  2] [data: 21120/21362] Training Loss: 0.0000 Training Acc: 99.9053%\n",
      "[Epoch:  1/  2] [data: 21248/21362] Training Loss: 0.0000 Training Acc: 99.9059%\n",
      "[Epoch:  1/  2] [data: 21362/21362] Training Loss: 0.0000 Training Acc: 99.9064%\n",
      "Testing-1...\n",
      "[Epoch:  1/  2] Validation Loss: 0.0000 Validation Acc: 100.0000%\n",
      "Time used: 1251.2459571361542s\n",
      "[Epoch:  2/  2] [data: 128/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 256/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 384/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 512/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 640/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 768/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 896/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1024/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1152/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1280/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1408/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1536/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1664/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1792/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1920/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2048/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2176/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2304/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2432/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2560/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2688/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2816/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2944/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3072/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3200/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3328/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3456/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3584/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3712/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3840/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3968/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4096/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4224/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4352/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4480/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 4608/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4736/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4864/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4992/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5120/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5248/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5376/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5504/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5632/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5760/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5888/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6016/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6144/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6272/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6400/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6528/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6656/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6784/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6912/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7040/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7168/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7296/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7424/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7552/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7680/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7808/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7936/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8064/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8192/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8320/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8448/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8576/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8704/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8832/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8960/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9088/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9216/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9344/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9472/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9600/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9728/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9856/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9984/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10112/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10240/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10368/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10496/21362] Training Loss: 0.0014 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10624/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10752/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10880/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11008/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11136/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11264/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11392/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11520/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11648/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11776/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11904/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12032/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12160/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12288/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12416/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12544/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12672/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12800/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12928/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13056/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13184/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13312/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13440/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13568/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13696/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13824/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13952/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14080/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14208/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14336/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14464/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14592/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14720/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14848/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14976/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15104/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15232/21362] Training Loss: 0.0066 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15360/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15488/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15616/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15744/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15872/21362] Training Loss: 0.0008 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16000/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16128/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16256/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16384/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16512/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16640/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16768/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16896/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17024/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17152/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17280/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17408/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 17536/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17664/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17792/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17920/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18048/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18176/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18304/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18432/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18560/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18688/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18816/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18944/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19072/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19200/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19328/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19456/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19584/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19712/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19840/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19968/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20096/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20224/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20352/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20480/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20608/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20736/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20864/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20992/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21120/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21248/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21362/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "Testing-1...\n",
      "[Epoch:  2/  2] Validation Loss: 0.0000 Validation Acc: 100.0000%\n",
      "Time used: 1307.4331023693085s\n",
      "Type-1: 1.0 1.0 1.0\n",
      "[Epoch:  1/  2] [data: 128/14329] Training Loss: 0.0373 Training Acc: 98.4375%\n",
      "[Epoch:  1/  2] [data: 256/14329] Training Loss: 0.0165 Training Acc: 98.8281%\n",
      "[Epoch:  1/  2] [data: 384/14329] Training Loss: 0.0142 Training Acc: 98.9583%\n",
      "[Epoch:  1/  2] [data: 512/14329] Training Loss: 0.0069 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 640/14329] Training Loss: 0.0122 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 768/14329] Training Loss: 0.0028 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 896/14329] Training Loss: 0.0004 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 1024/14329] Training Loss: 0.0029 Training Acc: 99.5117%\n",
      "[Epoch:  1/  2] [data: 1152/14329] Training Loss: 0.0222 Training Acc: 99.3056%\n",
      "[Epoch:  1/  2] [data: 1280/14329] Training Loss: 0.0117 Training Acc: 99.2969%\n",
      "[Epoch:  1/  2] [data: 1408/14329] Training Loss: 0.0044 Training Acc: 99.3608%\n",
      "[Epoch:  1/  2] [data: 1536/14329] Training Loss: 0.0110 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 1664/14329] Training Loss: 0.0027 Training Acc: 99.3990%\n",
      "[Epoch:  1/  2] [data: 1792/14329] Training Loss: 0.0115 Training Acc: 99.3862%\n",
      "[Epoch:  1/  2] [data: 1920/14329] Training Loss: 0.0073 Training Acc: 99.3750%\n",
      "[Epoch:  1/  2] [data: 2048/14329] Training Loss: 0.0060 Training Acc: 99.3652%\n",
      "[Epoch:  1/  2] [data: 2176/14329] Training Loss: 0.0032 Training Acc: 99.4026%\n",
      "[Epoch:  1/  2] [data: 2304/14329] Training Loss: 0.0066 Training Acc: 99.3924%\n",
      "[Epoch:  1/  2] [data: 2432/14329] Training Loss: 0.0140 Training Acc: 99.3832%\n",
      "[Epoch:  1/  2] [data: 2560/14329] Training Loss: 0.0086 Training Acc: 99.3359%\n",
      "[Epoch:  1/  2] [data: 2688/14329] Training Loss: 0.0016 Training Acc: 99.3676%\n",
      "[Epoch:  1/  2] [data: 2816/14329] Training Loss: 0.0014 Training Acc: 99.3963%\n",
      "[Epoch:  1/  2] [data: 2944/14329] Training Loss: 0.0053 Training Acc: 99.4226%\n",
      "[Epoch:  1/  2] [data: 3072/14329] Training Loss: 0.0013 Training Acc: 99.4466%\n",
      "[Epoch:  1/  2] [data: 3200/14329] Training Loss: 0.0049 Training Acc: 99.4375%\n",
      "[Epoch:  1/  2] [data: 3328/14329] Training Loss: 0.0049 Training Acc: 99.4291%\n",
      "[Epoch:  1/  2] [data: 3456/14329] Training Loss: 0.0057 Training Acc: 99.4213%\n",
      "[Epoch:  1/  2] [data: 3584/14329] Training Loss: 0.0018 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 3712/14329] Training Loss: 0.0020 Training Acc: 99.4612%\n",
      "[Epoch:  1/  2] [data: 3840/14329] Training Loss: 0.0017 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 3968/14329] Training Loss: 0.0012 Training Acc: 99.4960%\n",
      "[Epoch:  1/  2] [data: 4096/14329] Training Loss: 0.0170 Training Acc: 99.4629%\n",
      "[Epoch:  1/  2] [data: 4224/14329] Training Loss: 0.0006 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 4352/14329] Training Loss: 0.0024 Training Acc: 99.4715%\n",
      "[Epoch:  1/  2] [data: 4480/14329] Training Loss: 0.0097 Training Acc: 99.4196%\n",
      "[Epoch:  1/  2] [data: 4608/14329] Training Loss: 0.0014 Training Acc: 99.4358%\n",
      "[Epoch:  1/  2] [data: 4736/14329] Training Loss: 0.0008 Training Acc: 99.4510%\n",
      "[Epoch:  1/  2] [data: 4864/14329] Training Loss: 0.0018 Training Acc: 99.4655%\n",
      "[Epoch:  1/  2] [data: 4992/14329] Training Loss: 0.0004 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 5120/14329] Training Loss: 0.0082 Training Acc: 99.4727%\n",
      "[Epoch:  1/  2] [data: 5248/14329] Training Loss: 0.0107 Training Acc: 99.4474%\n",
      "[Epoch:  1/  2] [data: 5376/14329] Training Loss: 0.0019 Training Acc: 99.4606%\n",
      "[Epoch:  1/  2] [data: 5504/14329] Training Loss: 0.0087 Training Acc: 99.4368%\n",
      "[Epoch:  1/  2] [data: 5632/14329] Training Loss: 0.0004 Training Acc: 99.4496%\n",
      "[Epoch:  1/  2] [data: 5760/14329] Training Loss: 0.0008 Training Acc: 99.4618%\n",
      "[Epoch:  1/  2] [data: 5888/14329] Training Loss: 0.0108 Training Acc: 99.4395%\n",
      "[Epoch:  1/  2] [data: 6016/14329] Training Loss: 0.0090 Training Acc: 99.4348%\n",
      "[Epoch:  1/  2] [data: 6144/14329] Training Loss: 0.0035 Training Acc: 99.4466%\n",
      "[Epoch:  1/  2] [data: 6272/14329] Training Loss: 0.0024 Training Acc: 99.4579%\n",
      "[Epoch:  1/  2] [data: 6400/14329] Training Loss: 0.0006 Training Acc: 99.4688%\n",
      "[Epoch:  1/  2] [data: 6528/14329] Training Loss: 0.0041 Training Acc: 99.4638%\n",
      "[Epoch:  1/  2] [data: 6656/14329] Training Loss: 0.0052 Training Acc: 99.4742%\n",
      "[Epoch:  1/  2] [data: 6784/14329] Training Loss: 0.0049 Training Acc: 99.4546%\n",
      "[Epoch:  1/  2] [data: 6912/14329] Training Loss: 0.0028 Training Acc: 99.4647%\n",
      "[Epoch:  1/  2] [data: 7040/14329] Training Loss: 0.0030 Training Acc: 99.4744%\n",
      "[Epoch:  1/  2] [data: 7168/14329] Training Loss: 0.0001 Training Acc: 99.4838%\n",
      "[Epoch:  1/  2] [data: 7296/14329] Training Loss: 0.0033 Training Acc: 99.4929%\n",
      "[Epoch:  1/  2] [data: 7424/14329] Training Loss: 0.0047 Training Acc: 99.5016%\n",
      "[Epoch:  1/  2] [data: 7552/14329] Training Loss: 0.0019 Training Acc: 99.5101%\n",
      "[Epoch:  1/  2] [data: 7680/14329] Training Loss: 0.0013 Training Acc: 99.5182%\n",
      "[Epoch:  1/  2] [data: 7808/14329] Training Loss: 0.0044 Training Acc: 99.5133%\n",
      "[Epoch:  1/  2] [data: 7936/14329] Training Loss: 0.0061 Training Acc: 99.5086%\n",
      "[Epoch:  1/  2] [data: 8064/14329] Training Loss: 0.0012 Training Acc: 99.5164%\n",
      "[Epoch:  1/  2] [data: 8192/14329] Training Loss: 0.0018 Training Acc: 99.5239%\n",
      "[Epoch:  1/  2] [data: 8320/14329] Training Loss: 0.0011 Training Acc: 99.5312%\n",
      "[Epoch:  1/  2] [data: 8448/14329] Training Loss: 0.0030 Training Acc: 99.5384%\n",
      "[Epoch:  1/  2] [data: 8576/14329] Training Loss: 0.0041 Training Acc: 99.5336%\n",
      "[Epoch:  1/  2] [data: 8704/14329] Training Loss: 0.0015 Training Acc: 99.5404%\n",
      "[Epoch:  1/  2] [data: 8832/14329] Training Loss: 0.0051 Training Acc: 99.5358%\n",
      "[Epoch:  1/  2] [data: 8960/14329] Training Loss: 0.0012 Training Acc: 99.5424%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 9088/14329] Training Loss: 0.0025 Training Acc: 99.5489%\n",
      "[Epoch:  1/  2] [data: 9216/14329] Training Loss: 0.0007 Training Acc: 99.5551%\n",
      "[Epoch:  1/  2] [data: 9344/14329] Training Loss: 0.0006 Training Acc: 99.5612%\n",
      "[Epoch:  1/  2] [data: 9472/14329] Training Loss: 0.0075 Training Acc: 99.5566%\n",
      "[Epoch:  1/  2] [data: 9600/14329] Training Loss: 0.0006 Training Acc: 99.5625%\n",
      "[Epoch:  1/  2] [data: 9728/14329] Training Loss: 0.0031 Training Acc: 99.5683%\n",
      "[Epoch:  1/  2] [data: 9856/14329] Training Loss: 0.0014 Training Acc: 99.5739%\n",
      "[Epoch:  1/  2] [data: 9984/14329] Training Loss: 0.0048 Training Acc: 99.5693%\n",
      "[Epoch:  1/  2] [data: 10112/14329] Training Loss: 0.0016 Training Acc: 99.5748%\n",
      "[Epoch:  1/  2] [data: 10240/14329] Training Loss: 0.0104 Training Acc: 99.5801%\n",
      "[Epoch:  1/  2] [data: 10368/14329] Training Loss: 0.0049 Training Acc: 99.5756%\n",
      "[Epoch:  1/  2] [data: 10496/14329] Training Loss: 0.0016 Training Acc: 99.5808%\n",
      "[Epoch:  1/  2] [data: 10624/14329] Training Loss: 0.0077 Training Acc: 99.5764%\n",
      "[Epoch:  1/  2] [data: 10752/14329] Training Loss: 0.0029 Training Acc: 99.5815%\n",
      "[Epoch:  1/  2] [data: 10880/14329] Training Loss: 0.0029 Training Acc: 99.5864%\n",
      "[Epoch:  1/  2] [data: 11008/14329] Training Loss: 0.0030 Training Acc: 99.5912%\n",
      "[Epoch:  1/  2] [data: 11136/14329] Training Loss: 0.0010 Training Acc: 99.5959%\n",
      "[Epoch:  1/  2] [data: 11264/14329] Training Loss: 0.0008 Training Acc: 99.6005%\n",
      "[Epoch:  1/  2] [data: 11392/14329] Training Loss: 0.0001 Training Acc: 99.6050%\n",
      "[Epoch:  1/  2] [data: 11520/14329] Training Loss: 0.0003 Training Acc: 99.6094%\n",
      "[Epoch:  1/  2] [data: 11648/14329] Training Loss: 0.0000 Training Acc: 99.6137%\n",
      "[Epoch:  1/  2] [data: 11776/14329] Training Loss: 0.0009 Training Acc: 99.6179%\n",
      "[Epoch:  1/  2] [data: 11904/14329] Training Loss: 0.0001 Training Acc: 99.6220%\n",
      "[Epoch:  1/  2] [data: 12032/14329] Training Loss: 0.0008 Training Acc: 99.6260%\n",
      "[Epoch:  1/  2] [data: 12160/14329] Training Loss: 0.0039 Training Acc: 99.6217%\n",
      "[Epoch:  1/  2] [data: 12288/14329] Training Loss: 0.0029 Training Acc: 99.6257%\n",
      "[Epoch:  1/  2] [data: 12416/14329] Training Loss: 0.0022 Training Acc: 99.6295%\n",
      "[Epoch:  1/  2] [data: 12544/14329] Training Loss: 0.0015 Training Acc: 99.6333%\n",
      "[Epoch:  1/  2] [data: 12672/14329] Training Loss: 0.0006 Training Acc: 99.6370%\n",
      "[Epoch:  1/  2] [data: 12800/14329] Training Loss: 0.0030 Training Acc: 99.6328%\n",
      "[Epoch:  1/  2] [data: 12928/14329] Training Loss: 0.0034 Training Acc: 99.6364%\n",
      "[Epoch:  1/  2] [data: 13056/14329] Training Loss: 0.0072 Training Acc: 99.6324%\n",
      "[Epoch:  1/  2] [data: 13184/14329] Training Loss: 0.0065 Training Acc: 99.6283%\n",
      "[Epoch:  1/  2] [data: 13312/14329] Training Loss: 0.0041 Training Acc: 99.6244%\n",
      "[Epoch:  1/  2] [data: 13440/14329] Training Loss: 0.0008 Training Acc: 99.6280%\n",
      "[Epoch:  1/  2] [data: 13568/14329] Training Loss: 0.0016 Training Acc: 99.6315%\n",
      "[Epoch:  1/  2] [data: 13696/14329] Training Loss: 0.0005 Training Acc: 99.6349%\n",
      "[Epoch:  1/  2] [data: 13824/14329] Training Loss: 0.0049 Training Acc: 99.6383%\n",
      "[Epoch:  1/  2] [data: 13952/14329] Training Loss: 0.0005 Training Acc: 99.6416%\n",
      "[Epoch:  1/  2] [data: 14080/14329] Training Loss: 0.0012 Training Acc: 99.6449%\n",
      "[Epoch:  1/  2] [data: 14208/14329] Training Loss: 0.0001 Training Acc: 99.6481%\n",
      "[Epoch:  1/  2] [data: 14329/14329] Training Loss: 0.0003 Training Acc: 99.6511%\n",
      "Testing-2...\n",
      "[Epoch:  1/  2] Validation Loss: 0.0021 Validation Acc: 99.8052%\n",
      "Time used: 835.2273042201996s\n",
      "[Epoch:  2/  2] [data: 128/14329] Training Loss: 0.0017 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 256/14329] Training Loss: 0.0012 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 384/14329] Training Loss: 0.0005 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 512/14329] Training Loss: 0.0003 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 640/14329] Training Loss: 0.0010 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 768/14329] Training Loss: 0.0021 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 896/14329] Training Loss: 0.0000 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 1024/14329] Training Loss: 0.0001 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 1152/14329] Training Loss: 0.0026 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 1280/14329] Training Loss: 0.0025 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 1408/14329] Training Loss: 0.0008 Training Acc: 99.8580%\n",
      "[Epoch:  2/  2] [data: 1536/14329] Training Loss: 0.0013 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 1664/14329] Training Loss: 0.0002 Training Acc: 99.8798%\n",
      "[Epoch:  2/  2] [data: 1792/14329] Training Loss: 0.0026 Training Acc: 99.8326%\n",
      "[Epoch:  2/  2] [data: 1920/14329] Training Loss: 0.0012 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 2048/14329] Training Loss: 0.0021 Training Acc: 99.8535%\n",
      "[Epoch:  2/  2] [data: 2176/14329] Training Loss: 0.0008 Training Acc: 99.8621%\n",
      "[Epoch:  2/  2] [data: 2304/14329] Training Loss: 0.0019 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 2432/14329] Training Loss: 0.0027 Training Acc: 99.8355%\n",
      "[Epoch:  2/  2] [data: 2560/14329] Training Loss: 0.0011 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 2688/14329] Training Loss: 0.0002 Training Acc: 99.8512%\n",
      "[Epoch:  2/  2] [data: 2816/14329] Training Loss: 0.0003 Training Acc: 99.8580%\n",
      "[Epoch:  2/  2] [data: 2944/14329] Training Loss: 0.0013 Training Acc: 99.8641%\n",
      "[Epoch:  2/  2] [data: 3072/14329] Training Loss: 0.0003 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 3200/14329] Training Loss: 0.0014 Training Acc: 99.8750%\n",
      "[Epoch:  2/  2] [data: 3328/14329] Training Loss: 0.0012 Training Acc: 99.8798%\n",
      "[Epoch:  2/  2] [data: 3456/14329] Training Loss: 0.0016 Training Acc: 99.8843%\n",
      "[Epoch:  2/  2] [data: 3584/14329] Training Loss: 0.0004 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 3712/14329] Training Loss: 0.0001 Training Acc: 99.8922%\n",
      "[Epoch:  2/  2] [data: 3840/14329] Training Loss: 0.0002 Training Acc: 99.8958%\n",
      "[Epoch:  2/  2] [data: 3968/14329] Training Loss: 0.0002 Training Acc: 99.8992%\n",
      "[Epoch:  2/  2] [data: 4096/14329] Training Loss: 0.0057 Training Acc: 99.8779%\n",
      "[Epoch:  2/  2] [data: 4224/14329] Training Loss: 0.0003 Training Acc: 99.8816%\n",
      "[Epoch:  2/  2] [data: 4352/14329] Training Loss: 0.0005 Training Acc: 99.8851%\n",
      "[Epoch:  2/  2] [data: 4480/14329] Training Loss: 0.0030 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 4608/14329] Training Loss: 0.0007 Training Acc: 99.8915%\n",
      "[Epoch:  2/  2] [data: 4736/14329] Training Loss: 0.0002 Training Acc: 99.8944%\n",
      "[Epoch:  2/  2] [data: 4864/14329] Training Loss: 0.0004 Training Acc: 99.8972%\n",
      "[Epoch:  2/  2] [data: 4992/14329] Training Loss: 0.0002 Training Acc: 99.8998%\n",
      "[Epoch:  2/  2] [data: 5120/14329] Training Loss: 0.0031 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 5248/14329] Training Loss: 0.0041 Training Acc: 99.9047%\n",
      "[Epoch:  2/  2] [data: 5376/14329] Training Loss: 0.0006 Training Acc: 99.9070%\n",
      "[Epoch:  2/  2] [data: 5504/14329] Training Loss: 0.0033 Training Acc: 99.9092%\n",
      "[Epoch:  2/  2] [data: 5632/14329] Training Loss: 0.0001 Training Acc: 99.9112%\n",
      "[Epoch:  2/  2] [data: 5760/14329] Training Loss: 0.0003 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 5888/14329] Training Loss: 0.0045 Training Acc: 99.8981%\n",
      "[Epoch:  2/  2] [data: 6016/14329] Training Loss: 0.0036 Training Acc: 99.8836%\n",
      "[Epoch:  2/  2] [data: 6144/14329] Training Loss: 0.0012 Training Acc: 99.8861%\n",
      "[Epoch:  2/  2] [data: 6272/14329] Training Loss: 0.0009 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 6400/14329] Training Loss: 0.0002 Training Acc: 99.8906%\n",
      "[Epoch:  2/  2] [data: 6528/14329] Training Loss: 0.0013 Training Acc: 99.8928%\n",
      "[Epoch:  2/  2] [data: 6656/14329] Training Loss: 0.0019 Training Acc: 99.8948%\n",
      "[Epoch:  2/  2] [data: 6784/14329] Training Loss: 0.0017 Training Acc: 99.8968%\n",
      "[Epoch:  2/  2] [data: 6912/14329] Training Loss: 0.0007 Training Acc: 99.8987%\n",
      "[Epoch:  2/  2] [data: 7040/14329] Training Loss: 0.0010 Training Acc: 99.9006%\n",
      "[Epoch:  2/  2] [data: 7168/14329] Training Loss: 0.0000 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 7296/14329] Training Loss: 0.0004 Training Acc: 99.9041%\n",
      "[Epoch:  2/  2] [data: 7424/14329] Training Loss: 0.0020 Training Acc: 99.9057%\n",
      "[Epoch:  2/  2] [data: 7552/14329] Training Loss: 0.0011 Training Acc: 99.9073%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 7680/14329] Training Loss: 0.0001 Training Acc: 99.9089%\n",
      "[Epoch:  2/  2] [data: 7808/14329] Training Loss: 0.0019 Training Acc: 99.9103%\n",
      "[Epoch:  2/  2] [data: 7936/14329] Training Loss: 0.0018 Training Acc: 99.9118%\n",
      "[Epoch:  2/  2] [data: 8064/14329] Training Loss: 0.0007 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 8192/14329] Training Loss: 0.0007 Training Acc: 99.9146%\n",
      "[Epoch:  2/  2] [data: 8320/14329] Training Loss: 0.0002 Training Acc: 99.9159%\n",
      "[Epoch:  2/  2] [data: 8448/14329] Training Loss: 0.0014 Training Acc: 99.9171%\n",
      "[Epoch:  2/  2] [data: 8576/14329] Training Loss: 0.0020 Training Acc: 99.9184%\n",
      "[Epoch:  2/  2] [data: 8704/14329] Training Loss: 0.0003 Training Acc: 99.9196%\n",
      "[Epoch:  2/  2] [data: 8832/14329] Training Loss: 0.0026 Training Acc: 99.9094%\n",
      "[Epoch:  2/  2] [data: 8960/14329] Training Loss: 0.0004 Training Acc: 99.9107%\n",
      "[Epoch:  2/  2] [data: 9088/14329] Training Loss: 0.0009 Training Acc: 99.9120%\n",
      "[Epoch:  2/  2] [data: 9216/14329] Training Loss: 0.0003 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 9344/14329] Training Loss: 0.0003 Training Acc: 99.9144%\n",
      "[Epoch:  2/  2] [data: 9472/14329] Training Loss: 0.0031 Training Acc: 99.9155%\n",
      "[Epoch:  2/  2] [data: 9600/14329] Training Loss: 0.0003 Training Acc: 99.9167%\n",
      "[Epoch:  2/  2] [data: 9728/14329] Training Loss: 0.0012 Training Acc: 99.9178%\n",
      "[Epoch:  2/  2] [data: 9856/14329] Training Loss: 0.0004 Training Acc: 99.9188%\n",
      "[Epoch:  2/  2] [data: 9984/14329] Training Loss: 0.0024 Training Acc: 99.9199%\n",
      "[Epoch:  2/  2] [data: 10112/14329] Training Loss: 0.0008 Training Acc: 99.9209%\n",
      "[Epoch:  2/  2] [data: 10240/14329] Training Loss: 0.0080 Training Acc: 99.9219%\n",
      "[Epoch:  2/  2] [data: 10368/14329] Training Loss: 0.0018 Training Acc: 99.9228%\n",
      "[Epoch:  2/  2] [data: 10496/14329] Training Loss: 0.0006 Training Acc: 99.9238%\n",
      "[Epoch:  2/  2] [data: 10624/14329] Training Loss: 0.0036 Training Acc: 99.9247%\n",
      "[Epoch:  2/  2] [data: 10752/14329] Training Loss: 0.0007 Training Acc: 99.9256%\n",
      "[Epoch:  2/  2] [data: 10880/14329] Training Loss: 0.0011 Training Acc: 99.9265%\n",
      "[Epoch:  2/  2] [data: 11008/14329] Training Loss: 0.0013 Training Acc: 99.9273%\n",
      "[Epoch:  2/  2] [data: 11136/14329] Training Loss: 0.0005 Training Acc: 99.9282%\n",
      "[Epoch:  2/  2] [data: 11264/14329] Training Loss: 0.0004 Training Acc: 99.9290%\n",
      "[Epoch:  2/  2] [data: 11392/14329] Training Loss: 0.0000 Training Acc: 99.9298%\n",
      "[Epoch:  2/  2] [data: 11520/14329] Training Loss: 0.0001 Training Acc: 99.9306%\n",
      "[Epoch:  2/  2] [data: 11648/14329] Training Loss: 0.0000 Training Acc: 99.9313%\n",
      "[Epoch:  2/  2] [data: 11776/14329] Training Loss: 0.0004 Training Acc: 99.9321%\n",
      "[Epoch:  2/  2] [data: 11904/14329] Training Loss: 0.0000 Training Acc: 99.9328%\n",
      "[Epoch:  2/  2] [data: 12032/14329] Training Loss: 0.0004 Training Acc: 99.9335%\n",
      "[Epoch:  2/  2] [data: 12160/14329] Training Loss: 0.0017 Training Acc: 99.9342%\n",
      "[Epoch:  2/  2] [data: 12288/14329] Training Loss: 0.0008 Training Acc: 99.9349%\n",
      "[Epoch:  2/  2] [data: 12416/14329] Training Loss: 0.0009 Training Acc: 99.9356%\n",
      "[Epoch:  2/  2] [data: 12544/14329] Training Loss: 0.0008 Training Acc: 99.9362%\n",
      "[Epoch:  2/  2] [data: 12672/14329] Training Loss: 0.0003 Training Acc: 99.9369%\n",
      "[Epoch:  2/  2] [data: 12800/14329] Training Loss: 0.0018 Training Acc: 99.9375%\n",
      "[Epoch:  2/  2] [data: 12928/14329] Training Loss: 0.0009 Training Acc: 99.9381%\n",
      "[Epoch:  2/  2] [data: 13056/14329] Training Loss: 0.0036 Training Acc: 99.9387%\n",
      "[Epoch:  2/  2] [data: 13184/14329] Training Loss: 0.0036 Training Acc: 99.9317%\n",
      "[Epoch:  2/  2] [data: 13312/14329] Training Loss: 0.0022 Training Acc: 99.9324%\n",
      "[Epoch:  2/  2] [data: 13440/14329] Training Loss: 0.0003 Training Acc: 99.9330%\n",
      "[Epoch:  2/  2] [data: 13568/14329] Training Loss: 0.0008 Training Acc: 99.9337%\n",
      "[Epoch:  2/  2] [data: 13696/14329] Training Loss: 0.0003 Training Acc: 99.9343%\n",
      "[Epoch:  2/  2] [data: 13824/14329] Training Loss: 0.0036 Training Acc: 99.9349%\n",
      "[Epoch:  2/  2] [data: 13952/14329] Training Loss: 0.0002 Training Acc: 99.9355%\n",
      "[Epoch:  2/  2] [data: 14080/14329] Training Loss: 0.0004 Training Acc: 99.9361%\n",
      "[Epoch:  2/  2] [data: 14208/14329] Training Loss: 0.0000 Training Acc: 99.9367%\n",
      "[Epoch:  2/  2] [data: 14329/14329] Training Loss: 0.0000 Training Acc: 99.9372%\n",
      "Testing-2...\n",
      "[Epoch:  2/  2] Validation Loss: 0.0014 Validation Acc: 99.9784%\n",
      "Time used: 841.021595954895s\n",
      "Type-2: 1.0 0.9971671388101983 0.9985815602836879\n",
      "[Epoch:  1/  2] [data: 128/23054] Training Loss: 1.0165 Training Acc: 64.0625%\n",
      "[Epoch:  1/  2] [data: 256/23054] Training Loss: 1.4451 Training Acc: 62.1094%\n",
      "[Epoch:  1/  2] [data: 384/23054] Training Loss: 0.5614 Training Acc: 63.2812%\n",
      "[Epoch:  1/  2] [data: 512/23054] Training Loss: 0.3955 Training Acc: 65.2344%\n",
      "[Epoch:  1/  2] [data: 640/23054] Training Loss: 0.3626 Training Acc: 66.0938%\n",
      "[Epoch:  1/  2] [data: 768/23054] Training Loss: 0.4878 Training Acc: 66.1458%\n",
      "[Epoch:  1/  2] [data: 896/23054] Training Loss: 0.3838 Training Acc: 66.8527%\n",
      "[Epoch:  1/  2] [data: 1024/23054] Training Loss: 0.3044 Training Acc: 67.4805%\n",
      "[Epoch:  1/  2] [data: 1152/23054] Training Loss: 0.2568 Training Acc: 68.4028%\n",
      "[Epoch:  1/  2] [data: 1280/23054] Training Loss: 0.4614 Training Acc: 68.8281%\n",
      "[Epoch:  1/  2] [data: 1408/23054] Training Loss: 0.4440 Training Acc: 69.5312%\n",
      "[Epoch:  1/  2] [data: 1536/23054] Training Loss: 0.3214 Training Acc: 69.9219%\n",
      "[Epoch:  1/  2] [data: 1664/23054] Training Loss: 0.4465 Training Acc: 70.4928%\n",
      "[Epoch:  1/  2] [data: 1792/23054] Training Loss: 0.3554 Training Acc: 71.1496%\n",
      "[Epoch:  1/  2] [data: 1920/23054] Training Loss: 0.3414 Training Acc: 72.0833%\n",
      "[Epoch:  1/  2] [data: 2048/23054] Training Loss: 0.2620 Training Acc: 73.1445%\n",
      "[Epoch:  1/  2] [data: 2176/23054] Training Loss: 0.4254 Training Acc: 73.7592%\n",
      "[Epoch:  1/  2] [data: 2304/23054] Training Loss: 0.2502 Training Acc: 74.3056%\n",
      "[Epoch:  1/  2] [data: 2432/23054] Training Loss: 0.2038 Training Acc: 75.0822%\n",
      "[Epoch:  1/  2] [data: 2560/23054] Training Loss: 0.2510 Training Acc: 75.5469%\n",
      "[Epoch:  1/  2] [data: 2688/23054] Training Loss: 0.2903 Training Acc: 75.8557%\n",
      "[Epoch:  1/  2] [data: 2816/23054] Training Loss: 0.1820 Training Acc: 76.3494%\n",
      "[Epoch:  1/  2] [data: 2944/23054] Training Loss: 0.3004 Training Acc: 76.6304%\n",
      "[Epoch:  1/  2] [data: 3072/23054] Training Loss: 0.1494 Training Acc: 77.2135%\n",
      "[Epoch:  1/  2] [data: 3200/23054] Training Loss: 0.1865 Training Acc: 77.7812%\n",
      "[Epoch:  1/  2] [data: 3328/23054] Training Loss: 0.2344 Training Acc: 78.1550%\n",
      "[Epoch:  1/  2] [data: 3456/23054] Training Loss: 0.2261 Training Acc: 78.5590%\n",
      "[Epoch:  1/  2] [data: 3584/23054] Training Loss: 0.1781 Training Acc: 78.8504%\n",
      "[Epoch:  1/  2] [data: 3712/23054] Training Loss: 0.2042 Training Acc: 79.0140%\n",
      "[Epoch:  1/  2] [data: 3840/23054] Training Loss: 0.2530 Training Acc: 79.2708%\n",
      "[Epoch:  1/  2] [data: 3968/23054] Training Loss: 0.2150 Training Acc: 79.5363%\n",
      "[Epoch:  1/  2] [data: 4096/23054] Training Loss: 0.0988 Training Acc: 80.0049%\n",
      "[Epoch:  1/  2] [data: 4224/23054] Training Loss: 0.2632 Training Acc: 80.3504%\n",
      "[Epoch:  1/  2] [data: 4352/23054] Training Loss: 0.2006 Training Acc: 80.6756%\n",
      "[Epoch:  1/  2] [data: 4480/23054] Training Loss: 0.1335 Training Acc: 80.9821%\n",
      "[Epoch:  1/  2] [data: 4608/23054] Training Loss: 0.1663 Training Acc: 81.3151%\n",
      "[Epoch:  1/  2] [data: 4736/23054] Training Loss: 0.1677 Training Acc: 81.6090%\n",
      "[Epoch:  1/  2] [data: 4864/23054] Training Loss: 0.2985 Training Acc: 81.8257%\n",
      "[Epoch:  1/  2] [data: 4992/23054] Training Loss: 0.1037 Training Acc: 82.2115%\n",
      "[Epoch:  1/  2] [data: 5120/23054] Training Loss: 0.1200 Training Acc: 82.4805%\n",
      "[Epoch:  1/  2] [data: 5248/23054] Training Loss: 0.3476 Training Acc: 82.5838%\n",
      "[Epoch:  1/  2] [data: 5376/23054] Training Loss: 0.1485 Training Acc: 82.8683%\n",
      "[Epoch:  1/  2] [data: 5504/23054] Training Loss: 0.1347 Training Acc: 83.0850%\n",
      "[Epoch:  1/  2] [data: 5632/23054] Training Loss: 0.2637 Training Acc: 83.3097%\n",
      "[Epoch:  1/  2] [data: 5760/23054] Training Loss: 0.2655 Training Acc: 83.4896%\n",
      "[Epoch:  1/  2] [data: 5888/23054] Training Loss: 0.1296 Training Acc: 83.7466%\n",
      "[Epoch:  1/  2] [data: 6016/23054] Training Loss: 0.1190 Training Acc: 83.9927%\n",
      "[Epoch:  1/  2] [data: 6144/23054] Training Loss: 0.1326 Training Acc: 84.2285%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 6272/23054] Training Loss: 0.1925 Training Acc: 84.3591%\n",
      "[Epoch:  1/  2] [data: 6400/23054] Training Loss: 0.1908 Training Acc: 84.5781%\n",
      "[Epoch:  1/  2] [data: 6528/23054] Training Loss: 0.2214 Training Acc: 84.7426%\n",
      "[Epoch:  1/  2] [data: 6656/23054] Training Loss: 0.2975 Training Acc: 84.8708%\n",
      "[Epoch:  1/  2] [data: 6784/23054] Training Loss: 0.1214 Training Acc: 85.0678%\n",
      "[Epoch:  1/  2] [data: 6912/23054] Training Loss: 0.1964 Training Acc: 85.2141%\n",
      "[Epoch:  1/  2] [data: 7040/23054] Training Loss: 0.1474 Training Acc: 85.4261%\n",
      "[Epoch:  1/  2] [data: 7168/23054] Training Loss: 0.1078 Training Acc: 85.6445%\n",
      "[Epoch:  1/  2] [data: 7296/23054] Training Loss: 0.2490 Training Acc: 85.8004%\n",
      "[Epoch:  1/  2] [data: 7424/23054] Training Loss: 0.1400 Training Acc: 85.9914%\n",
      "[Epoch:  1/  2] [data: 7552/23054] Training Loss: 0.1672 Training Acc: 86.1494%\n",
      "[Epoch:  1/  2] [data: 7680/23054] Training Loss: 0.1472 Training Acc: 86.3151%\n",
      "[Epoch:  1/  2] [data: 7808/23054] Training Loss: 0.1477 Training Acc: 86.4370%\n",
      "[Epoch:  1/  2] [data: 7936/23054] Training Loss: 0.1685 Training Acc: 86.5675%\n",
      "[Epoch:  1/  2] [data: 8064/23054] Training Loss: 0.2518 Training Acc: 86.6567%\n",
      "[Epoch:  1/  2] [data: 8192/23054] Training Loss: 0.1747 Training Acc: 86.7798%\n",
      "[Epoch:  1/  2] [data: 8320/23054] Training Loss: 0.1292 Training Acc: 86.9231%\n",
      "[Epoch:  1/  2] [data: 8448/23054] Training Loss: 0.2554 Training Acc: 86.9792%\n",
      "[Epoch:  1/  2] [data: 8576/23054] Training Loss: 0.2554 Training Acc: 87.0569%\n",
      "[Epoch:  1/  2] [data: 8704/23054] Training Loss: 0.1921 Training Acc: 87.1898%\n",
      "[Epoch:  1/  2] [data: 8832/23054] Training Loss: 0.1617 Training Acc: 87.3188%\n",
      "[Epoch:  1/  2] [data: 8960/23054] Training Loss: 0.1536 Training Acc: 87.3884%\n",
      "[Epoch:  1/  2] [data: 9088/23054] Training Loss: 0.1431 Training Acc: 87.4890%\n",
      "[Epoch:  1/  2] [data: 9216/23054] Training Loss: 0.0753 Training Acc: 87.6411%\n",
      "[Epoch:  1/  2] [data: 9344/23054] Training Loss: 0.2094 Training Acc: 87.7247%\n",
      "[Epoch:  1/  2] [data: 9472/23054] Training Loss: 0.1525 Training Acc: 87.7956%\n",
      "[Epoch:  1/  2] [data: 9600/23054] Training Loss: 0.2112 Training Acc: 87.8646%\n",
      "[Epoch:  1/  2] [data: 9728/23054] Training Loss: 0.1366 Training Acc: 87.9626%\n",
      "[Epoch:  1/  2] [data: 9856/23054] Training Loss: 0.1430 Training Acc: 88.0682%\n",
      "[Epoch:  1/  2] [data: 9984/23054] Training Loss: 0.1374 Training Acc: 88.1611%\n",
      "[Epoch:  1/  2] [data: 10112/23054] Training Loss: 0.1694 Training Acc: 88.2417%\n",
      "[Epoch:  1/  2] [data: 10240/23054] Training Loss: 0.0924 Training Acc: 88.3594%\n",
      "[Epoch:  1/  2] [data: 10368/23054] Training Loss: 0.1374 Training Acc: 88.4356%\n",
      "[Epoch:  1/  2] [data: 10496/23054] Training Loss: 0.1570 Training Acc: 88.4813%\n",
      "[Epoch:  1/  2] [data: 10624/23054] Training Loss: 0.2133 Training Acc: 88.5354%\n",
      "[Epoch:  1/  2] [data: 10752/23054] Training Loss: 0.1275 Training Acc: 88.5975%\n",
      "[Epoch:  1/  2] [data: 10880/23054] Training Loss: 0.1057 Training Acc: 88.6765%\n",
      "[Epoch:  1/  2] [data: 11008/23054] Training Loss: 0.2206 Training Acc: 88.7445%\n",
      "[Epoch:  1/  2] [data: 11136/23054] Training Loss: 0.1645 Training Acc: 88.8290%\n",
      "[Epoch:  1/  2] [data: 11264/23054] Training Loss: 0.1738 Training Acc: 88.9205%\n",
      "[Epoch:  1/  2] [data: 11392/23054] Training Loss: 0.2292 Training Acc: 88.9484%\n",
      "[Epoch:  1/  2] [data: 11520/23054] Training Loss: 0.0954 Training Acc: 89.0451%\n",
      "[Epoch:  1/  2] [data: 11648/23054] Training Loss: 0.1438 Training Acc: 89.0968%\n",
      "[Epoch:  1/  2] [data: 11776/23054] Training Loss: 0.1834 Training Acc: 89.1814%\n",
      "[Epoch:  1/  2] [data: 11904/23054] Training Loss: 0.2747 Training Acc: 89.2221%\n",
      "[Epoch:  1/  2] [data: 12032/23054] Training Loss: 0.1864 Training Acc: 89.2703%\n",
      "[Epoch:  1/  2] [data: 12160/23054] Training Loss: 0.1741 Training Acc: 89.3010%\n",
      "[Epoch:  1/  2] [data: 12288/23054] Training Loss: 0.1670 Training Acc: 89.3473%\n",
      "[Epoch:  1/  2] [data: 12416/23054] Training Loss: 0.1556 Training Acc: 89.4249%\n",
      "[Epoch:  1/  2] [data: 12544/23054] Training Loss: 0.1642 Training Acc: 89.4850%\n",
      "[Epoch:  1/  2] [data: 12672/23054] Training Loss: 0.2284 Training Acc: 89.5281%\n",
      "[Epoch:  1/  2] [data: 12800/23054] Training Loss: 0.1624 Training Acc: 89.5859%\n",
      "[Epoch:  1/  2] [data: 12928/23054] Training Loss: 0.2079 Training Acc: 89.6194%\n",
      "[Epoch:  1/  2] [data: 13056/23054] Training Loss: 0.1535 Training Acc: 89.6599%\n",
      "[Epoch:  1/  2] [data: 13184/23054] Training Loss: 0.1789 Training Acc: 89.6996%\n",
      "[Epoch:  1/  2] [data: 13312/23054] Training Loss: 0.0897 Training Acc: 89.7686%\n",
      "[Epoch:  1/  2] [data: 13440/23054] Training Loss: 0.1242 Training Acc: 89.8289%\n",
      "[Epoch:  1/  2] [data: 13568/23054] Training Loss: 0.1795 Training Acc: 89.8585%\n",
      "[Epoch:  1/  2] [data: 13696/23054] Training Loss: 0.1976 Training Acc: 89.8949%\n",
      "[Epoch:  1/  2] [data: 13824/23054] Training Loss: 0.0863 Training Acc: 89.9740%\n",
      "[Epoch:  1/  2] [data: 13952/23054] Training Loss: 0.2018 Training Acc: 89.9799%\n",
      "[Epoch:  1/  2] [data: 14080/23054] Training Loss: 0.0815 Training Acc: 90.0497%\n",
      "[Epoch:  1/  2] [data: 14208/23054] Training Loss: 0.0538 Training Acc: 90.1253%\n",
      "[Epoch:  1/  2] [data: 14336/23054] Training Loss: 0.1297 Training Acc: 90.1925%\n",
      "[Epoch:  1/  2] [data: 14464/23054] Training Loss: 0.3713 Training Acc: 90.1825%\n",
      "[Epoch:  1/  2] [data: 14592/23054] Training Loss: 0.1255 Training Acc: 90.2275%\n",
      "[Epoch:  1/  2] [data: 14720/23054] Training Loss: 0.2239 Training Acc: 90.2785%\n",
      "[Epoch:  1/  2] [data: 14848/23054] Training Loss: 0.1597 Training Acc: 90.3219%\n",
      "[Epoch:  1/  2] [data: 14976/23054] Training Loss: 0.1946 Training Acc: 90.3646%\n",
      "[Epoch:  1/  2] [data: 15104/23054] Training Loss: 0.1195 Training Acc: 90.4131%\n",
      "[Epoch:  1/  2] [data: 15232/23054] Training Loss: 0.1322 Training Acc: 90.4412%\n",
      "[Epoch:  1/  2] [data: 15360/23054] Training Loss: 0.2076 Training Acc: 90.4557%\n",
      "[Epoch:  1/  2] [data: 15488/23054] Training Loss: 0.1381 Training Acc: 90.4959%\n",
      "[Epoch:  1/  2] [data: 15616/23054] Training Loss: 0.2404 Training Acc: 90.5225%\n",
      "[Epoch:  1/  2] [data: 15744/23054] Training Loss: 0.1216 Training Acc: 90.5678%\n",
      "[Epoch:  1/  2] [data: 15872/23054] Training Loss: 0.1721 Training Acc: 90.6187%\n",
      "[Epoch:  1/  2] [data: 16000/23054] Training Loss: 0.2096 Training Acc: 90.6500%\n",
      "[Epoch:  1/  2] [data: 16128/23054] Training Loss: 0.2503 Training Acc: 90.6684%\n",
      "[Epoch:  1/  2] [data: 16256/23054] Training Loss: 0.1285 Training Acc: 90.7050%\n",
      "[Epoch:  1/  2] [data: 16384/23054] Training Loss: 0.1969 Training Acc: 90.7166%\n",
      "[Epoch:  1/  2] [data: 16512/23054] Training Loss: 0.1111 Training Acc: 90.7643%\n",
      "[Epoch:  1/  2] [data: 16640/23054] Training Loss: 0.2310 Training Acc: 90.7812%\n",
      "[Epoch:  1/  2] [data: 16768/23054] Training Loss: 0.2468 Training Acc: 90.8039%\n",
      "[Epoch:  1/  2] [data: 16896/23054] Training Loss: 0.1112 Training Acc: 90.8440%\n",
      "[Epoch:  1/  2] [data: 17024/23054] Training Loss: 0.1285 Training Acc: 90.8717%\n",
      "[Epoch:  1/  2] [data: 17152/23054] Training Loss: 0.0677 Training Acc: 90.9223%\n",
      "[Epoch:  1/  2] [data: 17280/23054] Training Loss: 0.1714 Training Acc: 90.9491%\n",
      "[Epoch:  1/  2] [data: 17408/23054] Training Loss: 0.1140 Training Acc: 90.9754%\n",
      "[Epoch:  1/  2] [data: 17536/23054] Training Loss: 0.1126 Training Acc: 91.0014%\n",
      "[Epoch:  1/  2] [data: 17664/23054] Training Loss: 0.1442 Training Acc: 91.0326%\n",
      "[Epoch:  1/  2] [data: 17792/23054] Training Loss: 0.0860 Training Acc: 91.0690%\n",
      "[Epoch:  1/  2] [data: 17920/23054] Training Loss: 0.1657 Training Acc: 91.0993%\n",
      "[Epoch:  1/  2] [data: 18048/23054] Training Loss: 0.2334 Training Acc: 91.1015%\n",
      "[Epoch:  1/  2] [data: 18176/23054] Training Loss: 0.1640 Training Acc: 91.1422%\n",
      "[Epoch:  1/  2] [data: 18304/23054] Training Loss: 0.2051 Training Acc: 91.1713%\n",
      "[Epoch:  1/  2] [data: 18432/23054] Training Loss: 0.1022 Training Acc: 91.2164%\n",
      "[Epoch:  1/  2] [data: 18560/23054] Training Loss: 0.1115 Training Acc: 91.2446%\n",
      "[Epoch:  1/  2] [data: 18688/23054] Training Loss: 0.1201 Training Acc: 91.2832%\n",
      "[Epoch:  1/  2] [data: 18816/23054] Training Loss: 0.0937 Training Acc: 91.3318%\n",
      "[Epoch:  1/  2] [data: 18944/23054] Training Loss: 0.1606 Training Acc: 91.3587%\n",
      "[Epoch:  1/  2] [data: 19072/23054] Training Loss: 0.1677 Training Acc: 91.4010%\n",
      "[Epoch:  1/  2] [data: 19200/23054] Training Loss: 0.1779 Training Acc: 91.4115%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 19328/23054] Training Loss: 0.1859 Training Acc: 91.4114%\n",
      "[Epoch:  1/  2] [data: 19456/23054] Training Loss: 0.1312 Training Acc: 91.4474%\n",
      "[Epoch:  1/  2] [data: 19584/23054] Training Loss: 0.0989 Training Acc: 91.4828%\n",
      "[Epoch:  1/  2] [data: 19712/23054] Training Loss: 0.1727 Training Acc: 91.5128%\n",
      "[Epoch:  1/  2] [data: 19840/23054] Training Loss: 0.1093 Training Acc: 91.5373%\n",
      "[Epoch:  1/  2] [data: 19968/23054] Training Loss: 0.1740 Training Acc: 91.5715%\n",
      "[Epoch:  1/  2] [data: 20096/23054] Training Loss: 0.1011 Training Acc: 91.6103%\n",
      "[Epoch:  1/  2] [data: 20224/23054] Training Loss: 0.0915 Training Acc: 91.6436%\n",
      "[Epoch:  1/  2] [data: 20352/23054] Training Loss: 0.1668 Training Acc: 91.6618%\n",
      "[Epoch:  1/  2] [data: 20480/23054] Training Loss: 0.0916 Training Acc: 91.6846%\n",
      "[Epoch:  1/  2] [data: 20608/23054] Training Loss: 0.2088 Training Acc: 91.6974%\n",
      "[Epoch:  1/  2] [data: 20736/23054] Training Loss: 0.1076 Training Acc: 91.7149%\n",
      "[Epoch:  1/  2] [data: 20864/23054] Training Loss: 0.0750 Training Acc: 91.7418%\n",
      "[Epoch:  1/  2] [data: 20992/23054] Training Loss: 0.1400 Training Acc: 91.7540%\n",
      "[Epoch:  1/  2] [data: 21120/23054] Training Loss: 0.1580 Training Acc: 91.7756%\n",
      "[Epoch:  1/  2] [data: 21248/23054] Training Loss: 0.1107 Training Acc: 91.7969%\n",
      "[Epoch:  1/  2] [data: 21376/23054] Training Loss: 0.0913 Training Acc: 91.8132%\n",
      "[Epoch:  1/  2] [data: 21504/23054] Training Loss: 0.1775 Training Acc: 91.8248%\n",
      "[Epoch:  1/  2] [data: 21632/23054] Training Loss: 0.0943 Training Acc: 91.8593%\n",
      "[Epoch:  1/  2] [data: 21760/23054] Training Loss: 0.1569 Training Acc: 91.8704%\n",
      "[Epoch:  1/  2] [data: 21888/23054] Training Loss: 0.1671 Training Acc: 91.8951%\n",
      "[Epoch:  1/  2] [data: 22016/23054] Training Loss: 0.2012 Training Acc: 91.9013%\n",
      "[Epoch:  1/  2] [data: 22144/23054] Training Loss: 0.1487 Training Acc: 91.9211%\n",
      "[Epoch:  1/  2] [data: 22272/23054] Training Loss: 0.2009 Training Acc: 91.9406%\n",
      "[Epoch:  1/  2] [data: 22400/23054] Training Loss: 0.2159 Training Acc: 91.9509%\n",
      "[Epoch:  1/  2] [data: 22528/23054] Training Loss: 0.1806 Training Acc: 91.9744%\n",
      "[Epoch:  1/  2] [data: 22656/23054] Training Loss: 0.0786 Training Acc: 92.0109%\n",
      "[Epoch:  1/  2] [data: 22784/23054] Training Loss: 0.1069 Training Acc: 92.0207%\n",
      "[Epoch:  1/  2] [data: 22912/23054] Training Loss: 0.2399 Training Acc: 92.0304%\n",
      "[Epoch:  1/  2] [data: 23040/23054] Training Loss: 0.2503 Training Acc: 92.0226%\n",
      "[Epoch:  1/  2] [data: 23054/23054] Training Loss: 0.2963 Training Acc: 92.0231%\n",
      "Testing-3...\n",
      "[Epoch:  1/  2] Validation Loss: 0.1355 Validation Acc: 95.9232%\n",
      "Time used: 1154.5457317829132s\n",
      "[Epoch:  2/  2] [data: 128/23054] Training Loss: 0.1400 Training Acc: 94.5312%\n",
      "[Epoch:  2/  2] [data: 256/23054] Training Loss: 0.1760 Training Acc: 92.9688%\n",
      "[Epoch:  2/  2] [data: 384/23054] Training Loss: 0.1254 Training Acc: 94.2708%\n",
      "[Epoch:  2/  2] [data: 512/23054] Training Loss: 0.0641 Training Acc: 95.3125%\n",
      "[Epoch:  2/  2] [data: 640/23054] Training Loss: 0.1166 Training Acc: 95.7812%\n",
      "[Epoch:  2/  2] [data: 768/23054] Training Loss: 0.1379 Training Acc: 95.7031%\n",
      "[Epoch:  2/  2] [data: 896/23054] Training Loss: 0.0847 Training Acc: 95.9821%\n",
      "[Epoch:  2/  2] [data: 1024/23054] Training Loss: 0.0465 Training Acc: 96.2891%\n",
      "[Epoch:  2/  2] [data: 1152/23054] Training Loss: 0.1080 Training Acc: 96.4410%\n",
      "[Epoch:  2/  2] [data: 1280/23054] Training Loss: 0.2077 Training Acc: 96.1719%\n",
      "[Epoch:  2/  2] [data: 1408/23054] Training Loss: 0.1307 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 1536/23054] Training Loss: 0.1269 Training Acc: 96.0286%\n",
      "[Epoch:  2/  2] [data: 1664/23054] Training Loss: 0.2581 Training Acc: 95.9135%\n",
      "[Epoch:  2/  2] [data: 1792/23054] Training Loss: 0.1416 Training Acc: 95.8705%\n",
      "[Epoch:  2/  2] [data: 1920/23054] Training Loss: 0.1679 Training Acc: 95.6771%\n",
      "[Epoch:  2/  2] [data: 2048/23054] Training Loss: 0.1071 Training Acc: 95.8008%\n",
      "[Epoch:  2/  2] [data: 2176/23054] Training Loss: 0.1827 Training Acc: 95.7721%\n",
      "[Epoch:  2/  2] [data: 2304/23054] Training Loss: 0.0929 Training Acc: 95.9201%\n",
      "[Epoch:  2/  2] [data: 2432/23054] Training Loss: 0.1003 Training Acc: 95.9704%\n",
      "[Epoch:  2/  2] [data: 2560/23054] Training Loss: 0.1556 Training Acc: 95.8984%\n",
      "[Epoch:  2/  2] [data: 2688/23054] Training Loss: 0.1535 Training Acc: 95.9077%\n",
      "[Epoch:  2/  2] [data: 2816/23054] Training Loss: 0.0953 Training Acc: 95.9517%\n",
      "[Epoch:  2/  2] [data: 2944/23054] Training Loss: 0.1615 Training Acc: 95.8899%\n",
      "[Epoch:  2/  2] [data: 3072/23054] Training Loss: 0.0963 Training Acc: 95.9635%\n",
      "[Epoch:  2/  2] [data: 3200/23054] Training Loss: 0.1014 Training Acc: 96.0312%\n",
      "[Epoch:  2/  2] [data: 3328/23054] Training Loss: 0.1131 Training Acc: 96.0337%\n",
      "[Epoch:  2/  2] [data: 3456/23054] Training Loss: 0.1287 Training Acc: 96.0069%\n",
      "[Epoch:  2/  2] [data: 3584/23054] Training Loss: 0.0974 Training Acc: 96.0379%\n",
      "[Epoch:  2/  2] [data: 3712/23054] Training Loss: 0.1190 Training Acc: 96.0129%\n",
      "[Epoch:  2/  2] [data: 3840/23054] Training Loss: 0.1641 Training Acc: 95.9635%\n",
      "[Epoch:  2/  2] [data: 3968/23054] Training Loss: 0.1053 Training Acc: 95.9929%\n",
      "[Epoch:  2/  2] [data: 4096/23054] Training Loss: 0.0292 Training Acc: 96.0449%\n",
      "[Epoch:  2/  2] [data: 4224/23054] Training Loss: 0.1348 Training Acc: 96.0701%\n",
      "[Epoch:  2/  2] [data: 4352/23054] Training Loss: 0.1357 Training Acc: 96.0018%\n",
      "[Epoch:  2/  2] [data: 4480/23054] Training Loss: 0.0892 Training Acc: 96.0268%\n",
      "[Epoch:  2/  2] [data: 4608/23054] Training Loss: 0.1024 Training Acc: 96.0286%\n",
      "[Epoch:  2/  2] [data: 4736/23054] Training Loss: 0.1124 Training Acc: 96.0093%\n",
      "[Epoch:  2/  2] [data: 4864/23054] Training Loss: 0.2276 Training Acc: 95.9498%\n",
      "[Epoch:  2/  2] [data: 4992/23054] Training Loss: 0.0468 Training Acc: 96.0136%\n",
      "[Epoch:  2/  2] [data: 5120/23054] Training Loss: 0.0513 Training Acc: 96.0742%\n",
      "[Epoch:  2/  2] [data: 5248/23054] Training Loss: 0.2306 Training Acc: 96.0556%\n",
      "[Epoch:  2/  2] [data: 5376/23054] Training Loss: 0.0994 Training Acc: 96.0565%\n",
      "[Epoch:  2/  2] [data: 5504/23054] Training Loss: 0.0715 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 5632/23054] Training Loss: 0.1629 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 5760/23054] Training Loss: 0.1806 Training Acc: 96.0417%\n",
      "[Epoch:  2/  2] [data: 5888/23054] Training Loss: 0.0744 Training Acc: 96.0768%\n",
      "[Epoch:  2/  2] [data: 6016/23054] Training Loss: 0.0619 Training Acc: 96.1270%\n",
      "[Epoch:  2/  2] [data: 6144/23054] Training Loss: 0.0673 Training Acc: 96.1263%\n",
      "[Epoch:  2/  2] [data: 6272/23054] Training Loss: 0.1820 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6400/23054] Training Loss: 0.1245 Training Acc: 96.1094%\n",
      "[Epoch:  2/  2] [data: 6528/23054] Training Loss: 0.1610 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6656/23054] Training Loss: 0.1900 Training Acc: 96.0487%\n",
      "[Epoch:  2/  2] [data: 6784/23054] Training Loss: 0.0702 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6912/23054] Training Loss: 0.1417 Training Acc: 96.0793%\n",
      "[Epoch:  2/  2] [data: 7040/23054] Training Loss: 0.0757 Training Acc: 96.1222%\n",
      "[Epoch:  2/  2] [data: 7168/23054] Training Loss: 0.0630 Training Acc: 96.1635%\n",
      "[Epoch:  2/  2] [data: 7296/23054] Training Loss: 0.1680 Training Acc: 96.1623%\n",
      "[Epoch:  2/  2] [data: 7424/23054] Training Loss: 0.0686 Training Acc: 96.1746%\n",
      "[Epoch:  2/  2] [data: 7552/23054] Training Loss: 0.1197 Training Acc: 96.1997%\n",
      "[Epoch:  2/  2] [data: 7680/23054] Training Loss: 0.0823 Training Acc: 96.2109%\n",
      "[Epoch:  2/  2] [data: 7808/23054] Training Loss: 0.0899 Training Acc: 96.2218%\n",
      "[Epoch:  2/  2] [data: 7936/23054] Training Loss: 0.1320 Training Acc: 96.2198%\n",
      "[Epoch:  2/  2] [data: 8064/23054] Training Loss: 0.1644 Training Acc: 96.2178%\n",
      "[Epoch:  2/  2] [data: 8192/23054] Training Loss: 0.1145 Training Acc: 96.2158%\n",
      "[Epoch:  2/  2] [data: 8320/23054] Training Loss: 0.1064 Training Acc: 96.2260%\n",
      "[Epoch:  2/  2] [data: 8448/23054] Training Loss: 0.1726 Training Acc: 96.2003%\n",
      "[Epoch:  2/  2] [data: 8576/23054] Training Loss: 0.1884 Training Acc: 96.1870%\n",
      "[Epoch:  2/  2] [data: 8704/23054] Training Loss: 0.1335 Training Acc: 96.1857%\n",
      "[Epoch:  2/  2] [data: 8832/23054] Training Loss: 0.1168 Training Acc: 96.1957%\n",
      "[Epoch:  2/  2] [data: 8960/23054] Training Loss: 0.1020 Training Acc: 96.1942%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 9088/23054] Training Loss: 0.1135 Training Acc: 96.1928%\n",
      "[Epoch:  2/  2] [data: 9216/23054] Training Loss: 0.0272 Training Acc: 96.2457%\n",
      "[Epoch:  2/  2] [data: 9344/23054] Training Loss: 0.1743 Training Acc: 96.2436%\n",
      "[Epoch:  2/  2] [data: 9472/23054] Training Loss: 0.1085 Training Acc: 96.2204%\n",
      "[Epoch:  2/  2] [data: 9600/23054] Training Loss: 0.1501 Training Acc: 96.2083%\n",
      "[Epoch:  2/  2] [data: 9728/23054] Training Loss: 0.0966 Training Acc: 96.2274%\n",
      "[Epoch:  2/  2] [data: 9856/23054] Training Loss: 0.0946 Training Acc: 96.2561%\n",
      "[Epoch:  2/  2] [data: 9984/23054] Training Loss: 0.0852 Training Acc: 96.2740%\n",
      "[Epoch:  2/  2] [data: 10112/23054] Training Loss: 0.1270 Training Acc: 96.2718%\n",
      "[Epoch:  2/  2] [data: 10240/23054] Training Loss: 0.0587 Training Acc: 96.2988%\n",
      "[Epoch:  2/  2] [data: 10368/23054] Training Loss: 0.0922 Training Acc: 96.3156%\n",
      "[Epoch:  2/  2] [data: 10496/23054] Training Loss: 0.1113 Training Acc: 96.3034%\n",
      "[Epoch:  2/  2] [data: 10624/23054] Training Loss: 0.1582 Training Acc: 96.2820%\n",
      "[Epoch:  2/  2] [data: 10752/23054] Training Loss: 0.1053 Training Acc: 96.2891%\n",
      "[Epoch:  2/  2] [data: 10880/23054] Training Loss: 0.0627 Training Acc: 96.2960%\n",
      "[Epoch:  2/  2] [data: 11008/23054] Training Loss: 0.2021 Training Acc: 96.2754%\n",
      "[Epoch:  2/  2] [data: 11136/23054] Training Loss: 0.1273 Training Acc: 96.2733%\n",
      "[Epoch:  2/  2] [data: 11264/23054] Training Loss: 0.1283 Training Acc: 96.2713%\n",
      "[Epoch:  2/  2] [data: 11392/23054] Training Loss: 0.1769 Training Acc: 96.2430%\n",
      "[Epoch:  2/  2] [data: 11520/23054] Training Loss: 0.0634 Training Acc: 96.2674%\n",
      "[Epoch:  2/  2] [data: 11648/23054] Training Loss: 0.1050 Training Acc: 96.2655%\n",
      "[Epoch:  2/  2] [data: 11776/23054] Training Loss: 0.1485 Training Acc: 96.2721%\n",
      "[Epoch:  2/  2] [data: 11904/23054] Training Loss: 0.2159 Training Acc: 96.2450%\n",
      "[Epoch:  2/  2] [data: 12032/23054] Training Loss: 0.1379 Training Acc: 96.2267%\n",
      "[Epoch:  2/  2] [data: 12160/23054] Training Loss: 0.1542 Training Acc: 96.2007%\n",
      "[Epoch:  2/  2] [data: 12288/23054] Training Loss: 0.1313 Training Acc: 96.1833%\n",
      "[Epoch:  2/  2] [data: 12416/23054] Training Loss: 0.0993 Training Acc: 96.1985%\n",
      "[Epoch:  2/  2] [data: 12544/23054] Training Loss: 0.1283 Training Acc: 96.1974%\n",
      "[Epoch:  2/  2] [data: 12672/23054] Training Loss: 0.2145 Training Acc: 96.1806%\n",
      "[Epoch:  2/  2] [data: 12800/23054] Training Loss: 0.1306 Training Acc: 96.1875%\n",
      "[Epoch:  2/  2] [data: 12928/23054] Training Loss: 0.1756 Training Acc: 96.1634%\n",
      "[Epoch:  2/  2] [data: 13056/23054] Training Loss: 0.1076 Training Acc: 96.1627%\n",
      "[Epoch:  2/  2] [data: 13184/23054] Training Loss: 0.1432 Training Acc: 96.1620%\n",
      "[Epoch:  2/  2] [data: 13312/23054] Training Loss: 0.0610 Training Acc: 96.1839%\n",
      "[Epoch:  2/  2] [data: 13440/23054] Training Loss: 0.0708 Training Acc: 96.1979%\n",
      "[Epoch:  2/  2] [data: 13568/23054] Training Loss: 0.1618 Training Acc: 96.1822%\n",
      "[Epoch:  2/  2] [data: 13696/23054] Training Loss: 0.1525 Training Acc: 96.1741%\n",
      "[Epoch:  2/  2] [data: 13824/23054] Training Loss: 0.0536 Training Acc: 96.1950%\n",
      "[Epoch:  2/  2] [data: 13952/23054] Training Loss: 0.1560 Training Acc: 96.1798%\n",
      "[Epoch:  2/  2] [data: 14080/23054] Training Loss: 0.0566 Training Acc: 96.2003%\n",
      "[Epoch:  2/  2] [data: 14208/23054] Training Loss: 0.0314 Training Acc: 96.2275%\n",
      "[Epoch:  2/  2] [data: 14336/23054] Training Loss: 0.1050 Training Acc: 96.2402%\n",
      "[Epoch:  2/  2] [data: 14464/23054] Training Loss: 0.3070 Training Acc: 96.2113%\n",
      "[Epoch:  2/  2] [data: 14592/23054] Training Loss: 0.0870 Training Acc: 96.2103%\n",
      "[Epoch:  2/  2] [data: 14720/23054] Training Loss: 0.1548 Training Acc: 96.2160%\n",
      "[Epoch:  2/  2] [data: 14848/23054] Training Loss: 0.1194 Training Acc: 96.2150%\n",
      "[Epoch:  2/  2] [data: 14976/23054] Training Loss: 0.1566 Training Acc: 96.2139%\n",
      "[Epoch:  2/  2] [data: 15104/23054] Training Loss: 0.0942 Training Acc: 96.2195%\n",
      "[Epoch:  2/  2] [data: 15232/23054] Training Loss: 0.1097 Training Acc: 96.2185%\n",
      "[Epoch:  2/  2] [data: 15360/23054] Training Loss: 0.1673 Training Acc: 96.1914%\n",
      "[Epoch:  2/  2] [data: 15488/23054] Training Loss: 0.1125 Training Acc: 96.1971%\n",
      "[Epoch:  2/  2] [data: 15616/23054] Training Loss: 0.1964 Training Acc: 96.1898%\n",
      "[Epoch:  2/  2] [data: 15744/23054] Training Loss: 0.1054 Training Acc: 96.2017%\n",
      "[Epoch:  2/  2] [data: 15872/23054] Training Loss: 0.1400 Training Acc: 96.2072%\n",
      "[Epoch:  2/  2] [data: 16000/23054] Training Loss: 0.2026 Training Acc: 96.1937%\n",
      "[Epoch:  2/  2] [data: 16128/23054] Training Loss: 0.2267 Training Acc: 96.1620%\n",
      "[Epoch:  2/  2] [data: 16256/23054] Training Loss: 0.0973 Training Acc: 96.1614%\n",
      "[Epoch:  2/  2] [data: 16384/23054] Training Loss: 0.1509 Training Acc: 96.1426%\n",
      "[Epoch:  2/  2] [data: 16512/23054] Training Loss: 0.0889 Training Acc: 96.1543%\n",
      "[Epoch:  2/  2] [data: 16640/23054] Training Loss: 0.1924 Training Acc: 96.1358%\n",
      "[Epoch:  2/  2] [data: 16768/23054] Training Loss: 0.2013 Training Acc: 96.1236%\n",
      "[Epoch:  2/  2] [data: 16896/23054] Training Loss: 0.0870 Training Acc: 96.1233%\n",
      "[Epoch:  2/  2] [data: 17024/23054] Training Loss: 0.0954 Training Acc: 96.1231%\n",
      "[Epoch:  2/  2] [data: 17152/23054] Training Loss: 0.0408 Training Acc: 96.1462%\n",
      "[Epoch:  2/  2] [data: 17280/23054] Training Loss: 0.1432 Training Acc: 96.1400%\n",
      "[Epoch:  2/  2] [data: 17408/23054] Training Loss: 0.0916 Training Acc: 96.1340%\n",
      "[Epoch:  2/  2] [data: 17536/23054] Training Loss: 0.0737 Training Acc: 96.1337%\n",
      "[Epoch:  2/  2] [data: 17664/23054] Training Loss: 0.1180 Training Acc: 96.1390%\n",
      "[Epoch:  2/  2] [data: 17792/23054] Training Loss: 0.0546 Training Acc: 96.1443%\n",
      "[Epoch:  2/  2] [data: 17920/23054] Training Loss: 0.1361 Training Acc: 96.1440%\n",
      "[Epoch:  2/  2] [data: 18048/23054] Training Loss: 0.2462 Training Acc: 96.1215%\n",
      "[Epoch:  2/  2] [data: 18176/23054] Training Loss: 0.1409 Training Acc: 96.1268%\n",
      "[Epoch:  2/  2] [data: 18304/23054] Training Loss: 0.1747 Training Acc: 96.1211%\n",
      "[Epoch:  2/  2] [data: 18432/23054] Training Loss: 0.0987 Training Acc: 96.1155%\n",
      "[Epoch:  2/  2] [data: 18560/23054] Training Loss: 0.0897 Training Acc: 96.1207%\n",
      "[Epoch:  2/  2] [data: 18688/23054] Training Loss: 0.1023 Training Acc: 96.1259%\n",
      "[Epoch:  2/  2] [data: 18816/23054] Training Loss: 0.0570 Training Acc: 96.1469%\n",
      "[Epoch:  2/  2] [data: 18944/23054] Training Loss: 0.1421 Training Acc: 96.1465%\n",
      "[Epoch:  2/  2] [data: 19072/23054] Training Loss: 0.1320 Training Acc: 96.1567%\n",
      "[Epoch:  2/  2] [data: 19200/23054] Training Loss: 0.1722 Training Acc: 96.1354%\n",
      "[Epoch:  2/  2] [data: 19328/23054] Training Loss: 0.1683 Training Acc: 96.1248%\n",
      "[Epoch:  2/  2] [data: 19456/23054] Training Loss: 0.1132 Training Acc: 96.1297%\n",
      "[Epoch:  2/  2] [data: 19584/23054] Training Loss: 0.0735 Training Acc: 96.1397%\n",
      "[Epoch:  2/  2] [data: 19712/23054] Training Loss: 0.1487 Training Acc: 96.1394%\n",
      "[Epoch:  2/  2] [data: 19840/23054] Training Loss: 0.1060 Training Acc: 96.1341%\n",
      "[Epoch:  2/  2] [data: 19968/23054] Training Loss: 0.1543 Training Acc: 96.1388%\n",
      "[Epoch:  2/  2] [data: 20096/23054] Training Loss: 0.0862 Training Acc: 96.1535%\n",
      "[Epoch:  2/  2] [data: 20224/23054] Training Loss: 0.0641 Training Acc: 96.1679%\n",
      "[Epoch:  2/  2] [data: 20352/23054] Training Loss: 0.1482 Training Acc: 96.1625%\n",
      "[Epoch:  2/  2] [data: 20480/23054] Training Loss: 0.0756 Training Acc: 96.1475%\n",
      "[Epoch:  2/  2] [data: 20608/23054] Training Loss: 0.1851 Training Acc: 96.1374%\n",
      "[Epoch:  2/  2] [data: 20736/23054] Training Loss: 0.0848 Training Acc: 96.1468%\n",
      "[Epoch:  2/  2] [data: 20864/23054] Training Loss: 0.0595 Training Acc: 96.1561%\n",
      "[Epoch:  2/  2] [data: 20992/23054] Training Loss: 0.1117 Training Acc: 96.1509%\n",
      "[Epoch:  2/  2] [data: 21120/23054] Training Loss: 0.1249 Training Acc: 96.1506%\n",
      "[Epoch:  2/  2] [data: 21248/23054] Training Loss: 0.0815 Training Acc: 96.1502%\n",
      "[Epoch:  2/  2] [data: 21376/23054] Training Loss: 0.0717 Training Acc: 96.1452%\n",
      "[Epoch:  2/  2] [data: 21504/23054] Training Loss: 0.1601 Training Acc: 96.1403%\n",
      "[Epoch:  2/  2] [data: 21632/23054] Training Loss: 0.0709 Training Acc: 96.1538%\n",
      "[Epoch:  2/  2] [data: 21760/23054] Training Loss: 0.1426 Training Acc: 96.1443%\n",
      "[Epoch:  2/  2] [data: 21888/23054] Training Loss: 0.1421 Training Acc: 96.1486%\n",
      "[Epoch:  2/  2] [data: 22016/23054] Training Loss: 0.1860 Training Acc: 96.1346%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 22144/23054] Training Loss: 0.1413 Training Acc: 96.1299%\n",
      "[Epoch:  2/  2] [data: 22272/23054] Training Loss: 0.1793 Training Acc: 96.1252%\n",
      "[Epoch:  2/  2] [data: 22400/23054] Training Loss: 0.1946 Training Acc: 96.1205%\n",
      "[Epoch:  2/  2] [data: 22528/23054] Training Loss: 0.1563 Training Acc: 96.1159%\n",
      "[Epoch:  2/  2] [data: 22656/23054] Training Loss: 0.0585 Training Acc: 96.1291%\n",
      "[Epoch:  2/  2] [data: 22784/23054] Training Loss: 0.0822 Training Acc: 96.1157%\n",
      "[Epoch:  2/  2] [data: 22912/23054] Training Loss: 0.2099 Training Acc: 96.1112%\n",
      "[Epoch:  2/  2] [data: 23040/23054] Training Loss: 0.2349 Training Acc: 96.0807%\n",
      "[Epoch:  2/  2] [data: 23054/23054] Training Loss: 0.2632 Training Acc: 96.0788%\n",
      "Testing-3...\n",
      "[Epoch:  2/  2] Validation Loss: 0.1205 Validation Acc: 96.4492%\n",
      "Time used: 1109.9475662708282s\n",
      "Type-3: 1.0 0.9126266776225691 0.9543176285264213\n",
      "[Epoch:  1/  5] [data: 128/24058] Training Loss: 0.2702 Training Acc: 85.1562%\n",
      "[Epoch:  1/  5] [data: 256/24058] Training Loss: 0.3907 Training Acc: 81.6406%\n",
      "[Epoch:  1/  5] [data: 384/24058] Training Loss: 0.3654 Training Acc: 80.4688%\n",
      "[Epoch:  1/  5] [data: 512/24058] Training Loss: 0.2389 Training Acc: 82.2266%\n",
      "[Epoch:  1/  5] [data: 640/24058] Training Loss: 0.3127 Training Acc: 82.1875%\n",
      "[Epoch:  1/  5] [data: 768/24058] Training Loss: 0.3018 Training Acc: 82.1615%\n",
      "[Epoch:  1/  5] [data: 896/24058] Training Loss: 0.1664 Training Acc: 82.3661%\n",
      "[Epoch:  1/  5] [data: 1024/24058] Training Loss: 0.2193 Training Acc: 83.0078%\n",
      "[Epoch:  1/  5] [data: 1152/24058] Training Loss: 0.2530 Training Acc: 83.1597%\n",
      "[Epoch:  1/  5] [data: 1280/24058] Training Loss: 0.5216 Training Acc: 82.4219%\n",
      "[Epoch:  1/  5] [data: 1408/24058] Training Loss: 0.2670 Training Acc: 83.0256%\n",
      "[Epoch:  1/  5] [data: 1536/24058] Training Loss: 0.1509 Training Acc: 83.4635%\n",
      "[Epoch:  1/  5] [data: 1664/24058] Training Loss: 0.3041 Training Acc: 83.8341%\n",
      "[Epoch:  1/  5] [data: 1792/24058] Training Loss: 0.3042 Training Acc: 84.0960%\n",
      "[Epoch:  1/  5] [data: 1920/24058] Training Loss: 0.1677 Training Acc: 84.6875%\n",
      "[Epoch:  1/  5] [data: 2048/24058] Training Loss: 0.2394 Training Acc: 85.1074%\n",
      "[Epoch:  1/  5] [data: 2176/24058] Training Loss: 0.1541 Training Acc: 85.7077%\n",
      "[Epoch:  1/  5] [data: 2304/24058] Training Loss: 0.3004 Training Acc: 85.8941%\n",
      "[Epoch:  1/  5] [data: 2432/24058] Training Loss: 0.1293 Training Acc: 86.2664%\n",
      "[Epoch:  1/  5] [data: 2560/24058] Training Loss: 0.2401 Training Acc: 86.4453%\n",
      "[Epoch:  1/  5] [data: 2688/24058] Training Loss: 0.1418 Training Acc: 86.7560%\n",
      "[Epoch:  1/  5] [data: 2816/24058] Training Loss: 0.2487 Training Acc: 86.9673%\n",
      "[Epoch:  1/  5] [data: 2944/24058] Training Loss: 0.1361 Training Acc: 87.2622%\n",
      "[Epoch:  1/  5] [data: 3072/24058] Training Loss: 0.3250 Training Acc: 87.3698%\n",
      "[Epoch:  1/  5] [data: 3200/24058] Training Loss: 0.2640 Training Acc: 87.4688%\n",
      "[Epoch:  1/  5] [data: 3328/24058] Training Loss: 0.1985 Training Acc: 87.6803%\n",
      "[Epoch:  1/  5] [data: 3456/24058] Training Loss: 0.1803 Training Acc: 87.8762%\n",
      "[Epoch:  1/  5] [data: 3584/24058] Training Loss: 0.1632 Training Acc: 88.0580%\n",
      "[Epoch:  1/  5] [data: 3712/24058] Training Loss: 0.3578 Training Acc: 87.9580%\n",
      "[Epoch:  1/  5] [data: 3840/24058] Training Loss: 0.1421 Training Acc: 88.2292%\n",
      "[Epoch:  1/  5] [data: 3968/24058] Training Loss: 0.1863 Training Acc: 88.4577%\n",
      "[Epoch:  1/  5] [data: 4096/24058] Training Loss: 0.2596 Training Acc: 88.4766%\n",
      "[Epoch:  1/  5] [data: 4224/24058] Training Loss: 0.1755 Training Acc: 88.6837%\n",
      "[Epoch:  1/  5] [data: 4352/24058] Training Loss: 0.1279 Training Acc: 88.7408%\n",
      "[Epoch:  1/  5] [data: 4480/24058] Training Loss: 0.2629 Training Acc: 88.6830%\n",
      "[Epoch:  1/  5] [data: 4608/24058] Training Loss: 0.1364 Training Acc: 88.7587%\n",
      "[Epoch:  1/  5] [data: 4736/24058] Training Loss: 0.2840 Training Acc: 88.7880%\n",
      "[Epoch:  1/  5] [data: 4864/24058] Training Loss: 0.1482 Training Acc: 88.9186%\n",
      "[Epoch:  1/  5] [data: 4992/24058] Training Loss: 0.1661 Training Acc: 88.9623%\n",
      "[Epoch:  1/  5] [data: 5120/24058] Training Loss: 0.2417 Training Acc: 88.9453%\n",
      "[Epoch:  1/  5] [data: 5248/24058] Training Loss: 0.2261 Training Acc: 88.9863%\n",
      "[Epoch:  1/  5] [data: 5376/24058] Training Loss: 0.2402 Training Acc: 89.0253%\n",
      "[Epoch:  1/  5] [data: 5504/24058] Training Loss: 0.1695 Training Acc: 89.2078%\n",
      "[Epoch:  1/  5] [data: 5632/24058] Training Loss: 0.2169 Training Acc: 89.3111%\n",
      "[Epoch:  1/  5] [data: 5760/24058] Training Loss: 0.1813 Training Acc: 89.4097%\n",
      "[Epoch:  1/  5] [data: 5888/24058] Training Loss: 0.1740 Training Acc: 89.4871%\n",
      "[Epoch:  1/  5] [data: 6016/24058] Training Loss: 0.2005 Training Acc: 89.5612%\n",
      "[Epoch:  1/  5] [data: 6144/24058] Training Loss: 0.2066 Training Acc: 89.6322%\n",
      "[Epoch:  1/  5] [data: 6272/24058] Training Loss: 0.2504 Training Acc: 89.6524%\n",
      "[Epoch:  1/  5] [data: 6400/24058] Training Loss: 0.2192 Training Acc: 89.7031%\n",
      "[Epoch:  1/  5] [data: 6528/24058] Training Loss: 0.1886 Training Acc: 89.7365%\n",
      "[Epoch:  1/  5] [data: 6656/24058] Training Loss: 0.2144 Training Acc: 89.7236%\n",
      "[Epoch:  1/  5] [data: 6784/24058] Training Loss: 0.1699 Training Acc: 89.8290%\n",
      "[Epoch:  1/  5] [data: 6912/24058] Training Loss: 0.2339 Training Acc: 89.8582%\n",
      "[Epoch:  1/  5] [data: 7040/24058] Training Loss: 0.1874 Training Acc: 89.9290%\n",
      "[Epoch:  1/  5] [data: 7168/24058] Training Loss: 0.2422 Training Acc: 89.9414%\n",
      "[Epoch:  1/  5] [data: 7296/24058] Training Loss: 0.1276 Training Acc: 90.0630%\n",
      "[Epoch:  1/  5] [data: 7424/24058] Training Loss: 0.1510 Training Acc: 90.1401%\n",
      "[Epoch:  1/  5] [data: 7552/24058] Training Loss: 0.1145 Training Acc: 90.2542%\n",
      "[Epoch:  1/  5] [data: 7680/24058] Training Loss: 0.2136 Training Acc: 90.2474%\n",
      "[Epoch:  1/  5] [data: 7808/24058] Training Loss: 0.2656 Training Acc: 90.2664%\n",
      "[Epoch:  1/  5] [data: 7936/24058] Training Loss: 0.3702 Training Acc: 90.2218%\n",
      "[Epoch:  1/  5] [data: 8064/24058] Training Loss: 0.3340 Training Acc: 90.1290%\n",
      "[Epoch:  1/  5] [data: 8192/24058] Training Loss: 0.1211 Training Acc: 90.2100%\n",
      "[Epoch:  1/  5] [data: 8320/24058] Training Loss: 0.2188 Training Acc: 90.2644%\n",
      "[Epoch:  1/  5] [data: 8448/24058] Training Loss: 0.2668 Training Acc: 90.2699%\n",
      "[Epoch:  1/  5] [data: 8576/24058] Training Loss: 0.1172 Training Acc: 90.3801%\n",
      "[Epoch:  1/  5] [data: 8704/24058] Training Loss: 0.2286 Training Acc: 90.4067%\n",
      "[Epoch:  1/  5] [data: 8832/24058] Training Loss: 0.2957 Training Acc: 90.4099%\n",
      "[Epoch:  1/  5] [data: 8960/24058] Training Loss: 0.3059 Training Acc: 90.4018%\n",
      "[Epoch:  1/  5] [data: 9088/24058] Training Loss: 0.2290 Training Acc: 90.4049%\n",
      "[Epoch:  1/  5] [data: 9216/24058] Training Loss: 0.1589 Training Acc: 90.4622%\n",
      "[Epoch:  1/  5] [data: 9344/24058] Training Loss: 0.2105 Training Acc: 90.4966%\n",
      "[Epoch:  1/  5] [data: 9472/24058] Training Loss: 0.2159 Training Acc: 90.5194%\n",
      "[Epoch:  1/  5] [data: 9600/24058] Training Loss: 0.2930 Training Acc: 90.5312%\n",
      "[Epoch:  1/  5] [data: 9728/24058] Training Loss: 0.1930 Training Acc: 90.5736%\n",
      "[Epoch:  1/  5] [data: 9856/24058] Training Loss: 0.2340 Training Acc: 90.6047%\n",
      "[Epoch:  1/  5] [data: 9984/24058] Training Loss: 0.1814 Training Acc: 90.6350%\n",
      "[Epoch:  1/  5] [data: 10112/24058] Training Loss: 0.2280 Training Acc: 90.6744%\n",
      "[Epoch:  1/  5] [data: 10240/24058] Training Loss: 0.1871 Training Acc: 90.6836%\n",
      "[Epoch:  1/  5] [data: 10368/24058] Training Loss: 0.1914 Training Acc: 90.7118%\n",
      "[Epoch:  1/  5] [data: 10496/24058] Training Loss: 0.1414 Training Acc: 90.7774%\n",
      "[Epoch:  1/  5] [data: 10624/24058] Training Loss: 0.1218 Training Acc: 90.8321%\n",
      "[Epoch:  1/  5] [data: 10752/24058] Training Loss: 0.1524 Training Acc: 90.8575%\n",
      "[Epoch:  1/  5] [data: 10880/24058] Training Loss: 0.2043 Training Acc: 90.8915%\n",
      "[Epoch:  1/  5] [data: 11008/24058] Training Loss: 0.2656 Training Acc: 90.8703%\n",
      "[Epoch:  1/  5] [data: 11136/24058] Training Loss: 0.2318 Training Acc: 90.8944%\n",
      "[Epoch:  1/  5] [data: 11264/24058] Training Loss: 0.2173 Training Acc: 90.9002%\n",
      "[Epoch:  1/  5] [data: 11392/24058] Training Loss: 0.1001 Training Acc: 90.9673%\n",
      "[Epoch:  1/  5] [data: 11520/24058] Training Loss: 0.1864 Training Acc: 90.9896%\n",
      "[Epoch:  1/  5] [data: 11648/24058] Training Loss: 0.2069 Training Acc: 91.0199%\n",
      "[Epoch:  1/  5] [data: 11776/24058] Training Loss: 0.3017 Training Acc: 91.0241%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  5] [data: 11904/24058] Training Loss: 0.1757 Training Acc: 91.0366%\n",
      "[Epoch:  1/  5] [data: 12032/24058] Training Loss: 0.1775 Training Acc: 91.0738%\n",
      "[Epoch:  1/  5] [data: 12160/24058] Training Loss: 0.1806 Training Acc: 91.1020%\n",
      "[Epoch:  1/  5] [data: 12288/24058] Training Loss: 0.1759 Training Acc: 91.1540%\n",
      "[Epoch:  1/  5] [data: 12416/24058] Training Loss: 0.2805 Training Acc: 91.1485%\n",
      "[Epoch:  1/  5] [data: 12544/24058] Training Loss: 0.1644 Training Acc: 91.1910%\n",
      "[Epoch:  1/  5] [data: 12672/24058] Training Loss: 0.2476 Training Acc: 91.1932%\n",
      "[Epoch:  1/  5] [data: 12800/24058] Training Loss: 0.2447 Training Acc: 91.1875%\n",
      "[Epoch:  1/  5] [data: 12928/24058] Training Loss: 0.1383 Training Acc: 91.2361%\n",
      "[Epoch:  1/  5] [data: 13056/24058] Training Loss: 0.2213 Training Acc: 91.2531%\n",
      "[Epoch:  1/  5] [data: 13184/24058] Training Loss: 0.1988 Training Acc: 91.2849%\n",
      "[Epoch:  1/  5] [data: 13312/24058] Training Loss: 0.1332 Training Acc: 91.3386%\n",
      "[Epoch:  1/  5] [data: 13440/24058] Training Loss: 0.1159 Training Acc: 91.3914%\n",
      "[Epoch:  1/  5] [data: 13568/24058] Training Loss: 0.2029 Training Acc: 91.4210%\n",
      "[Epoch:  1/  5] [data: 13696/24058] Training Loss: 0.2127 Training Acc: 91.4501%\n",
      "[Epoch:  1/  5] [data: 13824/24058] Training Loss: 0.1645 Training Acc: 91.4569%\n",
      "[Epoch:  1/  5] [data: 13952/24058] Training Loss: 0.2038 Training Acc: 91.4851%\n",
      "[Epoch:  1/  5] [data: 14080/24058] Training Loss: 0.1788 Training Acc: 91.5270%\n",
      "[Epoch:  1/  5] [data: 14208/24058] Training Loss: 0.2834 Training Acc: 91.5118%\n",
      "[Epoch:  1/  5] [data: 14336/24058] Training Loss: 0.1647 Training Acc: 91.5527%\n",
      "[Epoch:  1/  5] [data: 14464/24058] Training Loss: 0.1126 Training Acc: 91.5929%\n",
      "[Epoch:  1/  5] [data: 14592/24058] Training Loss: 0.1938 Training Acc: 91.6187%\n",
      "[Epoch:  1/  5] [data: 14720/24058] Training Loss: 0.0864 Training Acc: 91.6712%\n",
      "[Epoch:  1/  5] [data: 14848/24058] Training Loss: 0.1944 Training Acc: 91.6756%\n",
      "[Epoch:  1/  5] [data: 14976/24058] Training Loss: 0.1623 Training Acc: 91.7067%\n",
      "[Epoch:  1/  5] [data: 15104/24058] Training Loss: 0.1551 Training Acc: 91.7307%\n",
      "[Epoch:  1/  5] [data: 15232/24058] Training Loss: 0.1879 Training Acc: 91.7411%\n",
      "[Epoch:  1/  5] [data: 15360/24058] Training Loss: 0.1740 Training Acc: 91.7839%\n",
      "[Epoch:  1/  5] [data: 15488/24058] Training Loss: 0.1566 Training Acc: 91.8001%\n",
      "[Epoch:  1/  5] [data: 15616/24058] Training Loss: 0.1779 Training Acc: 91.8225%\n",
      "[Epoch:  1/  5] [data: 15744/24058] Training Loss: 0.2624 Training Acc: 91.8255%\n",
      "[Epoch:  1/  5] [data: 15872/24058] Training Loss: 0.1627 Training Acc: 91.8536%\n",
      "[Epoch:  1/  5] [data: 16000/24058] Training Loss: 0.2230 Training Acc: 91.8625%\n",
      "[Epoch:  1/  5] [data: 16128/24058] Training Loss: 0.1656 Training Acc: 91.8713%\n",
      "[Epoch:  1/  5] [data: 16256/24058] Training Loss: 0.2239 Training Acc: 91.8861%\n",
      "[Epoch:  1/  5] [data: 16384/24058] Training Loss: 0.2185 Training Acc: 91.8884%\n",
      "[Epoch:  1/  5] [data: 16512/24058] Training Loss: 0.2933 Training Acc: 91.8665%\n",
      "[Epoch:  1/  5] [data: 16640/24058] Training Loss: 0.1978 Training Acc: 91.8870%\n",
      "[Epoch:  1/  5] [data: 16768/24058] Training Loss: 0.1506 Training Acc: 91.9191%\n",
      "[Epoch:  1/  5] [data: 16896/24058] Training Loss: 0.2114 Training Acc: 91.9389%\n",
      "[Epoch:  1/  5] [data: 17024/24058] Training Loss: 0.2983 Training Acc: 91.9114%\n",
      "[Epoch:  1/  5] [data: 17152/24058] Training Loss: 0.1966 Training Acc: 91.9310%\n",
      "[Epoch:  1/  5] [data: 17280/24058] Training Loss: 0.2590 Training Acc: 91.9329%\n",
      "[Epoch:  1/  5] [data: 17408/24058] Training Loss: 0.2554 Training Acc: 91.9290%\n",
      "[Epoch:  1/  5] [data: 17536/24058] Training Loss: 0.1477 Training Acc: 91.9537%\n",
      "[Epoch:  1/  5] [data: 17664/24058] Training Loss: 0.1774 Training Acc: 91.9780%\n",
      "[Epoch:  1/  5] [data: 17792/24058] Training Loss: 0.1670 Training Acc: 91.9908%\n",
      "[Epoch:  1/  5] [data: 17920/24058] Training Loss: 0.2446 Training Acc: 91.9922%\n",
      "[Epoch:  1/  5] [data: 18048/24058] Training Loss: 0.1489 Training Acc: 92.0047%\n",
      "[Epoch:  1/  5] [data: 18176/24058] Training Loss: 0.2807 Training Acc: 91.9949%\n",
      "[Epoch:  1/  5] [data: 18304/24058] Training Loss: 0.2046 Training Acc: 92.0072%\n",
      "[Epoch:  1/  5] [data: 18432/24058] Training Loss: 0.1680 Training Acc: 92.0356%\n",
      "[Epoch:  1/  5] [data: 18560/24058] Training Loss: 0.1953 Training Acc: 92.0420%\n",
      "[Epoch:  1/  5] [data: 18688/24058] Training Loss: 0.1213 Training Acc: 92.0751%\n",
      "[Epoch:  1/  5] [data: 18816/24058] Training Loss: 0.1697 Training Acc: 92.0972%\n",
      "[Epoch:  1/  5] [data: 18944/24058] Training Loss: 0.1844 Training Acc: 92.1189%\n",
      "[Epoch:  1/  5] [data: 19072/24058] Training Loss: 0.1685 Training Acc: 92.1456%\n",
      "[Epoch:  1/  5] [data: 19200/24058] Training Loss: 0.1030 Training Acc: 92.1771%\n",
      "[Epoch:  1/  5] [data: 19328/24058] Training Loss: 0.2267 Training Acc: 92.1823%\n",
      "[Epoch:  1/  5] [data: 19456/24058] Training Loss: 0.1976 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 19584/24058] Training Loss: 0.1424 Training Acc: 92.2028%\n",
      "[Epoch:  1/  5] [data: 19712/24058] Training Loss: 0.1667 Training Acc: 92.2179%\n",
      "[Epoch:  1/  5] [data: 19840/24058] Training Loss: 0.1486 Training Acc: 92.2228%\n",
      "[Epoch:  1/  5] [data: 19968/24058] Training Loss: 0.2217 Training Acc: 92.2326%\n",
      "[Epoch:  1/  5] [data: 20096/24058] Training Loss: 0.3190 Training Acc: 92.2223%\n",
      "[Epoch:  1/  5] [data: 20224/24058] Training Loss: 0.2499 Training Acc: 92.2172%\n",
      "[Epoch:  1/  5] [data: 20352/24058] Training Loss: 0.1693 Training Acc: 92.2366%\n",
      "[Epoch:  1/  5] [data: 20480/24058] Training Loss: 0.1462 Training Acc: 92.2656%\n",
      "[Epoch:  1/  5] [data: 20608/24058] Training Loss: 0.2926 Training Acc: 92.2554%\n",
      "[Epoch:  1/  5] [data: 20736/24058] Training Loss: 0.1408 Training Acc: 92.2695%\n",
      "[Epoch:  1/  5] [data: 20864/24058] Training Loss: 0.1509 Training Acc: 92.2834%\n",
      "[Epoch:  1/  5] [data: 20992/24058] Training Loss: 0.2233 Training Acc: 92.2828%\n",
      "[Epoch:  1/  5] [data: 21120/24058] Training Loss: 0.2132 Training Acc: 92.2964%\n",
      "[Epoch:  1/  5] [data: 21248/24058] Training Loss: 0.1375 Training Acc: 92.3193%\n",
      "[Epoch:  1/  5] [data: 21376/24058] Training Loss: 0.1639 Training Acc: 92.3325%\n",
      "[Epoch:  1/  5] [data: 21504/24058] Training Loss: 0.1151 Training Acc: 92.3503%\n",
      "[Epoch:  1/  5] [data: 21632/24058] Training Loss: 0.2871 Training Acc: 92.3401%\n",
      "[Epoch:  1/  5] [data: 21760/24058] Training Loss: 0.1905 Training Acc: 92.3438%\n",
      "[Epoch:  1/  5] [data: 21888/24058] Training Loss: 0.2351 Training Acc: 92.3474%\n",
      "[Epoch:  1/  5] [data: 22016/24058] Training Loss: 0.2134 Training Acc: 92.3556%\n",
      "[Epoch:  1/  5] [data: 22144/24058] Training Loss: 0.2043 Training Acc: 92.3681%\n",
      "[Epoch:  1/  5] [data: 22272/24058] Training Loss: 0.1611 Training Acc: 92.3851%\n",
      "[Epoch:  1/  5] [data: 22400/24058] Training Loss: 0.1721 Training Acc: 92.3973%\n",
      "[Epoch:  1/  5] [data: 22528/24058] Training Loss: 0.2008 Training Acc: 92.4094%\n",
      "[Epoch:  1/  5] [data: 22656/24058] Training Loss: 0.2679 Training Acc: 92.4038%\n",
      "[Epoch:  1/  5] [data: 22784/24058] Training Loss: 0.1963 Training Acc: 92.4157%\n",
      "[Epoch:  1/  5] [data: 22912/24058] Training Loss: 0.2080 Training Acc: 92.4232%\n",
      "[Epoch:  1/  5] [data: 23040/24058] Training Loss: 0.2528 Training Acc: 92.4175%\n",
      "[Epoch:  1/  5] [data: 23168/24058] Training Loss: 0.1802 Training Acc: 92.4292%\n",
      "[Epoch:  1/  5] [data: 23296/24058] Training Loss: 0.2097 Training Acc: 92.4365%\n",
      "[Epoch:  1/  5] [data: 23424/24058] Training Loss: 0.1402 Training Acc: 92.4565%\n",
      "[Epoch:  1/  5] [data: 23552/24058] Training Loss: 0.1695 Training Acc: 92.4720%\n",
      "[Epoch:  1/  5] [data: 23680/24058] Training Loss: 0.2168 Training Acc: 92.4747%\n",
      "[Epoch:  1/  5] [data: 23808/24058] Training Loss: 0.0810 Training Acc: 92.4983%\n",
      "[Epoch:  1/  5] [data: 23936/24058] Training Loss: 0.1569 Training Acc: 92.5175%\n",
      "[Epoch:  1/  5] [data: 24058/24058] Training Loss: 0.1783 Training Acc: 92.5222%\n",
      "Testing-4...\n",
      "[Epoch:  1/  5] Validation Loss: 0.1979 Validation Acc: 93.5447%\n",
      "Time used: 1169.54776096344s\n",
      "[Epoch:  2/  5] [data: 128/24058] Training Loss: 0.2346 Training Acc: 92.1875%\n",
      "[Epoch:  2/  5] [data: 256/24058] Training Loss: 0.3126 Training Acc: 89.8438%\n",
      "[Epoch:  2/  5] [data: 384/24058] Training Loss: 0.3259 Training Acc: 90.3646%\n",
      "[Epoch:  2/  5] [data: 512/24058] Training Loss: 0.1158 Training Acc: 91.7969%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 640/24058] Training Loss: 0.2030 Training Acc: 92.0312%\n",
      "[Epoch:  2/  5] [data: 768/24058] Training Loss: 0.2306 Training Acc: 92.1875%\n",
      "[Epoch:  2/  5] [data: 896/24058] Training Loss: 0.0916 Training Acc: 92.9688%\n",
      "[Epoch:  2/  5] [data: 1024/24058] Training Loss: 0.1730 Training Acc: 93.2617%\n",
      "[Epoch:  2/  5] [data: 1152/24058] Training Loss: 0.1832 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 1280/24058] Training Loss: 0.4563 Training Acc: 92.7344%\n",
      "[Epoch:  2/  5] [data: 1408/24058] Training Loss: 0.1927 Training Acc: 92.8267%\n",
      "[Epoch:  2/  5] [data: 1536/24058] Training Loss: 0.0998 Training Acc: 93.1641%\n",
      "[Epoch:  2/  5] [data: 1664/24058] Training Loss: 0.2702 Training Acc: 93.0889%\n",
      "[Epoch:  2/  5] [data: 1792/24058] Training Loss: 0.2834 Training Acc: 92.9688%\n",
      "[Epoch:  2/  5] [data: 1920/24058] Training Loss: 0.1081 Training Acc: 93.2292%\n",
      "[Epoch:  2/  5] [data: 2048/24058] Training Loss: 0.2084 Training Acc: 93.3105%\n",
      "[Epoch:  2/  5] [data: 2176/24058] Training Loss: 0.0997 Training Acc: 93.5662%\n",
      "[Epoch:  2/  5] [data: 2304/24058] Training Loss: 0.2441 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 2432/24058] Training Loss: 0.0968 Training Acc: 93.6678%\n",
      "[Epoch:  2/  5] [data: 2560/24058] Training Loss: 0.2070 Training Acc: 93.5938%\n",
      "[Epoch:  2/  5] [data: 2688/24058] Training Loss: 0.0890 Training Acc: 93.7872%\n",
      "[Epoch:  2/  5] [data: 2816/24058] Training Loss: 0.1861 Training Acc: 93.7855%\n",
      "[Epoch:  2/  5] [data: 2944/24058] Training Loss: 0.0843 Training Acc: 93.9198%\n",
      "[Epoch:  2/  5] [data: 3072/24058] Training Loss: 0.2658 Training Acc: 93.8477%\n",
      "[Epoch:  2/  5] [data: 3200/24058] Training Loss: 0.2073 Training Acc: 93.8438%\n",
      "[Epoch:  2/  5] [data: 3328/24058] Training Loss: 0.1307 Training Acc: 93.9603%\n",
      "[Epoch:  2/  5] [data: 3456/24058] Training Loss: 0.1349 Training Acc: 94.0394%\n",
      "[Epoch:  2/  5] [data: 3584/24058] Training Loss: 0.1146 Training Acc: 94.1406%\n",
      "[Epoch:  2/  5] [data: 3712/24058] Training Loss: 0.3024 Training Acc: 93.9925%\n",
      "[Epoch:  2/  5] [data: 3840/24058] Training Loss: 0.0892 Training Acc: 94.1406%\n",
      "[Epoch:  2/  5] [data: 3968/24058] Training Loss: 0.1441 Training Acc: 94.2036%\n",
      "[Epoch:  2/  5] [data: 4096/24058] Training Loss: 0.2319 Training Acc: 94.1650%\n",
      "[Epoch:  2/  5] [data: 4224/24058] Training Loss: 0.1371 Training Acc: 94.2472%\n",
      "[Epoch:  2/  5] [data: 4352/24058] Training Loss: 0.0702 Training Acc: 94.3244%\n",
      "[Epoch:  2/  5] [data: 4480/24058] Training Loss: 0.2420 Training Acc: 94.2634%\n",
      "[Epoch:  2/  5] [data: 4608/24058] Training Loss: 0.0837 Training Acc: 94.3142%\n",
      "[Epoch:  2/  5] [data: 4736/24058] Training Loss: 0.2509 Training Acc: 94.2990%\n",
      "[Epoch:  2/  5] [data: 4864/24058] Training Loss: 0.1094 Training Acc: 94.3873%\n",
      "[Epoch:  2/  5] [data: 4992/24058] Training Loss: 0.1265 Training Acc: 94.4311%\n",
      "[Epoch:  2/  5] [data: 5120/24058] Training Loss: 0.1948 Training Acc: 94.3750%\n",
      "[Epoch:  2/  5] [data: 5248/24058] Training Loss: 0.1882 Training Acc: 94.3788%\n",
      "[Epoch:  2/  5] [data: 5376/24058] Training Loss: 0.2085 Training Acc: 94.3824%\n",
      "[Epoch:  2/  5] [data: 5504/24058] Training Loss: 0.1266 Training Acc: 94.4586%\n",
      "[Epoch:  2/  5] [data: 5632/24058] Training Loss: 0.1761 Training Acc: 94.4602%\n",
      "[Epoch:  2/  5] [data: 5760/24058] Training Loss: 0.1499 Training Acc: 94.4792%\n",
      "[Epoch:  2/  5] [data: 5888/24058] Training Loss: 0.1515 Training Acc: 94.4973%\n",
      "[Epoch:  2/  5] [data: 6016/24058] Training Loss: 0.1827 Training Acc: 94.4814%\n",
      "[Epoch:  2/  5] [data: 6144/24058] Training Loss: 0.1618 Training Acc: 94.4987%\n",
      "[Epoch:  2/  5] [data: 6272/24058] Training Loss: 0.2152 Training Acc: 94.4834%\n",
      "[Epoch:  2/  5] [data: 6400/24058] Training Loss: 0.2136 Training Acc: 94.4688%\n",
      "[Epoch:  2/  5] [data: 6528/24058] Training Loss: 0.1551 Training Acc: 94.5006%\n",
      "[Epoch:  2/  5] [data: 6656/24058] Training Loss: 0.1826 Training Acc: 94.5162%\n",
      "[Epoch:  2/  5] [data: 6784/24058] Training Loss: 0.1401 Training Acc: 94.5607%\n",
      "[Epoch:  2/  5] [data: 6912/24058] Training Loss: 0.2046 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 7040/24058] Training Loss: 0.1765 Training Acc: 94.5455%\n",
      "[Epoch:  2/  5] [data: 7168/24058] Training Loss: 0.1933 Training Acc: 94.5173%\n",
      "[Epoch:  2/  5] [data: 7296/24058] Training Loss: 0.0911 Training Acc: 94.5587%\n",
      "[Epoch:  2/  5] [data: 7424/24058] Training Loss: 0.1169 Training Acc: 94.5851%\n",
      "[Epoch:  2/  5] [data: 7552/24058] Training Loss: 0.0815 Training Acc: 94.6239%\n",
      "[Epoch:  2/  5] [data: 7680/24058] Training Loss: 0.2079 Training Acc: 94.5833%\n",
      "[Epoch:  2/  5] [data: 7808/24058] Training Loss: 0.2168 Training Acc: 94.5697%\n",
      "[Epoch:  2/  5] [data: 7936/24058] Training Loss: 0.3199 Training Acc: 94.4934%\n",
      "[Epoch:  2/  5] [data: 8064/24058] Training Loss: 0.2871 Training Acc: 94.4568%\n",
      "[Epoch:  2/  5] [data: 8192/24058] Training Loss: 0.0971 Training Acc: 94.5068%\n",
      "[Epoch:  2/  5] [data: 8320/24058] Training Loss: 0.1980 Training Acc: 94.5192%\n",
      "[Epoch:  2/  5] [data: 8448/24058] Training Loss: 0.2432 Training Acc: 94.4839%\n",
      "[Epoch:  2/  5] [data: 8576/24058] Training Loss: 0.1113 Training Acc: 94.5429%\n",
      "[Epoch:  2/  5] [data: 8704/24058] Training Loss: 0.2010 Training Acc: 94.5198%\n",
      "[Epoch:  2/  5] [data: 8832/24058] Training Loss: 0.2793 Training Acc: 94.4860%\n",
      "[Epoch:  2/  5] [data: 8960/24058] Training Loss: 0.2891 Training Acc: 94.4420%\n",
      "[Epoch:  2/  5] [data: 9088/24058] Training Loss: 0.1851 Training Acc: 94.4212%\n",
      "[Epoch:  2/  5] [data: 9216/24058] Training Loss: 0.1280 Training Acc: 94.4444%\n",
      "[Epoch:  2/  5] [data: 9344/24058] Training Loss: 0.1908 Training Acc: 94.4563%\n",
      "[Epoch:  2/  5] [data: 9472/24058] Training Loss: 0.2089 Training Acc: 94.4468%\n",
      "[Epoch:  2/  5] [data: 9600/24058] Training Loss: 0.2640 Training Acc: 94.4271%\n",
      "[Epoch:  2/  5] [data: 9728/24058] Training Loss: 0.1696 Training Acc: 94.4387%\n",
      "[Epoch:  2/  5] [data: 9856/24058] Training Loss: 0.2020 Training Acc: 94.4399%\n",
      "[Epoch:  2/  5] [data: 9984/24058] Training Loss: 0.1652 Training Acc: 94.4411%\n",
      "[Epoch:  2/  5] [data: 10112/24058] Training Loss: 0.2247 Training Acc: 94.4324%\n",
      "[Epoch:  2/  5] [data: 10240/24058] Training Loss: 0.1573 Training Acc: 94.4141%\n",
      "[Epoch:  2/  5] [data: 10368/24058] Training Loss: 0.1611 Training Acc: 94.4252%\n",
      "[Epoch:  2/  5] [data: 10496/24058] Training Loss: 0.1228 Training Acc: 94.4550%\n",
      "[Epoch:  2/  5] [data: 10624/24058] Training Loss: 0.0960 Training Acc: 94.4748%\n",
      "[Epoch:  2/  5] [data: 10752/24058] Training Loss: 0.1286 Training Acc: 94.4847%\n",
      "[Epoch:  2/  5] [data: 10880/24058] Training Loss: 0.1760 Training Acc: 94.4945%\n",
      "[Epoch:  2/  5] [data: 11008/24058] Training Loss: 0.2510 Training Acc: 94.4677%\n",
      "[Epoch:  2/  5] [data: 11136/24058] Training Loss: 0.2195 Training Acc: 94.4594%\n",
      "[Epoch:  2/  5] [data: 11264/24058] Training Loss: 0.2014 Training Acc: 94.4602%\n",
      "[Epoch:  2/  5] [data: 11392/24058] Training Loss: 0.0803 Training Acc: 94.5049%\n",
      "[Epoch:  2/  5] [data: 11520/24058] Training Loss: 0.1714 Training Acc: 94.4965%\n",
      "[Epoch:  2/  5] [data: 11648/24058] Training Loss: 0.1828 Training Acc: 94.4969%\n",
      "[Epoch:  2/  5] [data: 11776/24058] Training Loss: 0.2869 Training Acc: 94.4633%\n",
      "[Epoch:  2/  5] [data: 11904/24058] Training Loss: 0.1579 Training Acc: 94.4808%\n",
      "[Epoch:  2/  5] [data: 12032/24058] Training Loss: 0.1487 Training Acc: 94.4814%\n",
      "[Epoch:  2/  5] [data: 12160/24058] Training Loss: 0.1734 Training Acc: 94.4901%\n",
      "[Epoch:  2/  5] [data: 12288/24058] Training Loss: 0.1604 Training Acc: 94.5068%\n",
      "[Epoch:  2/  5] [data: 12416/24058] Training Loss: 0.2704 Training Acc: 94.4668%\n",
      "[Epoch:  2/  5] [data: 12544/24058] Training Loss: 0.1491 Training Acc: 94.4834%\n",
      "[Epoch:  2/  5] [data: 12672/24058] Training Loss: 0.2300 Training Acc: 94.4681%\n",
      "[Epoch:  2/  5] [data: 12800/24058] Training Loss: 0.2269 Training Acc: 94.4531%\n",
      "[Epoch:  2/  5] [data: 12928/24058] Training Loss: 0.1241 Training Acc: 94.4771%\n",
      "[Epoch:  2/  5] [data: 13056/24058] Training Loss: 0.2079 Training Acc: 94.4700%\n",
      "[Epoch:  2/  5] [data: 13184/24058] Training Loss: 0.1812 Training Acc: 94.4782%\n",
      "[Epoch:  2/  5] [data: 13312/24058] Training Loss: 0.1160 Training Acc: 94.5012%\n",
      "[Epoch:  2/  5] [data: 13440/24058] Training Loss: 0.0961 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 13568/24058] Training Loss: 0.1883 Training Acc: 94.5386%\n",
      "[Epoch:  2/  5] [data: 13696/24058] Training Loss: 0.1931 Training Acc: 94.5386%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 13824/24058] Training Loss: 0.1355 Training Acc: 94.5530%\n",
      "[Epoch:  2/  5] [data: 13952/24058] Training Loss: 0.1779 Training Acc: 94.5528%\n",
      "[Epoch:  2/  5] [data: 14080/24058] Training Loss: 0.1615 Training Acc: 94.5668%\n",
      "[Epoch:  2/  5] [data: 14208/24058] Training Loss: 0.2702 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 14336/24058] Training Loss: 0.1309 Training Acc: 94.5522%\n",
      "[Epoch:  2/  5] [data: 14464/24058] Training Loss: 0.0772 Training Acc: 94.5796%\n",
      "[Epoch:  2/  5] [data: 14592/24058] Training Loss: 0.1787 Training Acc: 94.5861%\n",
      "[Epoch:  2/  5] [data: 14720/24058] Training Loss: 0.0760 Training Acc: 94.6196%\n",
      "[Epoch:  2/  5] [data: 14848/24058] Training Loss: 0.1731 Training Acc: 94.6188%\n",
      "[Epoch:  2/  5] [data: 14976/24058] Training Loss: 0.1530 Training Acc: 94.6314%\n",
      "[Epoch:  2/  5] [data: 15104/24058] Training Loss: 0.1283 Training Acc: 94.6438%\n",
      "[Epoch:  2/  5] [data: 15232/24058] Training Loss: 0.1581 Training Acc: 94.6560%\n",
      "[Epoch:  2/  5] [data: 15360/24058] Training Loss: 0.1479 Training Acc: 94.6745%\n",
      "[Epoch:  2/  5] [data: 15488/24058] Training Loss: 0.1504 Training Acc: 94.6733%\n",
      "[Epoch:  2/  5] [data: 15616/24058] Training Loss: 0.1774 Training Acc: 94.6785%\n",
      "[Epoch:  2/  5] [data: 15744/24058] Training Loss: 0.2556 Training Acc: 94.6710%\n",
      "[Epoch:  2/  5] [data: 15872/24058] Training Loss: 0.1369 Training Acc: 94.6888%\n",
      "[Epoch:  2/  5] [data: 16000/24058] Training Loss: 0.1984 Training Acc: 94.6937%\n",
      "[Epoch:  2/  5] [data: 16128/24058] Training Loss: 0.1485 Training Acc: 94.6987%\n",
      "[Epoch:  2/  5] [data: 16256/24058] Training Loss: 0.2059 Training Acc: 94.6973%\n",
      "[Epoch:  2/  5] [data: 16384/24058] Training Loss: 0.1963 Training Acc: 94.6960%\n",
      "[Epoch:  2/  5] [data: 16512/24058] Training Loss: 0.2579 Training Acc: 94.6766%\n",
      "[Epoch:  2/  5] [data: 16640/24058] Training Loss: 0.1863 Training Acc: 94.6815%\n",
      "[Epoch:  2/  5] [data: 16768/24058] Training Loss: 0.1356 Training Acc: 94.6982%\n",
      "[Epoch:  2/  5] [data: 16896/24058] Training Loss: 0.1931 Training Acc: 94.7029%\n",
      "[Epoch:  2/  5] [data: 17024/24058] Training Loss: 0.2933 Training Acc: 94.6664%\n",
      "[Epoch:  2/  5] [data: 17152/24058] Training Loss: 0.1791 Training Acc: 94.6712%\n",
      "[Epoch:  2/  5] [data: 17280/24058] Training Loss: 0.2536 Training Acc: 94.6586%\n",
      "[Epoch:  2/  5] [data: 17408/24058] Training Loss: 0.2375 Training Acc: 94.6461%\n",
      "[Epoch:  2/  5] [data: 17536/24058] Training Loss: 0.1276 Training Acc: 94.6510%\n",
      "[Epoch:  2/  5] [data: 17664/24058] Training Loss: 0.1675 Training Acc: 94.6558%\n",
      "[Epoch:  2/  5] [data: 17792/24058] Training Loss: 0.1596 Training Acc: 94.6661%\n",
      "[Epoch:  2/  5] [data: 17920/24058] Training Loss: 0.2267 Training Acc: 94.6596%\n",
      "[Epoch:  2/  5] [data: 18048/24058] Training Loss: 0.1442 Training Acc: 94.6753%\n",
      "[Epoch:  2/  5] [data: 18176/24058] Training Loss: 0.2667 Training Acc: 94.6523%\n",
      "[Epoch:  2/  5] [data: 18304/24058] Training Loss: 0.1822 Training Acc: 94.6569%\n",
      "[Epoch:  2/  5] [data: 18432/24058] Training Loss: 0.1544 Training Acc: 94.6669%\n",
      "[Epoch:  2/  5] [data: 18560/24058] Training Loss: 0.1894 Training Acc: 94.6659%\n",
      "[Epoch:  2/  5] [data: 18688/24058] Training Loss: 0.1136 Training Acc: 94.6811%\n",
      "[Epoch:  2/  5] [data: 18816/24058] Training Loss: 0.1603 Training Acc: 94.6854%\n",
      "[Epoch:  2/  5] [data: 18944/24058] Training Loss: 0.1866 Training Acc: 94.6843%\n",
      "[Epoch:  2/  5] [data: 19072/24058] Training Loss: 0.1367 Training Acc: 94.6938%\n",
      "[Epoch:  2/  5] [data: 19200/24058] Training Loss: 0.0946 Training Acc: 94.7135%\n",
      "[Epoch:  2/  5] [data: 19328/24058] Training Loss: 0.2156 Training Acc: 94.7020%\n",
      "[Epoch:  2/  5] [data: 19456/24058] Training Loss: 0.1776 Training Acc: 94.7060%\n",
      "[Epoch:  2/  5] [data: 19584/24058] Training Loss: 0.1198 Training Acc: 94.7151%\n",
      "[Epoch:  2/  5] [data: 19712/24058] Training Loss: 0.1546 Training Acc: 94.7240%\n",
      "[Epoch:  2/  5] [data: 19840/24058] Training Loss: 0.1334 Training Acc: 94.7278%\n",
      "[Epoch:  2/  5] [data: 19968/24058] Training Loss: 0.2117 Training Acc: 94.7216%\n",
      "[Epoch:  2/  5] [data: 20096/24058] Training Loss: 0.3157 Training Acc: 94.6905%\n",
      "[Epoch:  2/  5] [data: 20224/24058] Training Loss: 0.2465 Training Acc: 94.6746%\n",
      "[Epoch:  2/  5] [data: 20352/24058] Training Loss: 0.1450 Training Acc: 94.6836%\n",
      "[Epoch:  2/  5] [data: 20480/24058] Training Loss: 0.1067 Training Acc: 94.7021%\n",
      "[Epoch:  2/  5] [data: 20608/24058] Training Loss: 0.2784 Training Acc: 94.6817%\n",
      "[Epoch:  2/  5] [data: 20736/24058] Training Loss: 0.1285 Training Acc: 94.6904%\n",
      "[Epoch:  2/  5] [data: 20864/24058] Training Loss: 0.1273 Training Acc: 94.6990%\n",
      "[Epoch:  2/  5] [data: 20992/24058] Training Loss: 0.1979 Training Acc: 94.6837%\n",
      "[Epoch:  2/  5] [data: 21120/24058] Training Loss: 0.1968 Training Acc: 94.6828%\n",
      "[Epoch:  2/  5] [data: 21248/24058] Training Loss: 0.1239 Training Acc: 94.6960%\n",
      "[Epoch:  2/  5] [data: 21376/24058] Training Loss: 0.1610 Training Acc: 94.6997%\n",
      "[Epoch:  2/  5] [data: 21504/24058] Training Loss: 0.0938 Training Acc: 94.7080%\n",
      "[Epoch:  2/  5] [data: 21632/24058] Training Loss: 0.2851 Training Acc: 94.6884%\n",
      "[Epoch:  2/  5] [data: 21760/24058] Training Loss: 0.1766 Training Acc: 94.6921%\n",
      "[Epoch:  2/  5] [data: 21888/24058] Training Loss: 0.2271 Training Acc: 94.6820%\n",
      "[Epoch:  2/  5] [data: 22016/24058] Training Loss: 0.1871 Training Acc: 94.6766%\n",
      "[Epoch:  2/  5] [data: 22144/24058] Training Loss: 0.2033 Training Acc: 94.6758%\n",
      "[Epoch:  2/  5] [data: 22272/24058] Training Loss: 0.1421 Training Acc: 94.6884%\n",
      "[Epoch:  2/  5] [data: 22400/24058] Training Loss: 0.1871 Training Acc: 94.6830%\n",
      "[Epoch:  2/  5] [data: 22528/24058] Training Loss: 0.1944 Training Acc: 94.6822%\n",
      "[Epoch:  2/  5] [data: 22656/24058] Training Loss: 0.2442 Training Acc: 94.6769%\n",
      "[Epoch:  2/  5] [data: 22784/24058] Training Loss: 0.1750 Training Acc: 94.6805%\n",
      "[Epoch:  2/  5] [data: 22912/24058] Training Loss: 0.1952 Training Acc: 94.6753%\n",
      "[Epoch:  2/  5] [data: 23040/24058] Training Loss: 0.2405 Training Acc: 94.6658%\n",
      "[Epoch:  2/  5] [data: 23168/24058] Training Loss: 0.1563 Training Acc: 94.6694%\n",
      "[Epoch:  2/  5] [data: 23296/24058] Training Loss: 0.2029 Training Acc: 94.6643%\n",
      "[Epoch:  2/  5] [data: 23424/24058] Training Loss: 0.1224 Training Acc: 94.6764%\n",
      "[Epoch:  2/  5] [data: 23552/24058] Training Loss: 0.1553 Training Acc: 94.6799%\n",
      "[Epoch:  2/  5] [data: 23680/24058] Training Loss: 0.2090 Training Acc: 94.6748%\n",
      "[Epoch:  2/  5] [data: 23808/24058] Training Loss: 0.0720 Training Acc: 94.6951%\n",
      "[Epoch:  2/  5] [data: 23936/24058] Training Loss: 0.1492 Training Acc: 94.7025%\n",
      "[Epoch:  2/  5] [data: 24058/24058] Training Loss: 0.1777 Training Acc: 94.7045%\n",
      "Testing-4...\n",
      "[Epoch:  2/  5] Validation Loss: 0.1832 Validation Acc: 94.5048%\n",
      "Time used: 1151.8869507312775s\n",
      "[Epoch:  3/  5] [data: 128/24058] Training Loss: 0.2119 Training Acc: 92.1875%\n",
      "[Epoch:  3/  5] [data: 256/24058] Training Loss: 0.2851 Training Acc: 90.6250%\n",
      "[Epoch:  3/  5] [data: 384/24058] Training Loss: 0.2969 Training Acc: 91.1458%\n",
      "[Epoch:  3/  5] [data: 512/24058] Training Loss: 0.1058 Training Acc: 92.5781%\n",
      "[Epoch:  3/  5] [data: 640/24058] Training Loss: 0.1923 Training Acc: 92.9688%\n",
      "[Epoch:  3/  5] [data: 768/24058] Training Loss: 0.2170 Training Acc: 93.0990%\n",
      "[Epoch:  3/  5] [data: 896/24058] Training Loss: 0.0610 Training Acc: 93.9732%\n",
      "[Epoch:  3/  5] [data: 1024/24058] Training Loss: 0.1591 Training Acc: 94.2383%\n",
      "[Epoch:  3/  5] [data: 1152/24058] Training Loss: 0.1794 Training Acc: 94.3576%\n",
      "[Epoch:  3/  5] [data: 1280/24058] Training Loss: 0.4454 Training Acc: 93.5156%\n",
      "[Epoch:  3/  5] [data: 1408/24058] Training Loss: 0.1624 Training Acc: 93.6080%\n",
      "[Epoch:  3/  5] [data: 1536/24058] Training Loss: 0.0990 Training Acc: 93.9453%\n",
      "[Epoch:  3/  5] [data: 1664/24058] Training Loss: 0.2558 Training Acc: 93.8702%\n",
      "[Epoch:  3/  5] [data: 1792/24058] Training Loss: 0.2670 Training Acc: 93.6942%\n",
      "[Epoch:  3/  5] [data: 1920/24058] Training Loss: 0.1048 Training Acc: 93.9583%\n",
      "[Epoch:  3/  5] [data: 2048/24058] Training Loss: 0.2026 Training Acc: 93.9941%\n",
      "[Epoch:  3/  5] [data: 2176/24058] Training Loss: 0.0906 Training Acc: 94.2096%\n",
      "[Epoch:  3/  5] [data: 2304/24058] Training Loss: 0.2416 Training Acc: 94.0972%\n",
      "[Epoch:  3/  5] [data: 2432/24058] Training Loss: 0.0725 Training Acc: 94.2434%\n",
      "[Epoch:  3/  5] [data: 2560/24058] Training Loss: 0.2004 Training Acc: 94.1797%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  3/  5] [data: 2688/24058] Training Loss: 0.0788 Training Acc: 94.3824%\n",
      "[Epoch:  3/  5] [data: 2816/24058] Training Loss: 0.1706 Training Acc: 94.3537%\n",
      "[Epoch:  3/  5] [data: 2944/24058] Training Loss: 0.0776 Training Acc: 94.4973%\n",
      "[Epoch:  3/  5] [data: 3072/24058] Training Loss: 0.2595 Training Acc: 94.4010%\n",
      "[Epoch:  3/  5] [data: 3200/24058] Training Loss: 0.2038 Training Acc: 94.3750%\n",
      "[Epoch:  3/  5] [data: 3328/24058] Training Loss: 0.1209 Training Acc: 94.4712%\n",
      "[Epoch:  3/  5] [data: 3456/24058] Training Loss: 0.1252 Training Acc: 94.5602%\n",
      "[Epoch:  3/  5] [data: 3584/24058] Training Loss: 0.1066 Training Acc: 94.6429%\n",
      "[Epoch:  3/  5] [data: 3712/24058] Training Loss: 0.2971 Training Acc: 94.5043%\n",
      "[Epoch:  3/  5] [data: 3840/24058] Training Loss: 0.0775 Training Acc: 94.6094%\n",
      "[Epoch:  3/  5] [data: 3968/24058] Training Loss: 0.1269 Training Acc: 94.6825%\n",
      "[Epoch:  3/  5] [data: 4096/24058] Training Loss: 0.2168 Training Acc: 94.6533%\n",
      "[Epoch:  3/  5] [data: 4224/24058] Training Loss: 0.1266 Training Acc: 94.7206%\n",
      "[Epoch:  3/  5] [data: 4352/24058] Training Loss: 0.0654 Training Acc: 94.8070%\n",
      "[Epoch:  3/  5] [data: 4480/24058] Training Loss: 0.2200 Training Acc: 94.7321%\n",
      "[Epoch:  3/  5] [data: 4608/24058] Training Loss: 0.0723 Training Acc: 94.8134%\n",
      "[Epoch:  3/  5] [data: 4736/24058] Training Loss: 0.2327 Training Acc: 94.7635%\n",
      "[Epoch:  3/  5] [data: 4864/24058] Training Loss: 0.0997 Training Acc: 94.8396%\n",
      "[Epoch:  3/  5] [data: 4992/24058] Training Loss: 0.1112 Training Acc: 94.8718%\n",
      "[Epoch:  3/  5] [data: 5120/24058] Training Loss: 0.1869 Training Acc: 94.8438%\n",
      "[Epoch:  3/  5] [data: 5248/24058] Training Loss: 0.1769 Training Acc: 94.8552%\n",
      "[Epoch:  3/  5] [data: 5376/24058] Training Loss: 0.1986 Training Acc: 94.8475%\n",
      "[Epoch:  3/  5] [data: 5504/24058] Training Loss: 0.1146 Training Acc: 94.9128%\n",
      "[Epoch:  3/  5] [data: 5632/24058] Training Loss: 0.1712 Training Acc: 94.9041%\n",
      "[Epoch:  3/  5] [data: 5760/24058] Training Loss: 0.1498 Training Acc: 94.9306%\n",
      "[Epoch:  3/  5] [data: 5888/24058] Training Loss: 0.1488 Training Acc: 94.9558%\n",
      "[Epoch:  3/  5] [data: 6016/24058] Training Loss: 0.1686 Training Acc: 94.9468%\n",
      "[Epoch:  3/  5] [data: 6144/24058] Training Loss: 0.1459 Training Acc: 94.9544%\n",
      "[Epoch:  3/  5] [data: 6272/24058] Training Loss: 0.2089 Training Acc: 94.9298%\n",
      "[Epoch:  3/  5] [data: 6400/24058] Training Loss: 0.2118 Training Acc: 94.9062%\n",
      "[Epoch:  3/  5] [data: 6528/24058] Training Loss: 0.1515 Training Acc: 94.9295%\n",
      "[Epoch:  3/  5] [data: 6656/24058] Training Loss: 0.1747 Training Acc: 94.9369%\n",
      "[Epoch:  3/  5] [data: 6784/24058] Training Loss: 0.1342 Training Acc: 94.9735%\n",
      "[Epoch:  3/  5] [data: 6912/24058] Training Loss: 0.1993 Training Acc: 94.9653%\n",
      "[Epoch:  3/  5] [data: 7040/24058] Training Loss: 0.1700 Training Acc: 94.9716%\n",
      "[Epoch:  3/  5] [data: 7168/24058] Training Loss: 0.1829 Training Acc: 94.9777%\n",
      "[Epoch:  3/  5] [data: 7296/24058] Training Loss: 0.0748 Training Acc: 95.0247%\n",
      "[Epoch:  3/  5] [data: 7424/24058] Training Loss: 0.1061 Training Acc: 95.0566%\n",
      "[Epoch:  3/  5] [data: 7552/24058] Training Loss: 0.0759 Training Acc: 95.1006%\n",
      "[Epoch:  3/  5] [data: 7680/24058] Training Loss: 0.1908 Training Acc: 95.0911%\n",
      "[Epoch:  3/  5] [data: 7808/24058] Training Loss: 0.2015 Training Acc: 95.0820%\n",
      "[Epoch:  3/  5] [data: 7936/24058] Training Loss: 0.3122 Training Acc: 95.0353%\n",
      "[Epoch:  3/  5] [data: 8064/24058] Training Loss: 0.2746 Training Acc: 94.9901%\n",
      "[Epoch:  3/  5] [data: 8192/24058] Training Loss: 0.0814 Training Acc: 95.0317%\n",
      "[Epoch:  3/  5] [data: 8320/24058] Training Loss: 0.1829 Training Acc: 95.0361%\n",
      "[Epoch:  3/  5] [data: 8448/24058] Training Loss: 0.2381 Training Acc: 94.9929%\n",
      "[Epoch:  3/  5] [data: 8576/24058] Training Loss: 0.1039 Training Acc: 95.0443%\n",
      "[Epoch:  3/  5] [data: 8704/24058] Training Loss: 0.1946 Training Acc: 95.0253%\n",
      "[Epoch:  3/  5] [data: 8832/24058] Training Loss: 0.2755 Training Acc: 94.9841%\n",
      "[Epoch:  3/  5] [data: 8960/24058] Training Loss: 0.2734 Training Acc: 94.9442%\n",
      "[Epoch:  3/  5] [data: 9088/24058] Training Loss: 0.1805 Training Acc: 94.9384%\n",
      "[Epoch:  3/  5] [data: 9216/24058] Training Loss: 0.1258 Training Acc: 94.9544%\n",
      "[Epoch:  3/  5] [data: 9344/24058] Training Loss: 0.1803 Training Acc: 94.9593%\n",
      "[Epoch:  3/  5] [data: 9472/24058] Training Loss: 0.2178 Training Acc: 94.9219%\n",
      "[Epoch:  3/  5] [data: 9600/24058] Training Loss: 0.2693 Training Acc: 94.8854%\n",
      "[Epoch:  3/  5] [data: 9728/24058] Training Loss: 0.1716 Training Acc: 94.8910%\n",
      "[Epoch:  3/  5] [data: 9856/24058] Training Loss: 0.1905 Training Acc: 94.8864%\n",
      "[Epoch:  3/  5] [data: 9984/24058] Training Loss: 0.1617 Training Acc: 94.8818%\n",
      "[Epoch:  3/  5] [data: 10112/24058] Training Loss: 0.2270 Training Acc: 94.8675%\n",
      "[Epoch:  3/  5] [data: 10240/24058] Training Loss: 0.1449 Training Acc: 94.8535%\n",
      "[Epoch:  3/  5] [data: 10368/24058] Training Loss: 0.1542 Training Acc: 94.8592%\n",
      "[Epoch:  3/  5] [data: 10496/24058] Training Loss: 0.1247 Training Acc: 94.8838%\n",
      "[Epoch:  3/  5] [data: 10624/24058] Training Loss: 0.0906 Training Acc: 94.9078%\n",
      "[Epoch:  3/  5] [data: 10752/24058] Training Loss: 0.1265 Training Acc: 94.9126%\n",
      "[Epoch:  3/  5] [data: 10880/24058] Training Loss: 0.1674 Training Acc: 94.9173%\n",
      "[Epoch:  3/  5] [data: 11008/24058] Training Loss: 0.2435 Training Acc: 94.8855%\n",
      "[Epoch:  3/  5] [data: 11136/24058] Training Loss: 0.2141 Training Acc: 94.8725%\n",
      "[Epoch:  3/  5] [data: 11264/24058] Training Loss: 0.1908 Training Acc: 94.8686%\n",
      "[Epoch:  3/  5] [data: 11392/24058] Training Loss: 0.0718 Training Acc: 94.9087%\n",
      "[Epoch:  3/  5] [data: 11520/24058] Training Loss: 0.1619 Training Acc: 94.9045%\n",
      "[Epoch:  3/  5] [data: 11648/24058] Training Loss: 0.1800 Training Acc: 94.9004%\n",
      "[Epoch:  3/  5] [data: 11776/24058] Training Loss: 0.2775 Training Acc: 94.8624%\n",
      "[Epoch:  3/  5] [data: 11904/24058] Training Loss: 0.1546 Training Acc: 94.8757%\n",
      "[Epoch:  3/  5] [data: 12032/24058] Training Loss: 0.1346 Training Acc: 94.8803%\n",
      "[Epoch:  3/  5] [data: 12160/24058] Training Loss: 0.1635 Training Acc: 94.8849%\n",
      "[Epoch:  3/  5] [data: 12288/24058] Training Loss: 0.1605 Training Acc: 94.8975%\n",
      "[Epoch:  3/  5] [data: 12416/24058] Training Loss: 0.2617 Training Acc: 94.8615%\n",
      "[Epoch:  3/  5] [data: 12544/24058] Training Loss: 0.1478 Training Acc: 94.8740%\n",
      "[Epoch:  3/  5] [data: 12672/24058] Training Loss: 0.2260 Training Acc: 94.8548%\n",
      "[Epoch:  3/  5] [data: 12800/24058] Training Loss: 0.2256 Training Acc: 94.8438%\n",
      "[Epoch:  3/  5] [data: 12928/24058] Training Loss: 0.1237 Training Acc: 94.8639%\n",
      "[Epoch:  3/  5] [data: 13056/24058] Training Loss: 0.2065 Training Acc: 94.8529%\n",
      "[Epoch:  3/  5] [data: 13184/24058] Training Loss: 0.1801 Training Acc: 94.8574%\n",
      "[Epoch:  3/  5] [data: 13312/24058] Training Loss: 0.1040 Training Acc: 94.8768%\n",
      "[Epoch:  3/  5] [data: 13440/24058] Training Loss: 0.0796 Training Acc: 94.9107%\n",
      "[Epoch:  3/  5] [data: 13568/24058] Training Loss: 0.1818 Training Acc: 94.9145%\n",
      "[Epoch:  3/  5] [data: 13696/24058] Training Loss: 0.1847 Training Acc: 94.9109%\n",
      "[Epoch:  3/  5] [data: 13824/24058] Training Loss: 0.1296 Training Acc: 94.9146%\n",
      "[Epoch:  3/  5] [data: 13952/24058] Training Loss: 0.1684 Training Acc: 94.9183%\n",
      "[Epoch:  3/  5] [data: 14080/24058] Training Loss: 0.1553 Training Acc: 94.9290%\n",
      "[Epoch:  3/  5] [data: 14208/24058] Training Loss: 0.2655 Training Acc: 94.9113%\n",
      "[Epoch:  3/  5] [data: 14336/24058] Training Loss: 0.1270 Training Acc: 94.9289%\n",
      "[Epoch:  3/  5] [data: 14464/24058] Training Loss: 0.0720 Training Acc: 94.9599%\n",
      "[Epoch:  3/  5] [data: 14592/24058] Training Loss: 0.1728 Training Acc: 94.9630%\n",
      "[Epoch:  3/  5] [data: 14720/24058] Training Loss: 0.0715 Training Acc: 94.9932%\n",
      "[Epoch:  3/  5] [data: 14848/24058] Training Loss: 0.1735 Training Acc: 94.9892%\n",
      "[Epoch:  3/  5] [data: 14976/24058] Training Loss: 0.1502 Training Acc: 94.9920%\n",
      "[Epoch:  3/  5] [data: 15104/24058] Training Loss: 0.1285 Training Acc: 95.0013%\n",
      "[Epoch:  3/  5] [data: 15232/24058] Training Loss: 0.1488 Training Acc: 95.0105%\n",
      "[Epoch:  3/  5] [data: 15360/24058] Training Loss: 0.1322 Training Acc: 95.0260%\n",
      "[Epoch:  3/  5] [data: 15488/24058] Training Loss: 0.1477 Training Acc: 95.0284%\n",
      "[Epoch:  3/  5] [data: 15616/24058] Training Loss: 0.1750 Training Acc: 95.0307%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  3/  5] [data: 15744/24058] Training Loss: 0.2417 Training Acc: 95.0203%\n",
      "[Epoch:  3/  5] [data: 15872/24058] Training Loss: 0.1313 Training Acc: 95.0353%\n",
      "[Epoch:  3/  5] [data: 16000/24058] Training Loss: 0.1893 Training Acc: 95.0375%\n",
      "[Epoch:  3/  5] [data: 16128/24058] Training Loss: 0.1408 Training Acc: 95.0459%\n",
      "[Epoch:  3/  5] [data: 16256/24058] Training Loss: 0.2029 Training Acc: 95.0418%\n",
      "[Epoch:  3/  5] [data: 16384/24058] Training Loss: 0.1933 Training Acc: 95.0439%\n",
      "[Epoch:  3/  5] [data: 16512/24058] Training Loss: 0.2543 Training Acc: 95.0218%\n",
      "[Epoch:  3/  5] [data: 16640/24058] Training Loss: 0.1802 Training Acc: 95.0180%\n",
      "[Epoch:  3/  5] [data: 16768/24058] Training Loss: 0.1325 Training Acc: 95.0322%\n",
      "[Epoch:  3/  5] [data: 16896/24058] Training Loss: 0.1857 Training Acc: 95.0343%\n",
      "[Epoch:  3/  5] [data: 17024/24058] Training Loss: 0.2842 Training Acc: 95.0012%\n",
      "[Epoch:  3/  5] [data: 17152/24058] Training Loss: 0.1778 Training Acc: 94.9977%\n",
      "[Epoch:  3/  5] [data: 17280/24058] Training Loss: 0.2524 Training Acc: 94.9826%\n",
      "[Epoch:  3/  5] [data: 17408/24058] Training Loss: 0.2291 Training Acc: 94.9736%\n",
      "[Epoch:  3/  5] [data: 17536/24058] Training Loss: 0.1159 Training Acc: 94.9875%\n",
      "[Epoch:  3/  5] [data: 17664/24058] Training Loss: 0.1560 Training Acc: 94.9955%\n",
      "[Epoch:  3/  5] [data: 17792/24058] Training Loss: 0.1478 Training Acc: 95.0034%\n",
      "[Epoch:  3/  5] [data: 17920/24058] Training Loss: 0.2192 Training Acc: 94.9944%\n",
      "[Epoch:  3/  5] [data: 18048/24058] Training Loss: 0.1275 Training Acc: 95.0022%\n",
      "[Epoch:  3/  5] [data: 18176/24058] Training Loss: 0.2799 Training Acc: 94.9769%\n",
      "[Epoch:  3/  5] [data: 18304/24058] Training Loss: 0.1773 Training Acc: 94.9792%\n",
      "[Epoch:  3/  5] [data: 18432/24058] Training Loss: 0.1374 Training Acc: 94.9870%\n",
      "[Epoch:  3/  5] [data: 18560/24058] Training Loss: 0.1816 Training Acc: 94.9838%\n",
      "[Epoch:  3/  5] [data: 18688/24058] Training Loss: 0.1020 Training Acc: 94.9968%\n",
      "[Epoch:  3/  5] [data: 18816/24058] Training Loss: 0.1474 Training Acc: 94.9989%\n",
      "[Epoch:  3/  5] [data: 18944/24058] Training Loss: 0.1800 Training Acc: 95.0011%\n",
      "[Epoch:  3/  5] [data: 19072/24058] Training Loss: 0.1357 Training Acc: 95.0084%\n",
      "[Epoch:  3/  5] [data: 19200/24058] Training Loss: 0.0910 Training Acc: 95.0260%\n",
      "[Epoch:  3/  5] [data: 19328/24058] Training Loss: 0.2113 Training Acc: 95.0124%\n",
      "[Epoch:  3/  5] [data: 19456/24058] Training Loss: 0.1725 Training Acc: 95.0144%\n",
      "[Epoch:  3/  5] [data: 19584/24058] Training Loss: 0.1117 Training Acc: 95.0266%\n",
      "[Epoch:  3/  5] [data: 19712/24058] Training Loss: 0.1514 Training Acc: 95.0335%\n",
      "[Epoch:  3/  5] [data: 19840/24058] Training Loss: 0.1306 Training Acc: 95.0403%\n",
      "[Epoch:  3/  5] [data: 19968/24058] Training Loss: 0.2089 Training Acc: 95.0321%\n",
      "[Epoch:  3/  5] [data: 20096/24058] Training Loss: 0.3211 Training Acc: 95.0040%\n",
      "[Epoch:  3/  5] [data: 20224/24058] Training Loss: 0.2336 Training Acc: 94.9862%\n",
      "[Epoch:  3/  5] [data: 20352/24058] Training Loss: 0.1347 Training Acc: 94.9931%\n",
      "[Epoch:  3/  5] [data: 20480/24058] Training Loss: 0.0900 Training Acc: 95.0146%\n",
      "[Epoch:  3/  5] [data: 20608/24058] Training Loss: 0.2749 Training Acc: 94.9874%\n",
      "[Epoch:  3/  5] [data: 20736/24058] Training Loss: 0.1266 Training Acc: 94.9942%\n",
      "[Epoch:  3/  5] [data: 20864/24058] Training Loss: 0.1176 Training Acc: 95.0010%\n",
      "[Epoch:  3/  5] [data: 20992/24058] Training Loss: 0.1924 Training Acc: 94.9886%\n",
      "[Epoch:  3/  5] [data: 21120/24058] Training Loss: 0.1956 Training Acc: 94.9763%\n",
      "[Epoch:  3/  5] [data: 21248/24058] Training Loss: 0.1239 Training Acc: 94.9878%\n",
      "[Epoch:  3/  5] [data: 21376/24058] Training Loss: 0.1519 Training Acc: 94.9897%\n",
      "[Epoch:  3/  5] [data: 21504/24058] Training Loss: 0.0798 Training Acc: 95.0056%\n",
      "[Epoch:  3/  5] [data: 21632/24058] Training Loss: 0.2724 Training Acc: 94.9889%\n",
      "[Epoch:  3/  5] [data: 21760/24058] Training Loss: 0.1737 Training Acc: 94.9908%\n",
      "[Epoch:  3/  5] [data: 21888/24058] Training Loss: 0.2257 Training Acc: 94.9790%\n",
      "[Epoch:  3/  5] [data: 22016/24058] Training Loss: 0.1855 Training Acc: 94.9764%\n",
      "[Epoch:  3/  5] [data: 22144/24058] Training Loss: 0.1974 Training Acc: 94.9738%\n",
      "[Epoch:  3/  5] [data: 22272/24058] Training Loss: 0.1449 Training Acc: 94.9847%\n",
      "[Epoch:  3/  5] [data: 22400/24058] Training Loss: 0.2002 Training Acc: 94.9821%\n",
      "[Epoch:  3/  5] [data: 22528/24058] Training Loss: 0.1878 Training Acc: 94.9796%\n",
      "[Epoch:  3/  5] [data: 22656/24058] Training Loss: 0.2353 Training Acc: 94.9726%\n",
      "[Epoch:  3/  5] [data: 22784/24058] Training Loss: 0.1678 Training Acc: 94.9745%\n",
      "[Epoch:  3/  5] [data: 22912/24058] Training Loss: 0.2000 Training Acc: 94.9721%\n",
      "[Epoch:  3/  5] [data: 23040/24058] Training Loss: 0.2348 Training Acc: 94.9609%\n",
      "[Epoch:  3/  5] [data: 23168/24058] Training Loss: 0.1529 Training Acc: 94.9629%\n",
      "[Epoch:  3/  5] [data: 23296/24058] Training Loss: 0.2072 Training Acc: 94.9605%\n",
      "[Epoch:  3/  5] [data: 23424/24058] Training Loss: 0.1102 Training Acc: 94.9710%\n",
      "[Epoch:  3/  5] [data: 23552/24058] Training Loss: 0.1512 Training Acc: 94.9728%\n",
      "[Epoch:  3/  5] [data: 23680/24058] Training Loss: 0.2034 Training Acc: 94.9662%\n",
      "[Epoch:  3/  5] [data: 23808/24058] Training Loss: 0.0630 Training Acc: 94.9891%\n",
      "[Epoch:  3/  5] [data: 23936/24058] Training Loss: 0.1463 Training Acc: 94.9950%\n",
      "[Epoch:  3/  5] [data: 24058/24058] Training Loss: 0.1695 Training Acc: 94.9996%\n",
      "Testing-4...\n",
      "[Epoch:  3/  5] Validation Loss: 0.1795 Validation Acc: 94.7575%\n",
      "Time used: 1174.888810634613s\n",
      "[Epoch:  4/  5] [data: 128/24058] Training Loss: 0.2075 Training Acc: 92.9688%\n",
      "[Epoch:  4/  5] [data: 256/24058] Training Loss: 0.2728 Training Acc: 91.4062%\n",
      "[Epoch:  4/  5] [data: 384/24058] Training Loss: 0.2898 Training Acc: 91.6667%\n",
      "[Epoch:  4/  5] [data: 512/24058] Training Loss: 0.0964 Training Acc: 93.1641%\n",
      "[Epoch:  4/  5] [data: 640/24058] Training Loss: 0.1872 Training Acc: 93.4375%\n",
      "[Epoch:  4/  5] [data: 768/24058] Training Loss: 0.2164 Training Acc: 93.4896%\n",
      "[Epoch:  4/  5] [data: 896/24058] Training Loss: 0.0580 Training Acc: 94.3080%\n",
      "[Epoch:  4/  5] [data: 1024/24058] Training Loss: 0.1542 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 1152/24058] Training Loss: 0.1763 Training Acc: 94.6181%\n",
      "[Epoch:  4/  5] [data: 1280/24058] Training Loss: 0.4469 Training Acc: 93.8281%\n",
      "[Epoch:  4/  5] [data: 1408/24058] Training Loss: 0.1568 Training Acc: 93.8920%\n",
      "[Epoch:  4/  5] [data: 1536/24058] Training Loss: 0.0937 Training Acc: 94.2057%\n",
      "[Epoch:  4/  5] [data: 1664/24058] Training Loss: 0.2480 Training Acc: 94.0505%\n",
      "[Epoch:  4/  5] [data: 1792/24058] Training Loss: 0.2615 Training Acc: 93.8616%\n",
      "[Epoch:  4/  5] [data: 1920/24058] Training Loss: 0.1061 Training Acc: 94.0625%\n",
      "[Epoch:  4/  5] [data: 2048/24058] Training Loss: 0.2001 Training Acc: 94.0918%\n",
      "[Epoch:  4/  5] [data: 2176/24058] Training Loss: 0.0861 Training Acc: 94.3015%\n",
      "[Epoch:  4/  5] [data: 2304/24058] Training Loss: 0.2362 Training Acc: 94.2274%\n",
      "[Epoch:  4/  5] [data: 2432/24058] Training Loss: 0.0709 Training Acc: 94.3668%\n",
      "[Epoch:  4/  5] [data: 2560/24058] Training Loss: 0.1933 Training Acc: 94.3359%\n",
      "[Epoch:  4/  5] [data: 2688/24058] Training Loss: 0.0733 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 2816/24058] Training Loss: 0.1610 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 2944/24058] Training Loss: 0.0721 Training Acc: 94.7011%\n",
      "[Epoch:  4/  5] [data: 3072/24058] Training Loss: 0.2575 Training Acc: 94.5964%\n",
      "[Epoch:  4/  5] [data: 3200/24058] Training Loss: 0.2011 Training Acc: 94.5938%\n",
      "[Epoch:  4/  5] [data: 3328/24058] Training Loss: 0.1164 Training Acc: 94.6815%\n",
      "[Epoch:  4/  5] [data: 3456/24058] Training Loss: 0.1235 Training Acc: 94.7627%\n",
      "[Epoch:  4/  5] [data: 3584/24058] Training Loss: 0.1083 Training Acc: 94.8382%\n",
      "[Epoch:  4/  5] [data: 3712/24058] Training Loss: 0.2863 Training Acc: 94.6929%\n",
      "[Epoch:  4/  5] [data: 3840/24058] Training Loss: 0.0798 Training Acc: 94.7917%\n",
      "[Epoch:  4/  5] [data: 3968/24058] Training Loss: 0.1241 Training Acc: 94.8589%\n",
      "[Epoch:  4/  5] [data: 4096/24058] Training Loss: 0.2153 Training Acc: 94.8242%\n",
      "[Epoch:  4/  5] [data: 4224/24058] Training Loss: 0.1240 Training Acc: 94.8864%\n",
      "[Epoch:  4/  5] [data: 4352/24058] Training Loss: 0.0617 Training Acc: 94.9678%\n",
      "[Epoch:  4/  5] [data: 4480/24058] Training Loss: 0.2080 Training Acc: 94.8884%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 4608/24058] Training Loss: 0.0657 Training Acc: 94.9653%\n",
      "[Epoch:  4/  5] [data: 4736/24058] Training Loss: 0.2302 Training Acc: 94.9324%\n",
      "[Epoch:  4/  5] [data: 4864/24058] Training Loss: 0.0980 Training Acc: 95.0041%\n",
      "[Epoch:  4/  5] [data: 4992/24058] Training Loss: 0.1058 Training Acc: 95.0321%\n",
      "[Epoch:  4/  5] [data: 5120/24058] Training Loss: 0.1800 Training Acc: 95.0000%\n",
      "[Epoch:  4/  5] [data: 5248/24058] Training Loss: 0.1736 Training Acc: 95.0076%\n",
      "[Epoch:  4/  5] [data: 5376/24058] Training Loss: 0.2000 Training Acc: 94.9963%\n",
      "[Epoch:  4/  5] [data: 5504/24058] Training Loss: 0.1123 Training Acc: 95.0581%\n",
      "[Epoch:  4/  5] [data: 5632/24058] Training Loss: 0.1724 Training Acc: 95.0639%\n",
      "[Epoch:  4/  5] [data: 5760/24058] Training Loss: 0.1544 Training Acc: 95.0694%\n",
      "[Epoch:  4/  5] [data: 5888/24058] Training Loss: 0.1507 Training Acc: 95.0917%\n",
      "[Epoch:  4/  5] [data: 6016/24058] Training Loss: 0.1678 Training Acc: 95.0798%\n",
      "[Epoch:  4/  5] [data: 6144/24058] Training Loss: 0.1387 Training Acc: 95.1009%\n",
      "[Epoch:  4/  5] [data: 6272/24058] Training Loss: 0.2085 Training Acc: 95.0733%\n",
      "[Epoch:  4/  5] [data: 6400/24058] Training Loss: 0.2108 Training Acc: 95.0469%\n",
      "[Epoch:  4/  5] [data: 6528/24058] Training Loss: 0.1494 Training Acc: 95.0674%\n",
      "[Epoch:  4/  5] [data: 6656/24058] Training Loss: 0.1751 Training Acc: 95.0721%\n",
      "[Epoch:  4/  5] [data: 6784/24058] Training Loss: 0.1268 Training Acc: 95.1061%\n",
      "[Epoch:  4/  5] [data: 6912/24058] Training Loss: 0.1936 Training Acc: 95.0955%\n",
      "[Epoch:  4/  5] [data: 7040/24058] Training Loss: 0.1658 Training Acc: 95.0994%\n",
      "[Epoch:  4/  5] [data: 7168/24058] Training Loss: 0.1812 Training Acc: 95.1032%\n",
      "[Epoch:  4/  5] [data: 7296/24058] Training Loss: 0.0774 Training Acc: 95.1617%\n",
      "[Epoch:  4/  5] [data: 7424/24058] Training Loss: 0.1049 Training Acc: 95.1913%\n",
      "[Epoch:  4/  5] [data: 7552/24058] Training Loss: 0.0727 Training Acc: 95.2331%\n",
      "[Epoch:  4/  5] [data: 7680/24058] Training Loss: 0.1860 Training Acc: 95.2214%\n",
      "[Epoch:  4/  5] [data: 7808/24058] Training Loss: 0.1970 Training Acc: 95.2100%\n",
      "[Epoch:  4/  5] [data: 7936/24058] Training Loss: 0.3061 Training Acc: 95.1613%\n",
      "[Epoch:  4/  5] [data: 8064/24058] Training Loss: 0.2725 Training Acc: 95.1141%\n",
      "[Epoch:  4/  5] [data: 8192/24058] Training Loss: 0.0770 Training Acc: 95.1538%\n",
      "[Epoch:  4/  5] [data: 8320/24058] Training Loss: 0.1793 Training Acc: 95.1562%\n",
      "[Epoch:  4/  5] [data: 8448/24058] Training Loss: 0.2371 Training Acc: 95.1113%\n",
      "[Epoch:  4/  5] [data: 8576/24058] Training Loss: 0.0982 Training Acc: 95.1609%\n",
      "[Epoch:  4/  5] [data: 8704/24058] Training Loss: 0.1882 Training Acc: 95.1402%\n",
      "[Epoch:  4/  5] [data: 8832/24058] Training Loss: 0.2708 Training Acc: 95.0974%\n",
      "[Epoch:  4/  5] [data: 8960/24058] Training Loss: 0.2714 Training Acc: 95.0558%\n",
      "[Epoch:  4/  5] [data: 9088/24058] Training Loss: 0.1691 Training Acc: 95.0484%\n",
      "[Epoch:  4/  5] [data: 9216/24058] Training Loss: 0.1277 Training Acc: 95.0629%\n",
      "[Epoch:  4/  5] [data: 9344/24058] Training Loss: 0.1837 Training Acc: 95.0664%\n",
      "[Epoch:  4/  5] [data: 9472/24058] Training Loss: 0.2141 Training Acc: 95.0486%\n",
      "[Epoch:  4/  5] [data: 9600/24058] Training Loss: 0.2693 Training Acc: 95.0104%\n",
      "[Epoch:  4/  5] [data: 9728/24058] Training Loss: 0.1765 Training Acc: 95.0144%\n",
      "[Epoch:  4/  5] [data: 9856/24058] Training Loss: 0.1888 Training Acc: 95.0081%\n",
      "[Epoch:  4/  5] [data: 9984/24058] Training Loss: 0.1587 Training Acc: 95.0020%\n",
      "[Epoch:  4/  5] [data: 10112/24058] Training Loss: 0.2259 Training Acc: 94.9862%\n",
      "[Epoch:  4/  5] [data: 10240/24058] Training Loss: 0.1463 Training Acc: 94.9609%\n",
      "[Epoch:  4/  5] [data: 10368/24058] Training Loss: 0.1521 Training Acc: 94.9653%\n",
      "[Epoch:  4/  5] [data: 10496/24058] Training Loss: 0.1241 Training Acc: 94.9886%\n",
      "[Epoch:  4/  5] [data: 10624/24058] Training Loss: 0.0875 Training Acc: 95.0113%\n",
      "[Epoch:  4/  5] [data: 10752/24058] Training Loss: 0.1209 Training Acc: 95.0335%\n",
      "[Epoch:  4/  5] [data: 10880/24058] Training Loss: 0.1695 Training Acc: 95.0368%\n",
      "[Epoch:  4/  5] [data: 11008/24058] Training Loss: 0.2448 Training Acc: 95.0036%\n",
      "[Epoch:  4/  5] [data: 11136/24058] Training Loss: 0.2063 Training Acc: 94.9892%\n",
      "[Epoch:  4/  5] [data: 11264/24058] Training Loss: 0.1941 Training Acc: 94.9751%\n",
      "[Epoch:  4/  5] [data: 11392/24058] Training Loss: 0.0710 Training Acc: 95.0140%\n",
      "[Epoch:  4/  5] [data: 11520/24058] Training Loss: 0.1557 Training Acc: 95.0087%\n",
      "[Epoch:  4/  5] [data: 11648/24058] Training Loss: 0.1821 Training Acc: 95.0034%\n",
      "[Epoch:  4/  5] [data: 11776/24058] Training Loss: 0.2770 Training Acc: 94.9643%\n",
      "[Epoch:  4/  5] [data: 11904/24058] Training Loss: 0.1555 Training Acc: 94.9765%\n",
      "[Epoch:  4/  5] [data: 12032/24058] Training Loss: 0.1364 Training Acc: 94.9801%\n",
      "[Epoch:  4/  5] [data: 12160/24058] Training Loss: 0.1586 Training Acc: 94.9836%\n",
      "[Epoch:  4/  5] [data: 12288/24058] Training Loss: 0.1591 Training Acc: 94.9951%\n",
      "[Epoch:  4/  5] [data: 12416/24058] Training Loss: 0.2605 Training Acc: 94.9581%\n",
      "[Epoch:  4/  5] [data: 12544/24058] Training Loss: 0.1449 Training Acc: 94.9697%\n",
      "[Epoch:  4/  5] [data: 12672/24058] Training Loss: 0.2152 Training Acc: 94.9495%\n",
      "[Epoch:  4/  5] [data: 12800/24058] Training Loss: 0.2236 Training Acc: 94.9375%\n",
      "[Epoch:  4/  5] [data: 12928/24058] Training Loss: 0.1231 Training Acc: 94.9567%\n",
      "[Epoch:  4/  5] [data: 13056/24058] Training Loss: 0.2000 Training Acc: 94.9525%\n",
      "[Epoch:  4/  5] [data: 13184/24058] Training Loss: 0.1768 Training Acc: 94.9560%\n",
      "[Epoch:  4/  5] [data: 13312/24058] Training Loss: 0.0998 Training Acc: 94.9820%\n",
      "[Epoch:  4/  5] [data: 13440/24058] Training Loss: 0.0785 Training Acc: 95.0149%\n",
      "[Epoch:  4/  5] [data: 13568/24058] Training Loss: 0.1789 Training Acc: 95.0177%\n",
      "[Epoch:  4/  5] [data: 13696/24058] Training Loss: 0.1815 Training Acc: 95.0131%\n",
      "[Epoch:  4/  5] [data: 13824/24058] Training Loss: 0.1308 Training Acc: 95.0231%\n",
      "[Epoch:  4/  5] [data: 13952/24058] Training Loss: 0.1655 Training Acc: 95.0258%\n",
      "[Epoch:  4/  5] [data: 14080/24058] Training Loss: 0.1554 Training Acc: 95.0355%\n",
      "[Epoch:  4/  5] [data: 14208/24058] Training Loss: 0.2600 Training Acc: 95.0169%\n",
      "[Epoch:  4/  5] [data: 14336/24058] Training Loss: 0.1236 Training Acc: 95.0335%\n",
      "[Epoch:  4/  5] [data: 14464/24058] Training Loss: 0.0674 Training Acc: 95.0636%\n",
      "[Epoch:  4/  5] [data: 14592/24058] Training Loss: 0.1685 Training Acc: 95.0658%\n",
      "[Epoch:  4/  5] [data: 14720/24058] Training Loss: 0.0694 Training Acc: 95.0951%\n",
      "[Epoch:  4/  5] [data: 14848/24058] Training Loss: 0.1725 Training Acc: 95.0902%\n",
      "[Epoch:  4/  5] [data: 14976/24058] Training Loss: 0.1494 Training Acc: 95.0988%\n",
      "[Epoch:  4/  5] [data: 15104/24058] Training Loss: 0.1174 Training Acc: 95.1073%\n",
      "[Epoch:  4/  5] [data: 15232/24058] Training Loss: 0.1483 Training Acc: 95.1155%\n",
      "[Epoch:  4/  5] [data: 15360/24058] Training Loss: 0.1269 Training Acc: 95.1302%\n",
      "[Epoch:  4/  5] [data: 15488/24058] Training Loss: 0.1369 Training Acc: 95.1317%\n",
      "[Epoch:  4/  5] [data: 15616/24058] Training Loss: 0.1737 Training Acc: 95.1332%\n",
      "[Epoch:  4/  5] [data: 15744/24058] Training Loss: 0.2376 Training Acc: 95.1220%\n",
      "[Epoch:  4/  5] [data: 15872/24058] Training Loss: 0.1314 Training Acc: 95.1361%\n",
      "[Epoch:  4/  5] [data: 16000/24058] Training Loss: 0.1889 Training Acc: 95.1375%\n",
      "[Epoch:  4/  5] [data: 16128/24058] Training Loss: 0.1325 Training Acc: 95.1451%\n",
      "[Epoch:  4/  5] [data: 16256/24058] Training Loss: 0.1971 Training Acc: 95.1403%\n",
      "[Epoch:  4/  5] [data: 16384/24058] Training Loss: 0.1831 Training Acc: 95.1416%\n",
      "[Epoch:  4/  5] [data: 16512/24058] Training Loss: 0.2489 Training Acc: 95.1187%\n",
      "[Epoch:  4/  5] [data: 16640/24058] Training Loss: 0.1785 Training Acc: 95.1142%\n",
      "[Epoch:  4/  5] [data: 16768/24058] Training Loss: 0.1314 Training Acc: 95.1276%\n",
      "[Epoch:  4/  5] [data: 16896/24058] Training Loss: 0.1841 Training Acc: 95.1290%\n",
      "[Epoch:  4/  5] [data: 17024/24058] Training Loss: 0.2840 Training Acc: 95.1069%\n",
      "[Epoch:  4/  5] [data: 17152/24058] Training Loss: 0.1765 Training Acc: 95.1026%\n",
      "[Epoch:  4/  5] [data: 17280/24058] Training Loss: 0.2518 Training Acc: 95.0868%\n",
      "[Epoch:  4/  5] [data: 17408/24058] Training Loss: 0.2288 Training Acc: 95.0770%\n",
      "[Epoch:  4/  5] [data: 17536/24058] Training Loss: 0.1058 Training Acc: 95.0901%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 17664/24058] Training Loss: 0.1527 Training Acc: 95.0974%\n",
      "[Epoch:  4/  5] [data: 17792/24058] Training Loss: 0.1440 Training Acc: 95.1045%\n",
      "[Epoch:  4/  5] [data: 17920/24058] Training Loss: 0.2131 Training Acc: 95.0949%\n",
      "[Epoch:  4/  5] [data: 18048/24058] Training Loss: 0.1250 Training Acc: 95.1075%\n",
      "[Epoch:  4/  5] [data: 18176/24058] Training Loss: 0.2821 Training Acc: 95.0814%\n",
      "[Epoch:  4/  5] [data: 18304/24058] Training Loss: 0.1741 Training Acc: 95.0830%\n",
      "[Epoch:  4/  5] [data: 18432/24058] Training Loss: 0.1328 Training Acc: 95.0901%\n",
      "[Epoch:  4/  5] [data: 18560/24058] Training Loss: 0.1826 Training Acc: 95.0862%\n",
      "[Epoch:  4/  5] [data: 18688/24058] Training Loss: 0.0976 Training Acc: 95.1038%\n",
      "[Epoch:  4/  5] [data: 18816/24058] Training Loss: 0.1512 Training Acc: 95.1052%\n",
      "[Epoch:  4/  5] [data: 18944/24058] Training Loss: 0.1781 Training Acc: 95.1066%\n",
      "[Epoch:  4/  5] [data: 19072/24058] Training Loss: 0.1286 Training Acc: 95.1133%\n",
      "[Epoch:  4/  5] [data: 19200/24058] Training Loss: 0.0934 Training Acc: 95.1302%\n",
      "[Epoch:  4/  5] [data: 19328/24058] Training Loss: 0.2073 Training Acc: 95.1159%\n",
      "[Epoch:  4/  5] [data: 19456/24058] Training Loss: 0.1709 Training Acc: 95.1172%\n",
      "[Epoch:  4/  5] [data: 19584/24058] Training Loss: 0.1063 Training Acc: 95.1287%\n",
      "[Epoch:  4/  5] [data: 19712/24058] Training Loss: 0.1498 Training Acc: 95.1349%\n",
      "[Epoch:  4/  5] [data: 19840/24058] Training Loss: 0.1280 Training Acc: 95.1411%\n",
      "[Epoch:  4/  5] [data: 19968/24058] Training Loss: 0.2047 Training Acc: 95.1322%\n",
      "[Epoch:  4/  5] [data: 20096/24058] Training Loss: 0.3153 Training Acc: 95.1035%\n",
      "[Epoch:  4/  5] [data: 20224/24058] Training Loss: 0.2312 Training Acc: 95.0850%\n",
      "[Epoch:  4/  5] [data: 20352/24058] Training Loss: 0.1294 Training Acc: 95.0914%\n",
      "[Epoch:  4/  5] [data: 20480/24058] Training Loss: 0.0846 Training Acc: 95.1123%\n",
      "[Epoch:  4/  5] [data: 20608/24058] Training Loss: 0.2636 Training Acc: 95.0844%\n",
      "[Epoch:  4/  5] [data: 20736/24058] Training Loss: 0.1223 Training Acc: 95.0907%\n",
      "[Epoch:  4/  5] [data: 20864/24058] Training Loss: 0.1090 Training Acc: 95.0968%\n",
      "[Epoch:  4/  5] [data: 20992/24058] Training Loss: 0.1846 Training Acc: 95.0886%\n",
      "[Epoch:  4/  5] [data: 21120/24058] Training Loss: 0.1892 Training Acc: 95.0852%\n",
      "[Epoch:  4/  5] [data: 21248/24058] Training Loss: 0.1222 Training Acc: 95.0960%\n",
      "[Epoch:  4/  5] [data: 21376/24058] Training Loss: 0.1494 Training Acc: 95.0973%\n",
      "[Epoch:  4/  5] [data: 21504/24058] Training Loss: 0.0719 Training Acc: 95.1172%\n",
      "[Epoch:  4/  5] [data: 21632/24058] Training Loss: 0.2693 Training Acc: 95.0999%\n",
      "[Epoch:  4/  5] [data: 21760/24058] Training Loss: 0.1717 Training Acc: 95.1011%\n",
      "[Epoch:  4/  5] [data: 21888/24058] Training Loss: 0.2264 Training Acc: 95.0932%\n",
      "[Epoch:  4/  5] [data: 22016/24058] Training Loss: 0.1821 Training Acc: 95.0899%\n",
      "[Epoch:  4/  5] [data: 22144/24058] Training Loss: 0.1962 Training Acc: 95.0867%\n",
      "[Epoch:  4/  5] [data: 22272/24058] Training Loss: 0.1420 Training Acc: 95.0970%\n",
      "[Epoch:  4/  5] [data: 22400/24058] Training Loss: 0.2018 Training Acc: 95.0938%\n",
      "[Epoch:  4/  5] [data: 22528/24058] Training Loss: 0.1829 Training Acc: 95.0906%\n",
      "[Epoch:  4/  5] [data: 22656/24058] Training Loss: 0.2298 Training Acc: 95.0830%\n",
      "[Epoch:  4/  5] [data: 22784/24058] Training Loss: 0.1596 Training Acc: 95.0843%\n",
      "[Epoch:  4/  5] [data: 22912/24058] Training Loss: 0.1931 Training Acc: 95.0812%\n",
      "[Epoch:  4/  5] [data: 23040/24058] Training Loss: 0.2235 Training Acc: 95.0738%\n",
      "[Epoch:  4/  5] [data: 23168/24058] Training Loss: 0.1377 Training Acc: 95.0751%\n",
      "[Epoch:  4/  5] [data: 23296/24058] Training Loss: 0.2040 Training Acc: 95.0678%\n",
      "[Epoch:  4/  5] [data: 23424/24058] Training Loss: 0.1066 Training Acc: 95.0777%\n",
      "[Epoch:  4/  5] [data: 23552/24058] Training Loss: 0.1459 Training Acc: 95.0790%\n",
      "[Epoch:  4/  5] [data: 23680/24058] Training Loss: 0.1934 Training Acc: 95.0718%\n",
      "[Epoch:  4/  5] [data: 23808/24058] Training Loss: 0.0548 Training Acc: 95.0941%\n",
      "[Epoch:  4/  5] [data: 23936/24058] Training Loss: 0.1463 Training Acc: 95.0994%\n",
      "[Epoch:  4/  5] [data: 24058/24058] Training Loss: 0.1692 Training Acc: 95.1035%\n",
      "Testing-4...\n",
      "[Epoch:  4/  5] Validation Loss: 0.1756 Validation Acc: 94.9090%\n",
      "Time used: 1216.7156257629395s\n",
      "[Epoch:  5/  5] [data: 128/24058] Training Loss: 0.1982 Training Acc: 92.9688%\n",
      "[Epoch:  5/  5] [data: 256/24058] Training Loss: 0.2634 Training Acc: 91.4062%\n",
      "[Epoch:  5/  5] [data: 384/24058] Training Loss: 0.2814 Training Acc: 91.6667%\n",
      "[Epoch:  5/  5] [data: 512/24058] Training Loss: 0.1007 Training Acc: 93.1641%\n",
      "[Epoch:  5/  5] [data: 640/24058] Training Loss: 0.1799 Training Acc: 93.4375%\n",
      "[Epoch:  5/  5] [data: 768/24058] Training Loss: 0.2146 Training Acc: 93.4896%\n",
      "[Epoch:  5/  5] [data: 896/24058] Training Loss: 0.0519 Training Acc: 94.3080%\n",
      "[Epoch:  5/  5] [data: 1024/24058] Training Loss: 0.1520 Training Acc: 94.5312%\n",
      "[Epoch:  5/  5] [data: 1152/24058] Training Loss: 0.1755 Training Acc: 94.6181%\n",
      "[Epoch:  5/  5] [data: 1280/24058] Training Loss: 0.4457 Training Acc: 93.8281%\n",
      "[Epoch:  5/  5] [data: 1408/24058] Training Loss: 0.1531 Training Acc: 94.0341%\n",
      "[Epoch:  5/  5] [data: 1536/24058] Training Loss: 0.0886 Training Acc: 94.3359%\n",
      "[Epoch:  5/  5] [data: 1664/24058] Training Loss: 0.2449 Training Acc: 94.1707%\n",
      "[Epoch:  5/  5] [data: 1792/24058] Training Loss: 0.2628 Training Acc: 93.9732%\n",
      "[Epoch:  5/  5] [data: 1920/24058] Training Loss: 0.1027 Training Acc: 94.1667%\n",
      "[Epoch:  5/  5] [data: 2048/24058] Training Loss: 0.1978 Training Acc: 94.1895%\n",
      "[Epoch:  5/  5] [data: 2176/24058] Training Loss: 0.0847 Training Acc: 94.3934%\n",
      "[Epoch:  5/  5] [data: 2304/24058] Training Loss: 0.2282 Training Acc: 94.3142%\n",
      "[Epoch:  5/  5] [data: 2432/24058] Training Loss: 0.0749 Training Acc: 94.4490%\n",
      "[Epoch:  5/  5] [data: 2560/24058] Training Loss: 0.1849 Training Acc: 94.4531%\n",
      "[Epoch:  5/  5] [data: 2688/24058] Training Loss: 0.0726 Training Acc: 94.6429%\n",
      "[Epoch:  5/  5] [data: 2816/24058] Training Loss: 0.1580 Training Acc: 94.6733%\n",
      "[Epoch:  5/  5] [data: 2944/24058] Training Loss: 0.0682 Training Acc: 94.8370%\n",
      "[Epoch:  5/  5] [data: 3072/24058] Training Loss: 0.2611 Training Acc: 94.7266%\n",
      "[Epoch:  5/  5] [data: 3200/24058] Training Loss: 0.2010 Training Acc: 94.7188%\n",
      "[Epoch:  5/  5] [data: 3328/24058] Training Loss: 0.1141 Training Acc: 94.8017%\n",
      "[Epoch:  5/  5] [data: 3456/24058] Training Loss: 0.1238 Training Acc: 94.8785%\n",
      "[Epoch:  5/  5] [data: 3584/24058] Training Loss: 0.1061 Training Acc: 94.9498%\n",
      "[Epoch:  5/  5] [data: 3712/24058] Training Loss: 0.2785 Training Acc: 94.8006%\n",
      "[Epoch:  5/  5] [data: 3840/24058] Training Loss: 0.0791 Training Acc: 94.9219%\n",
      "[Epoch:  5/  5] [data: 3968/24058] Training Loss: 0.1203 Training Acc: 94.9849%\n",
      "[Epoch:  5/  5] [data: 4096/24058] Training Loss: 0.2142 Training Acc: 94.9463%\n",
      "[Epoch:  5/  5] [data: 4224/24058] Training Loss: 0.1204 Training Acc: 95.0047%\n",
      "[Epoch:  5/  5] [data: 4352/24058] Training Loss: 0.0600 Training Acc: 95.1057%\n",
      "[Epoch:  5/  5] [data: 4480/24058] Training Loss: 0.1909 Training Acc: 95.0223%\n",
      "[Epoch:  5/  5] [data: 4608/24058] Training Loss: 0.0643 Training Acc: 95.1172%\n",
      "[Epoch:  5/  5] [data: 4736/24058] Training Loss: 0.2282 Training Acc: 95.1014%\n",
      "[Epoch:  5/  5] [data: 4864/24058] Training Loss: 0.0969 Training Acc: 95.1686%\n",
      "[Epoch:  5/  5] [data: 4992/24058] Training Loss: 0.1038 Training Acc: 95.1923%\n",
      "[Epoch:  5/  5] [data: 5120/24058] Training Loss: 0.1794 Training Acc: 95.1953%\n",
      "[Epoch:  5/  5] [data: 5248/24058] Training Loss: 0.1727 Training Acc: 95.1982%\n",
      "[Epoch:  5/  5] [data: 5376/24058] Training Loss: 0.1979 Training Acc: 95.1823%\n",
      "[Epoch:  5/  5] [data: 5504/24058] Training Loss: 0.1073 Training Acc: 95.2398%\n",
      "[Epoch:  5/  5] [data: 5632/24058] Training Loss: 0.1684 Training Acc: 95.2592%\n",
      "[Epoch:  5/  5] [data: 5760/24058] Training Loss: 0.1494 Training Acc: 95.2778%\n",
      "[Epoch:  5/  5] [data: 5888/24058] Training Loss: 0.1494 Training Acc: 95.2955%\n",
      "[Epoch:  5/  5] [data: 6016/24058] Training Loss: 0.1568 Training Acc: 95.2959%\n",
      "[Epoch:  5/  5] [data: 6144/24058] Training Loss: 0.1326 Training Acc: 95.3125%\n",
      "[Epoch:  5/  5] [data: 6272/24058] Training Loss: 0.2038 Training Acc: 95.2806%\n",
      "[Epoch:  5/  5] [data: 6400/24058] Training Loss: 0.2064 Training Acc: 95.2500%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 6528/24058] Training Loss: 0.1483 Training Acc: 95.2665%\n",
      "[Epoch:  5/  5] [data: 6656/24058] Training Loss: 0.1728 Training Acc: 95.2674%\n",
      "[Epoch:  5/  5] [data: 6784/24058] Training Loss: 0.1244 Training Acc: 95.2978%\n",
      "[Epoch:  5/  5] [data: 6912/24058] Training Loss: 0.1958 Training Acc: 95.2836%\n",
      "[Epoch:  5/  5] [data: 7040/24058] Training Loss: 0.1636 Training Acc: 95.2841%\n",
      "[Epoch:  5/  5] [data: 7168/24058] Training Loss: 0.1840 Training Acc: 95.2846%\n",
      "[Epoch:  5/  5] [data: 7296/24058] Training Loss: 0.0757 Training Acc: 95.3399%\n",
      "[Epoch:  5/  5] [data: 7424/24058] Training Loss: 0.1062 Training Acc: 95.3664%\n",
      "[Epoch:  5/  5] [data: 7552/24058] Training Loss: 0.0714 Training Acc: 95.4052%\n",
      "[Epoch:  5/  5] [data: 7680/24058] Training Loss: 0.1809 Training Acc: 95.3906%\n",
      "[Epoch:  5/  5] [data: 7808/24058] Training Loss: 0.1895 Training Acc: 95.3765%\n",
      "[Epoch:  5/  5] [data: 7936/24058] Training Loss: 0.2968 Training Acc: 95.3251%\n",
      "[Epoch:  5/  5] [data: 8064/24058] Training Loss: 0.2629 Training Acc: 95.2877%\n",
      "[Epoch:  5/  5] [data: 8192/24058] Training Loss: 0.0728 Training Acc: 95.3247%\n",
      "[Epoch:  5/  5] [data: 8320/24058] Training Loss: 0.1740 Training Acc: 95.3245%\n",
      "[Epoch:  5/  5] [data: 8448/24058] Training Loss: 0.2341 Training Acc: 95.2770%\n",
      "[Epoch:  5/  5] [data: 8576/24058] Training Loss: 0.0790 Training Acc: 95.3242%\n",
      "[Epoch:  5/  5] [data: 8704/24058] Training Loss: 0.1844 Training Acc: 95.3010%\n",
      "[Epoch:  5/  5] [data: 8832/24058] Training Loss: 0.2657 Training Acc: 95.2559%\n",
      "[Epoch:  5/  5] [data: 8960/24058] Training Loss: 0.2667 Training Acc: 95.2121%\n",
      "[Epoch:  5/  5] [data: 9088/24058] Training Loss: 0.1713 Training Acc: 95.2025%\n",
      "[Epoch:  5/  5] [data: 9216/24058] Training Loss: 0.1256 Training Acc: 95.2148%\n",
      "[Epoch:  5/  5] [data: 9344/24058] Training Loss: 0.1803 Training Acc: 95.2055%\n",
      "[Epoch:  5/  5] [data: 9472/24058] Training Loss: 0.2136 Training Acc: 95.1964%\n",
      "[Epoch:  5/  5] [data: 9600/24058] Training Loss: 0.2609 Training Acc: 95.1667%\n",
      "[Epoch:  5/  5] [data: 9728/24058] Training Loss: 0.1766 Training Acc: 95.1789%\n",
      "[Epoch:  5/  5] [data: 9856/24058] Training Loss: 0.1922 Training Acc: 95.1705%\n",
      "[Epoch:  5/  5] [data: 9984/24058] Training Loss: 0.1553 Training Acc: 95.1623%\n",
      "[Epoch:  5/  5] [data: 10112/24058] Training Loss: 0.2274 Training Acc: 95.1444%\n",
      "[Epoch:  5/  5] [data: 10240/24058] Training Loss: 0.1363 Training Acc: 95.1465%\n",
      "[Epoch:  5/  5] [data: 10368/24058] Training Loss: 0.1501 Training Acc: 95.1582%\n",
      "[Epoch:  5/  5] [data: 10496/24058] Training Loss: 0.1230 Training Acc: 95.1791%\n",
      "[Epoch:  5/  5] [data: 10624/24058] Training Loss: 0.0811 Training Acc: 95.1995%\n",
      "[Epoch:  5/  5] [data: 10752/24058] Training Loss: 0.1205 Training Acc: 95.2195%\n",
      "[Epoch:  5/  5] [data: 10880/24058] Training Loss: 0.1666 Training Acc: 95.2298%\n",
      "[Epoch:  5/  5] [data: 11008/24058] Training Loss: 0.2323 Training Acc: 95.1944%\n",
      "[Epoch:  5/  5] [data: 11136/24058] Training Loss: 0.2074 Training Acc: 95.1868%\n",
      "[Epoch:  5/  5] [data: 11264/24058] Training Loss: 0.1938 Training Acc: 95.1705%\n",
      "[Epoch:  5/  5] [data: 11392/24058] Training Loss: 0.0737 Training Acc: 95.2072%\n",
      "[Epoch:  5/  5] [data: 11520/24058] Training Loss: 0.1521 Training Acc: 95.1997%\n",
      "[Epoch:  5/  5] [data: 11648/24058] Training Loss: 0.1814 Training Acc: 95.2009%\n",
      "[Epoch:  5/  5] [data: 11776/24058] Training Loss: 0.2810 Training Acc: 95.1596%\n",
      "[Epoch:  5/  5] [data: 11904/24058] Training Loss: 0.1504 Training Acc: 95.1697%\n",
      "[Epoch:  5/  5] [data: 12032/24058] Training Loss: 0.1342 Training Acc: 95.1712%\n",
      "[Epoch:  5/  5] [data: 12160/24058] Training Loss: 0.1468 Training Acc: 95.1727%\n",
      "[Epoch:  5/  5] [data: 12288/24058] Training Loss: 0.1536 Training Acc: 95.1823%\n",
      "[Epoch:  5/  5] [data: 12416/24058] Training Loss: 0.2574 Training Acc: 95.1434%\n",
      "[Epoch:  5/  5] [data: 12544/24058] Training Loss: 0.1448 Training Acc: 95.1531%\n",
      "[Epoch:  5/  5] [data: 12672/24058] Training Loss: 0.2146 Training Acc: 95.1310%\n",
      "[Epoch:  5/  5] [data: 12800/24058] Training Loss: 0.2255 Training Acc: 95.1172%\n",
      "[Epoch:  5/  5] [data: 12928/24058] Training Loss: 0.1269 Training Acc: 95.1346%\n",
      "[Epoch:  5/  5] [data: 13056/24058] Training Loss: 0.2010 Training Acc: 95.1287%\n",
      "[Epoch:  5/  5] [data: 13184/24058] Training Loss: 0.1769 Training Acc: 95.1305%\n",
      "[Epoch:  5/  5] [data: 13312/24058] Training Loss: 0.1025 Training Acc: 95.1547%\n",
      "[Epoch:  5/  5] [data: 13440/24058] Training Loss: 0.0780 Training Acc: 95.1860%\n",
      "[Epoch:  5/  5] [data: 13568/24058] Training Loss: 0.1768 Training Acc: 95.1872%\n",
      "[Epoch:  5/  5] [data: 13696/24058] Training Loss: 0.1828 Training Acc: 95.1811%\n",
      "[Epoch:  5/  5] [data: 13824/24058] Training Loss: 0.1315 Training Acc: 95.1895%\n",
      "[Epoch:  5/  5] [data: 13952/24058] Training Loss: 0.1605 Training Acc: 95.1907%\n",
      "[Epoch:  5/  5] [data: 14080/24058] Training Loss: 0.1522 Training Acc: 95.1989%\n",
      "[Epoch:  5/  5] [data: 14208/24058] Training Loss: 0.2588 Training Acc: 95.1788%\n",
      "[Epoch:  5/  5] [data: 14336/24058] Training Loss: 0.1258 Training Acc: 95.1939%\n",
      "[Epoch:  5/  5] [data: 14464/24058] Training Loss: 0.0652 Training Acc: 95.2226%\n",
      "[Epoch:  5/  5] [data: 14592/24058] Training Loss: 0.1705 Training Acc: 95.2234%\n",
      "[Epoch:  5/  5] [data: 14720/24058] Training Loss: 0.0678 Training Acc: 95.2514%\n",
      "[Epoch:  5/  5] [data: 14848/24058] Training Loss: 0.1656 Training Acc: 95.2452%\n",
      "[Epoch:  5/  5] [data: 14976/24058] Training Loss: 0.1464 Training Acc: 95.2457%\n",
      "[Epoch:  5/  5] [data: 15104/24058] Training Loss: 0.1161 Training Acc: 95.2529%\n",
      "[Epoch:  5/  5] [data: 15232/24058] Training Loss: 0.1460 Training Acc: 95.2600%\n",
      "[Epoch:  5/  5] [data: 15360/24058] Training Loss: 0.1240 Training Acc: 95.2734%\n",
      "[Epoch:  5/  5] [data: 15488/24058] Training Loss: 0.1309 Training Acc: 95.2738%\n",
      "[Epoch:  5/  5] [data: 15616/24058] Training Loss: 0.1721 Training Acc: 95.2741%\n",
      "[Epoch:  5/  5] [data: 15744/24058] Training Loss: 0.2333 Training Acc: 95.2617%\n",
      "[Epoch:  5/  5] [data: 15872/24058] Training Loss: 0.1333 Training Acc: 95.2747%\n",
      "[Epoch:  5/  5] [data: 16000/24058] Training Loss: 0.1916 Training Acc: 95.2750%\n",
      "[Epoch:  5/  5] [data: 16128/24058] Training Loss: 0.1315 Training Acc: 95.2815%\n",
      "[Epoch:  5/  5] [data: 16256/24058] Training Loss: 0.1960 Training Acc: 95.2756%\n",
      "[Epoch:  5/  5] [data: 16384/24058] Training Loss: 0.1822 Training Acc: 95.2759%\n",
      "[Epoch:  5/  5] [data: 16512/24058] Training Loss: 0.2515 Training Acc: 95.2519%\n",
      "[Epoch:  5/  5] [data: 16640/24058] Training Loss: 0.1755 Training Acc: 95.2464%\n",
      "[Epoch:  5/  5] [data: 16768/24058] Training Loss: 0.1275 Training Acc: 95.2588%\n",
      "[Epoch:  5/  5] [data: 16896/24058] Training Loss: 0.1829 Training Acc: 95.2592%\n",
      "[Epoch:  5/  5] [data: 17024/24058] Training Loss: 0.2840 Training Acc: 95.2361%\n",
      "[Epoch:  5/  5] [data: 17152/24058] Training Loss: 0.1760 Training Acc: 95.2309%\n",
      "[Epoch:  5/  5] [data: 17280/24058] Training Loss: 0.2517 Training Acc: 95.2141%\n",
      "[Epoch:  5/  5] [data: 17408/24058] Training Loss: 0.2270 Training Acc: 95.2034%\n",
      "[Epoch:  5/  5] [data: 17536/24058] Training Loss: 0.1077 Training Acc: 95.2213%\n",
      "[Epoch:  5/  5] [data: 17664/24058] Training Loss: 0.1524 Training Acc: 95.2276%\n",
      "[Epoch:  5/  5] [data: 17792/24058] Training Loss: 0.1367 Training Acc: 95.2394%\n",
      "[Epoch:  5/  5] [data: 17920/24058] Training Loss: 0.2048 Training Acc: 95.2288%\n",
      "[Epoch:  5/  5] [data: 18048/24058] Training Loss: 0.1262 Training Acc: 95.2405%\n",
      "[Epoch:  5/  5] [data: 18176/24058] Training Loss: 0.2825 Training Acc: 95.2135%\n",
      "[Epoch:  5/  5] [data: 18304/24058] Training Loss: 0.1744 Training Acc: 95.2142%\n",
      "[Epoch:  5/  5] [data: 18432/24058] Training Loss: 0.1334 Training Acc: 95.2203%\n",
      "[Epoch:  5/  5] [data: 18560/24058] Training Loss: 0.1870 Training Acc: 95.2101%\n",
      "[Epoch:  5/  5] [data: 18688/24058] Training Loss: 0.0967 Training Acc: 95.2269%\n",
      "[Epoch:  5/  5] [data: 18816/24058] Training Loss: 0.1488 Training Acc: 95.2328%\n",
      "[Epoch:  5/  5] [data: 18944/24058] Training Loss: 0.1734 Training Acc: 95.2333%\n",
      "[Epoch:  5/  5] [data: 19072/24058] Training Loss: 0.1290 Training Acc: 95.2443%\n",
      "[Epoch:  5/  5] [data: 19200/24058] Training Loss: 0.0922 Training Acc: 95.2604%\n",
      "[Epoch:  5/  5] [data: 19328/24058] Training Loss: 0.2057 Training Acc: 95.2504%\n",
      "[Epoch:  5/  5] [data: 19456/24058] Training Loss: 0.1684 Training Acc: 95.2508%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 19584/24058] Training Loss: 0.1014 Training Acc: 95.2614%\n",
      "[Epoch:  5/  5] [data: 19712/24058] Training Loss: 0.1513 Training Acc: 95.2668%\n",
      "[Epoch:  5/  5] [data: 19840/24058] Training Loss: 0.1272 Training Acc: 95.2722%\n",
      "[Epoch:  5/  5] [data: 19968/24058] Training Loss: 0.2007 Training Acc: 95.2624%\n",
      "[Epoch:  5/  5] [data: 20096/24058] Training Loss: 0.3121 Training Acc: 95.2329%\n",
      "[Epoch:  5/  5] [data: 20224/24058] Training Loss: 0.2314 Training Acc: 95.2136%\n",
      "[Epoch:  5/  5] [data: 20352/24058] Training Loss: 0.1273 Training Acc: 95.2191%\n",
      "[Epoch:  5/  5] [data: 20480/24058] Training Loss: 0.0812 Training Acc: 95.2393%\n",
      "[Epoch:  5/  5] [data: 20608/24058] Training Loss: 0.2590 Training Acc: 95.2106%\n",
      "[Epoch:  5/  5] [data: 20736/24058] Training Loss: 0.1208 Training Acc: 95.2160%\n",
      "[Epoch:  5/  5] [data: 20864/24058] Training Loss: 0.1032 Training Acc: 95.2262%\n",
      "[Epoch:  5/  5] [data: 20992/24058] Training Loss: 0.1824 Training Acc: 95.2172%\n",
      "[Epoch:  5/  5] [data: 21120/24058] Training Loss: 0.1877 Training Acc: 95.2131%\n",
      "[Epoch:  5/  5] [data: 21248/24058] Training Loss: 0.1207 Training Acc: 95.2231%\n",
      "[Epoch:  5/  5] [data: 21376/24058] Training Loss: 0.1472 Training Acc: 95.2236%\n",
      "[Epoch:  5/  5] [data: 21504/24058] Training Loss: 0.0702 Training Acc: 95.2427%\n",
      "[Epoch:  5/  5] [data: 21632/24058] Training Loss: 0.2690 Training Acc: 95.2247%\n",
      "[Epoch:  5/  5] [data: 21760/24058] Training Loss: 0.1706 Training Acc: 95.2252%\n",
      "[Epoch:  5/  5] [data: 21888/24058] Training Loss: 0.2264 Training Acc: 95.2166%\n",
      "[Epoch:  5/  5] [data: 22016/24058] Training Loss: 0.1856 Training Acc: 95.2126%\n",
      "[Epoch:  5/  5] [data: 22144/24058] Training Loss: 0.1950 Training Acc: 95.2086%\n",
      "[Epoch:  5/  5] [data: 22272/24058] Training Loss: 0.1425 Training Acc: 95.2182%\n",
      "[Epoch:  5/  5] [data: 22400/24058] Training Loss: 0.2026 Training Acc: 95.2143%\n",
      "[Epoch:  5/  5] [data: 22528/24058] Training Loss: 0.1794 Training Acc: 95.2104%\n",
      "[Epoch:  5/  5] [data: 22656/24058] Training Loss: 0.2270 Training Acc: 95.2022%\n",
      "[Epoch:  5/  5] [data: 22784/24058] Training Loss: 0.1545 Training Acc: 95.2028%\n",
      "[Epoch:  5/  5] [data: 22912/24058] Training Loss: 0.1931 Training Acc: 95.1990%\n",
      "[Epoch:  5/  5] [data: 23040/24058] Training Loss: 0.2158 Training Acc: 95.1910%\n",
      "[Epoch:  5/  5] [data: 23168/24058] Training Loss: 0.1318 Training Acc: 95.1960%\n",
      "[Epoch:  5/  5] [data: 23296/24058] Training Loss: 0.2038 Training Acc: 95.1923%\n",
      "[Epoch:  5/  5] [data: 23424/24058] Training Loss: 0.1040 Training Acc: 95.2015%\n",
      "[Epoch:  5/  5] [data: 23552/24058] Training Loss: 0.1430 Training Acc: 95.2021%\n",
      "[Epoch:  5/  5] [data: 23680/24058] Training Loss: 0.1911 Training Acc: 95.1943%\n",
      "[Epoch:  5/  5] [data: 23808/24058] Training Loss: 0.0520 Training Acc: 95.2159%\n",
      "[Epoch:  5/  5] [data: 23936/24058] Training Loss: 0.1460 Training Acc: 95.2206%\n",
      "[Epoch:  5/  5] [data: 24058/24058] Training Loss: 0.1627 Training Acc: 95.2240%\n",
      "Testing-4...\n",
      "[Epoch:  5/  5] Validation Loss: 0.1728 Validation Acc: 95.0480%\n",
      "Time used: 1154.2623541355133s\n",
      "Type-4: 1.0 0.90067214339059 0.9477406679764243\n",
      "[Epoch:  1/  5] [data: 128/24026] Training Loss: 0.3361 Training Acc: 89.8438%\n",
      "[Epoch:  1/  5] [data: 256/24026] Training Loss: 0.2518 Training Acc: 91.0156%\n",
      "[Epoch:  1/  5] [data: 384/24026] Training Loss: 0.2204 Training Acc: 91.9271%\n",
      "[Epoch:  1/  5] [data: 512/24026] Training Loss: 0.3869 Training Acc: 91.2109%\n",
      "[Epoch:  1/  5] [data: 640/24026] Training Loss: 0.1686 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 768/24026] Training Loss: 0.3553 Training Acc: 91.9271%\n",
      "[Epoch:  1/  5] [data: 896/24026] Training Loss: 0.2678 Training Acc: 91.9643%\n",
      "[Epoch:  1/  5] [data: 1024/24026] Training Loss: 0.2296 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 1152/24026] Training Loss: 0.1086 Training Acc: 92.7083%\n",
      "[Epoch:  1/  5] [data: 1280/24026] Training Loss: 0.1787 Training Acc: 92.8906%\n",
      "[Epoch:  1/  5] [data: 1408/24026] Training Loss: 0.1938 Training Acc: 93.0398%\n",
      "[Epoch:  1/  5] [data: 1536/24026] Training Loss: 0.3329 Training Acc: 92.8385%\n",
      "[Epoch:  1/  5] [data: 1664/24026] Training Loss: 0.2110 Training Acc: 92.9688%\n",
      "[Epoch:  1/  5] [data: 1792/24026] Training Loss: 0.2849 Training Acc: 92.9129%\n",
      "[Epoch:  1/  5] [data: 1920/24026] Training Loss: 0.2114 Training Acc: 93.0208%\n",
      "[Epoch:  1/  5] [data: 2048/24026] Training Loss: 0.2064 Training Acc: 93.0664%\n",
      "[Epoch:  1/  5] [data: 2176/24026] Training Loss: 0.1729 Training Acc: 93.1985%\n",
      "[Epoch:  1/  5] [data: 2304/24026] Training Loss: 0.2028 Training Acc: 93.2726%\n",
      "[Epoch:  1/  5] [data: 2432/24026] Training Loss: 0.2515 Training Acc: 93.2155%\n",
      "[Epoch:  1/  5] [data: 2560/24026] Training Loss: 0.1497 Training Acc: 93.3594%\n",
      "[Epoch:  1/  5] [data: 2688/24026] Training Loss: 0.2188 Training Acc: 93.3780%\n",
      "[Epoch:  1/  5] [data: 2816/24026] Training Loss: 0.1536 Training Acc: 93.5014%\n",
      "[Epoch:  1/  5] [data: 2944/24026] Training Loss: 0.2153 Training Acc: 93.5122%\n",
      "[Epoch:  1/  5] [data: 3072/24026] Training Loss: 0.4258 Training Acc: 93.2617%\n",
      "[Epoch:  1/  5] [data: 3200/24026] Training Loss: 0.2037 Training Acc: 93.3125%\n",
      "[Epoch:  1/  5] [data: 3328/24026] Training Loss: 0.2131 Training Acc: 93.2993%\n",
      "[Epoch:  1/  5] [data: 3456/24026] Training Loss: 0.2746 Training Acc: 93.2292%\n",
      "[Epoch:  1/  5] [data: 3584/24026] Training Loss: 0.2659 Training Acc: 93.1920%\n",
      "[Epoch:  1/  5] [data: 3712/24026] Training Loss: 0.2056 Training Acc: 93.2381%\n",
      "[Epoch:  1/  5] [data: 3840/24026] Training Loss: 0.2070 Training Acc: 93.2812%\n",
      "[Epoch:  1/  5] [data: 3968/24026] Training Loss: 0.1019 Training Acc: 93.4476%\n",
      "[Epoch:  1/  5] [data: 4096/24026] Training Loss: 0.2058 Training Acc: 93.4814%\n",
      "[Epoch:  1/  5] [data: 4224/24026] Training Loss: 0.2606 Training Acc: 93.4659%\n",
      "[Epoch:  1/  5] [data: 4352/24026] Training Loss: 0.2387 Training Acc: 93.4743%\n",
      "[Epoch:  1/  5] [data: 4480/24026] Training Loss: 0.3365 Training Acc: 93.4152%\n",
      "[Epoch:  1/  5] [data: 4608/24026] Training Loss: 0.2869 Training Acc: 93.3811%\n",
      "[Epoch:  1/  5] [data: 4736/24026] Training Loss: 0.1445 Training Acc: 93.4333%\n",
      "[Epoch:  1/  5] [data: 4864/24026] Training Loss: 0.1245 Training Acc: 93.5238%\n",
      "[Epoch:  1/  5] [data: 4992/24026] Training Loss: 0.3333 Training Acc: 93.4295%\n",
      "[Epoch:  1/  5] [data: 5120/24026] Training Loss: 0.2247 Training Acc: 93.4180%\n",
      "[Epoch:  1/  5] [data: 5248/24026] Training Loss: 0.1802 Training Acc: 93.4451%\n",
      "[Epoch:  1/  5] [data: 5376/24026] Training Loss: 0.2802 Training Acc: 93.4152%\n",
      "[Epoch:  1/  5] [data: 5504/24026] Training Loss: 0.1784 Training Acc: 93.4775%\n",
      "[Epoch:  1/  5] [data: 5632/24026] Training Loss: 0.2777 Training Acc: 93.4482%\n",
      "[Epoch:  1/  5] [data: 5760/24026] Training Loss: 0.3363 Training Acc: 93.3681%\n",
      "[Epoch:  1/  5] [data: 5888/24026] Training Loss: 0.2048 Training Acc: 93.3933%\n",
      "[Epoch:  1/  5] [data: 6016/24026] Training Loss: 0.1291 Training Acc: 93.4674%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 1\n",
    "EPOCHS = [2,2,2,5,5,10]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = BatchProgramCC(EMBEDDING_DIM,\n",
    "                       HIDDEN_DIM,\n",
    "                       MAX_TOKENS+1,\n",
    "                       ENCODE_DIM,\n",
    "                       LABELS,\n",
    "                       BATCH_SIZE,\n",
    "                       device,\n",
    "                       embeddings)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adamax(parameters)\n",
    "#loss_function = torch.nn.BCELoss()\n",
    "loss_function = ContrastiveLoss(2.0)\n",
    "\n",
    "best_model_state_dict = model.state_dict()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_p = []\n",
    "test_r = []\n",
    "test_f = []\n",
    "best_acc = 0.0\n",
    "precision, recall, f1 = 0, 0, 0\n",
    "print('Start training...')\n",
    "\n",
    "for t in range(1, categories+1):\n",
    "    if lang == 'java':\n",
    "        train_data_t = train_data[train_data['label'].isin([t, 0])]\n",
    "        train_data_t.loc[train_data_t['label'] > 0, 'label'] = 1\n",
    "        \n",
    "        validation_data_t = validation_data[validation_data['label'].isin([t, 0])]\n",
    "        validation_data_t.loc[validation_data_t['label'] > 0, 'label'] = 1\n",
    "        \n",
    "        test_data_t = test_data[test_data['label'].isin([t, 0])]\n",
    "        test_data_t.loc[test_data_t['label'] > 0, 'label'] = 1\n",
    "    else:\n",
    "        train_data_t, validation_data_t, test_data_t = train_data, validation_data, test_data\n",
    "    \n",
    "    for epoch in range(0, EPOCHS[t-1]):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss, total_acc = train(model, train_data_t, BATCH_SIZE, device, EPOCHS[t-1], epoch, optimizer)\n",
    "        train_loss.append(total_loss)\n",
    "        train_acc.append(total_acc)\n",
    "        \n",
    "        print(\"Testing-%d...\"%t)\n",
    "        model.eval()\n",
    "        total_loss, total_acc = validation(model, validation_data_t, BATCH_SIZE, device, EPOCHS[t-1], epoch)\n",
    "        val_loss.append(total_loss)\n",
    "        val_acc.append(total_acc)\n",
    "        \n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Time used: {}s\".format(end_time-start_time))\n",
    "    \n",
    "    torch.save(model.state_dict(), 'code_clone_detection_java_model_train_with_ContrastiveLoss_on_java_label_' + str(t) + '_epoch_' + str(EPOCHS[t-1]) + '.pt')\n",
    "    total_loss, total_acc, precision, recall, f1 = test(model, test_data_t, BATCH_SIZE, device)\n",
    "    test_loss.append(total_loss)\n",
    "    test_acc.append(total_acc)\n",
    "    test_p.append(precision)\n",
    "    test_r.append(recall)\n",
    "    test_f.append(f1)\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), 'code_clone_detection_java_model_train_on_30_percent.pt')\n",
    "#model.load_state_dict(best_model_state_dict)\n",
    "#model.eval()\n",
    "#test(model, test_data, BATCH_SIZE*2, device)\n",
    "#print(\"Total testing results(P,R,F1):%.3f, %.3f, %.3f\" % (precision, recall, f1))\n",
    "\n",
    "#torch.save(model.state_dict(), 'code_clone_detection_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-21T06:38:29.566Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#epochs = len(hist.history['loss'][9:])\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(train_loss)), train_loss, label='loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='val_loss')\n",
    "plt.plot(range(len(test_loss)), test_loss, label='test_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(train_acc)), train_acc, label='accuracy')\n",
    "plt.plot(range(len(val_acc)), val_acc, label='val_accuracy')\n",
    "plt.plot(range(len(test_acc)), test_acc, label='test_accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(test_p)), test_p, label='precision')\n",
    "plt.plot(range(len(test_r)), test_r, label='recall')\n",
    "plt.plot(range(len(test_f)), test_f, label='f1')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T15:23:05.171694Z",
     "start_time": "2020-08-12T15:20:30.405009Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse source code...\n",
      "read id pairs...\n",
      "split data...\n",
      "train word embedding...\n",
      "generate block sequences...\n",
      "merge pairs and blocks...\n",
      "generate query source...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "class Pipeline:\n",
    "    def __init__(self,  ratio, root, language):\n",
    "        self.ratio = ratio\n",
    "        self.root = root\n",
    "        self.language = language\n",
    "        self.sources = None\n",
    "        self.blocks = None\n",
    "        self.pairs = None\n",
    "        self.train_file_path = None\n",
    "        self.dev_file_path = None\n",
    "        self.test_file_path = None\n",
    "        self.size = None\n",
    "        \n",
    "        self.seen_ids = None\n",
    "        self.unseen_ids = None\n",
    "\n",
    "    # parse source code\n",
    "    def parse_source(self, output_file, option):\n",
    "        path = self.root+self.language+'/'+output_file\n",
    "        if os.path.exists(path) and option == 'existing':\n",
    "            source = pd.read_pickle(path)\n",
    "        else:\n",
    "            if self.language == 'c':\n",
    "                from pycparser import c_parser\n",
    "                parser = c_parser.CParser()\n",
    "                source = pd.read_pickle(self.root+self.language+'/programs.pkl')\n",
    "                source.columns = ['id', 'code', 'label']\n",
    "                source['code'] = source['code'].apply(parser.parse)\n",
    "                source.to_pickle(path)\n",
    "            else:\n",
    "                import javalang\n",
    "                def parse_program(func):\n",
    "                    tokens = javalang.tokenizer.tokenize(func)\n",
    "                    parser = javalang.parser.Parser(tokens)\n",
    "                    tree = parser.parse_member_declaration()\n",
    "                    return tree\n",
    "                source = pd.read_csv(self.root+self.language+'/bcb_funcs_all.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "                source.columns = ['id', 'code']\n",
    "                source.loc[15167, 'code'] = source['code'][15167]+'\\r     */'\n",
    "                source['code'] = source['code'].apply(parse_program)\n",
    "                source.to_pickle(path)\n",
    "        self.sources = source\n",
    "        return source\n",
    "\n",
    "    # create clone pairs\n",
    "    def read_pairs(self, filename):\n",
    "        pairs = pd.read_pickle(self.root+self.language+'/'+filename)\n",
    "        self.pairs = pairs\n",
    "\n",
    "    # split data for training, developing and testing\n",
    "    def split_data(self):\n",
    "        data_path = self.root+self.language+'/'\n",
    "        data = self.pairs\n",
    "        data_num = len(data)\n",
    "        ratios = [int(r) for r in self.ratio.split(':')]\n",
    "        train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "        val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "\n",
    "        data = data.sample(frac=1, random_state=666)\n",
    "        train = data.iloc[:train_split]\n",
    "        dev = data.iloc[train_split:val_split]\n",
    "        test = data.iloc[val_split:]\n",
    "        #  train  test \n",
    "\n",
    "        def check_or_create(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "        train_path = data_path+'train/'\n",
    "        check_or_create(train_path)\n",
    "        self.train_file_path = train_path+'train_.pkl'\n",
    "        train.to_pickle(self.train_file_path)\n",
    "\n",
    "        dev_path = data_path+'dev/'\n",
    "        check_or_create(dev_path)\n",
    "        self.dev_file_path = dev_path+'dev_.pkl'\n",
    "        dev.to_pickle(self.dev_file_path)\n",
    "\n",
    "        test_path = data_path+'test/'\n",
    "        check_or_create(test_path)\n",
    "        self.test_file_path = test_path+'test_.pkl'\n",
    "        test.to_pickle(self.test_file_path)\n",
    "        \n",
    "        train_ids = train['id1'].append(train['id2']).unique()\n",
    "        dev_ids = dev['id1'].append(dev['id2']).unique()\n",
    "        test_ids = test['id1'].append(test['id2']).unique()\n",
    "        \n",
    "        import numpy as np\n",
    "        self.seen_ids = np.unique(np.hstack([train_ids, dev_ids]))\n",
    "        self.unseen_ids = np.setdiff1d(self.sources['id'].unique(), self.seen_ids)\n",
    "\n",
    "    # construct dictionary and train word embedding\n",
    "    def dictionary_and_embedding(self, input_file, size):\n",
    "        self.size = size\n",
    "        data_path = self.root+self.language+'/'\n",
    "        if not input_file:\n",
    "            input_file = self.train_file_path\n",
    "        pairs = pd.read_pickle(input_file)\n",
    "        train_ids = pairs['id1'].append(pairs['id2']).unique()\n",
    "\n",
    "        temp = self.sources.set_index('id',drop=False)\n",
    "        trees = temp.loc[temp.index.intersection(train_ids)]\n",
    "        if not os.path.exists(data_path+'train/embedding'):\n",
    "            os.mkdir(data_path+'train/embedding')\n",
    "        if self.language == 'c':\n",
    "            sys.path.append('../')\n",
    "            from prepare_data import get_sequences as func\n",
    "        else:\n",
    "            from utils import get_sequence as func\n",
    "\n",
    "        def trans_to_sequences(ast):\n",
    "            sequence = []\n",
    "            func(ast, sequence)\n",
    "            return sequence\n",
    "        corpus = trees['code'].apply(trans_to_sequences)\n",
    "        str_corpus = [' '.join(c) for c in corpus]\n",
    "        trees['code'] = pd.Series(str_corpus)\n",
    "        # trees.to_csv(data_path+'train/programs_ns.tsv')\n",
    "\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, max_final_vocab=3000)\n",
    "        w2v.save(data_path+'train/embedding/node_w2v_' + str(size))\n",
    "\n",
    "    # generate block sequences with index representations\n",
    "    def generate_block_seqs(self):\n",
    "        if self.language == 'c':\n",
    "            from prepare_data import get_blocks as func\n",
    "        else:\n",
    "            from utils import get_blocks_v1 as func\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "        word2vec = Word2Vec.load(self.root+self.language+'/train/embedding/node_w2v_' + str(self.size)).wv\n",
    "        vocab = word2vec.vocab\n",
    "        max_token = word2vec.vectors.shape[0]\n",
    "\n",
    "        def tree_to_index(node):\n",
    "            token = node.token\n",
    "            result = [vocab[token].index if token in vocab else max_token]\n",
    "            children = node.children\n",
    "            for child in children:\n",
    "                result.append(tree_to_index(child))\n",
    "            return result\n",
    "\n",
    "        def trans2seq(r):\n",
    "            blocks = []\n",
    "            func(r, blocks)\n",
    "            tree = []\n",
    "            for b in blocks:\n",
    "                btree = tree_to_index(b)\n",
    "                tree.append(btree)\n",
    "            return tree\n",
    "        trees = pd.DataFrame(self.sources, copy=True)\n",
    "        trees['code'] = trees['code'].apply(trans2seq)\n",
    "        if 'label' in trees.columns:\n",
    "            trees.drop('label', axis=1, inplace=True)\n",
    "        self.blocks = trees\n",
    "\n",
    "    # merge pairs\n",
    "    def merge(self,data_path,part):\n",
    "        pairs = pd.read_pickle(data_path)\n",
    "        pairs['id1'] = pairs['id1'].astype(int)\n",
    "        pairs['id2'] = pairs['id2'].astype(int)\n",
    "        df = pd.merge(pairs, self.blocks, how='left', left_on='id1', right_on='id')\n",
    "        df = pd.merge(df, self.blocks, how='left', left_on='id2', right_on='id')\n",
    "        df.drop(['id_x', 'id_y'], axis=1,inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        df.to_pickle(self.root+self.language+'/'+part+'/blocks.pkl')\n",
    "\n",
    "        \n",
    "    def generate_query_source(self):\n",
    "        self.sources['block'] = self.blocks['code']\n",
    "        self.sources.drop(columns=['code'], axis=1, inplace=True)\n",
    "        self.sources = self.sources.set_index('id')\n",
    "        self.query_source = self.sources.loc[self.sources.index.intersection(self.seen_ids)]\n",
    "        self.unseen_source = self.sources.loc[self.sources.index.intersection(self.unseen_ids)]\n",
    "        self.query_source.to_pickle(self.root+self.language+'/query_source.pkl')\n",
    "        self.unseen_source.to_pickle(self.root+self.language+'/unseen_source.pkl')\n",
    "        \n",
    "    # run for processing data to train\n",
    "    def run(self):\n",
    "        print('parse source code...')\n",
    "        self.parse_source(output_file='ast.pkl',option='existing')\n",
    "        print('read id pairs...')\n",
    "        if self.language == 'c':\n",
    "            self.read_pairs('oj_clone_ids.pkl')\n",
    "        else:\n",
    "            self.read_pairs('bcb_pair_ids.pkl')\n",
    "        print('split data...')\n",
    "        self.split_data()\n",
    "        print('train word embedding...')\n",
    "        self.dictionary_and_embedding(None,128)\n",
    "        print('generate block sequences...')\n",
    "        self.generate_block_seqs()\n",
    "        print('merge pairs and blocks...')\n",
    "        self.merge(self.train_file_path, 'train')\n",
    "        self.merge(self.dev_file_path, 'dev')\n",
    "        self.merge(self.test_file_path, 'test')\n",
    "        print('generate query source...')\n",
    "        self.generate_query_source()\n",
    "\n",
    "\n",
    "ppl = Pipeline('3:1:1', 'data/', 'java')\n",
    "ppl.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
