{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramCC\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    x1, x2, labels = [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        x2.append(item['code_y'])\n",
    "        labels.append([item['label']])\n",
    "    return x1, x2, torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Choose a dataset:[c|java]\")\n",
    "    parser.add_argument('--lang')\n",
    "    args = parser.parse_args()\n",
    "    if not args.lang:\n",
    "        print(\"No specified dataset\")\n",
    "        exit(1)\n",
    "    root = 'data/'\n",
    "    lang = args.lang\n",
    "    categories = 1\n",
    "    if lang == 'java':\n",
    "        categories = 5\n",
    "    print(\"Train for \", str.upper(lang))\n",
    "    train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1)\n",
    "    test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1)\n",
    "\n",
    "    word2vec = Word2Vec.load(root+lang+\"/train/embedding/node_w2v_128\").wv\n",
    "    MAX_TOKENS = word2vec.syn0.shape[0]\n",
    "    EMBEDDING_DIM = word2vec.syn0.shape[1]\n",
    "    embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "    embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0\n",
    "\n",
    "    HIDDEN_DIM = 100\n",
    "    ENCODE_DIM = 128\n",
    "    LABELS = 1\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 32\n",
    "    USE_GPU = True\n",
    "\n",
    "    model = BatchProgramCC(EMBEDDING_DIM,HIDDEN_DIM,MAX_TOKENS+1,ENCODE_DIM,LABELS,BATCH_SIZE,\n",
    "                                   USE_GPU, embeddings)\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.Adamax(parameters)\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    print(train_data)\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    print('Start training...')\n",
    "    for t in range(1, categories+1):\n",
    "        if lang == 'java':\n",
    "            train_data_t = train_data[train_data['label'].isin([t, 0])]\n",
    "            train_data_t.loc[train_data_t['label'] > 0, 'label'] = 1\n",
    "\n",
    "            test_data_t = test_data[test_data['label'].isin([t, 0])]\n",
    "            test_data_t.loc[test_data_t['label'] > 0, 'label'] = 1\n",
    "        else:\n",
    "            train_data_t, test_data_t = train_data, test_data\n",
    "        # training procedure\n",
    "        for epoch in range(EPOCHS):\n",
    "            start_time = time.time()\n",
    "            # training epoch\n",
    "            total_acc = 0.0\n",
    "            total_loss = 0.0\n",
    "            total = 0.0\n",
    "            i = 0\n",
    "            while i < len(train_data_t):\n",
    "                batch = get_batch(train_data_t, i, BATCH_SIZE)\n",
    "                i += BATCH_SIZE\n",
    "                train1_inputs, train2_inputs, train_labels = batch\n",
    "                if USE_GPU:\n",
    "                    train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "\n",
    "                model.zero_grad()\n",
    "                model.batch_size = len(train_labels)\n",
    "                model.hidden = model.init_hidden()\n",
    "                output = model(train1_inputs, train2_inputs)\n",
    "\n",
    "                loss = loss_function(output, Variable(train_labels))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(\"Testing-%d...\"%t)\n",
    "        # testing procedure\n",
    "        predicts = []\n",
    "        trues = []\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(test_data_t):\n",
    "            batch = get_batch(test_data_t, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            test1_inputs, test2_inputs, test_labels = batch\n",
    "            if USE_GPU:\n",
    "                test_labels = test_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test1_inputs, test2_inputs)\n",
    "\n",
    "            loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "            # calc testing acc\n",
    "            predicted = (output.data > 0.5).cpu().numpy()\n",
    "            predicts.extend(predicted)\n",
    "            trues.extend(test_labels.cpu().numpy())\n",
    "            total += len(test_labels)\n",
    "            total_loss += loss.item() * len(test_labels)\n",
    "        if lang == 'java':\n",
    "            weights = [0, 0.005, 0.001, 0.002, 0.010, 0.982]\n",
    "            p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "            precision += weights[t] * p\n",
    "            recall += weights[t] * r\n",
    "            f1 += weights[t] * f\n",
    "            print(\"Type-\" + str(t) + \": \" + str(p) + \" \" + str(r) + \" \" + str(f))\n",
    "        else:\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "\n",
    "    print(\"Total testing results(P,R,F1):%.3f, %.3f, %.3f\" % (precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "\n",
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        self.max_index = vocab_size\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, node, batch_index):\n",
    "        size = len(node)\n",
    "        if not size:\n",
    "            return None\n",
    "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.embedding_dim)))\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "        for i in range(size):\n",
    "            # if node[i][0] is not -1:\n",
    "                index.append(i)\n",
    "                current_node.append(node[i][0])\n",
    "                temp = node[i][1:]\n",
    "                c_num = len(temp)\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] is not -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            # else:\n",
    "            #     batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
    "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_index = [i for i in batch_index if i is not -1]\n",
    "        b_in = Variable(self.th.LongTensor(batch_index))\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, x, bs):\n",
    "        self.batch_size = bs\n",
    "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(x, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]\n",
    "\n",
    "\n",
    "class BatchProgramCC(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramCC, self).__init__()\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def encode(self, x):\n",
    "        lens = [len(item) for item in x]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        encodes = []\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(lens[i]):\n",
    "                encodes.append(x[i][j])\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "            seq.append(encodes[start:end])\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "        # return encodes\n",
    "\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        return gru_out\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        lvec, rvec = self.encode(x1), self.encode(x2)\n",
    "\n",
    "        abs_dist = torch.abs(torch.add(lvec, -rvec))\n",
    "\n",
    "        y = torch.sigmoid(self.hidden2label(abs_dist))\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# tree.py\n",
    "\n",
    "from javalang.ast import Node\n",
    "\n",
    "class ASTNode(object):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        # self.vocab = word_map\n",
    "        self.is_str = isinstance(self.node, str)\n",
    "        self.token = self.get_token()\n",
    "        # self.index = self.token_to_index(self.token)\n",
    "        self.children = self.add_children()\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.is_str:\n",
    "            return True\n",
    "        return len(self.node.children()) == 0\n",
    "\n",
    "    def get_token(self, lower=True):\n",
    "        if self.is_str:\n",
    "            return self.node\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "        is_name = False\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if attr_names:\n",
    "                if 'names' in attr_names:\n",
    "                    token = self.node.names[0]\n",
    "                elif 'name' in attr_names:\n",
    "                    token = self.node.name\n",
    "                    is_name = True\n",
    "                else:\n",
    "                    token = self.node.value\n",
    "            else:\n",
    "                token = name\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    if self.node.op[0] == 'p':\n",
    "                        token = self.node.op[1:]\n",
    "                    else:\n",
    "                        token = self.node.op\n",
    "        if token is None:\n",
    "            token = name\n",
    "        if lower and is_name:\n",
    "            token = token.lower()\n",
    "        return token\n",
    "\n",
    "    # def token_to_index(self, token):\n",
    "    #     self.index = self.vocab[token].index if token in self.vocab else MAX_TOKENS\n",
    "    #     return self.index\n",
    "\n",
    "    # def get_index(self):\n",
    "    #     return self.index\n",
    "\n",
    "    def add_children(self):\n",
    "        if self.is_str:\n",
    "            return []\n",
    "        children = self.node.children()\n",
    "        if self.token in ['FuncDef', 'If', 'While', 'DoWhile']:\n",
    "            return [ASTNode(children[0][1])]\n",
    "        elif self.token == 'For':\n",
    "            return [ASTNode(children[c][1]) for c in range(0, len(children)-1)]\n",
    "        else:\n",
    "            return [ASTNode(child) for _, child in children]\n",
    "\n",
    "\n",
    "class BlockNode(object):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.is_str = isinstance(self.node, str)\n",
    "        self.token = self.get_token(node)\n",
    "        self.children = self.add_children()\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.is_str:\n",
    "            return True\n",
    "        return len(self.node.children) == 0\n",
    "\n",
    "    def get_token(self, node):\n",
    "        if isinstance(node, str):\n",
    "            token = node\n",
    "        elif isinstance(node, set):\n",
    "            token = 'Modifier'\n",
    "        elif isinstance(node, Node):\n",
    "            token = node.__class__.__name__\n",
    "        else:\n",
    "            token = ''\n",
    "        return token\n",
    "\n",
    "    def ori_children(self, root):\n",
    "        if isinstance(root, Node):\n",
    "            if self.token in ['MethodDeclaration', 'ConstructorDeclaration']:\n",
    "                children = root.children[:-1]\n",
    "            else:\n",
    "                children = root.children\n",
    "        elif isinstance(root, set):\n",
    "            children = list(root)\n",
    "        else:\n",
    "            children = []\n",
    "\n",
    "        def expand(nested_list):\n",
    "            for item in nested_list:\n",
    "                if isinstance(item, list):\n",
    "                    for sub_item in expand(item):\n",
    "                        yield sub_item\n",
    "                elif item:\n",
    "                    yield item\n",
    "\n",
    "        return list(expand(children))\n",
    "\n",
    "    def add_children(self):\n",
    "        if self.is_str:\n",
    "            return []\n",
    "        logic = ['SwitchStatement', 'IfStatement', 'ForStatement', 'WhileStatement', 'DoStatement']\n",
    "        children = self.ori_children(self.node)\n",
    "        if self.token in logic:\n",
    "            return [BlockNode(children[0])]\n",
    "        elif self.token in ['MethodDeclaration', 'ConstructorDeclaration']:\n",
    "            return [BlockNode(child) for child in children]\n",
    "        else:\n",
    "            return [BlockNode(child) for child in children if self.get_token( child) not in logic]\n",
    "\n",
    "class SingleNode(ASTNode):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.is_str = isinstance(self.node, str)\n",
    "        self.token = self.get_token()\n",
    "        self.children = []\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.is_str:\n",
    "            return True\n",
    "        return len(self.node.children()) == 0\n",
    "\n",
    "    def get_token(self, lower=True):\n",
    "        if self.is_str:\n",
    "            return self.node\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "        is_name = False\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if attr_names:\n",
    "                if 'names' in attr_names:\n",
    "                    token = self.node.names[0]\n",
    "                elif 'name' in attr_names:\n",
    "                    token = self.node.name\n",
    "                    is_name = True\n",
    "                else:\n",
    "                    token = self.node.value\n",
    "            else:\n",
    "                token = name\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    if self.node.op[0] == 'p':\n",
    "                        token = self.node.op[1:]\n",
    "                    else:\n",
    "                        token = self.node.op\n",
    "        if token is None:\n",
    "            token = name\n",
    "        if lower and is_name:\n",
    "            token = token.lower()\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import pandas as pd\n",
    "import javalang\n",
    "from javalang.ast import Node\n",
    "from tree import ASTNode, BlockNode\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def get_token(node):\n",
    "    token = ''\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'#node.pop()\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_children(root):\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "def get_sequence(node, sequence):\n",
    "    token, children = get_token(node), get_children(node)\n",
    "    sequence.append(token)\n",
    "\n",
    "    for child in children:\n",
    "        get_sequence(child, sequence)\n",
    "\n",
    "    if token in ['ForStatement', 'WhileStatement', 'DoStatement','SwitchStatement', 'IfStatement']:\n",
    "        sequence.append('End')\n",
    "\n",
    "\n",
    "def get_blocks_v1(node, block_seq):\n",
    "    name, children = get_token(node), get_children(node)\n",
    "    logic = ['SwitchStatement','IfStatement', 'ForStatement', 'WhileStatement', 'DoStatement']\n",
    "    if name in ['MethodDeclaration', 'ConstructorDeclaration']:\n",
    "        block_seq.append(BlockNode(node))\n",
    "        body = node.body\n",
    "        for child in body:\n",
    "            if get_token(child) not in logic and not hasattr(child, 'block'):\n",
    "                block_seq.append(BlockNode(child))\n",
    "            else:\n",
    "                get_blocks_v1(child, block_seq)\n",
    "    elif name in logic:\n",
    "        block_seq.append(BlockNode(node))\n",
    "        for child in children[1:]:\n",
    "            token = get_token(child)\n",
    "            if not hasattr(node, 'block') and token not in logic+['BlockStatement']:\n",
    "                block_seq.append(BlockNode(child))\n",
    "            else:\n",
    "                get_blocks_v1(child, block_seq)\n",
    "            block_seq.append(BlockNode('End'))\n",
    "    elif name is 'BlockStatement' or hasattr(node, 'block'):\n",
    "        block_seq.append(BlockNode(name))\n",
    "        for child in children:\n",
    "            if get_token(child)not in logic:\n",
    "                block_seq.append(BlockNode(child))\n",
    "            else:\n",
    "                get_blocks_v1(child, block_seq)\n",
    "    else:\n",
    "        for child in children:\n",
    "            get_blocks_v1(child, block_seq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# prepare_data.py\n",
    "from pycparser import c_parser, c_ast\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pickle\n",
    "from tree import ASTNode, SingleNode\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_sequences(node, sequence):\n",
    "    current = SingleNode(node)\n",
    "    sequence.append(current.get_token())\n",
    "    for _, child in node.children():\n",
    "        get_sequences(child, sequence)\n",
    "    if current.get_token().lower() == 'compound':\n",
    "        sequence.append('End')\n",
    "\n",
    "\n",
    "def get_blocks(node, block_seq):\n",
    "    children = node.children()\n",
    "    name = node.__class__.__name__\n",
    "    if name in ['FuncDef', 'If', 'For', 'While', 'DoWhile']:\n",
    "        block_seq.append(ASTNode(node))\n",
    "        if name is not 'For':\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = len(children) - 1\n",
    "\n",
    "        for i in range(skip, len(children)):\n",
    "            child = children[i][1]\n",
    "            if child.__class__.__name__ not in ['FuncDef', 'If', 'For', 'While', 'DoWhile', 'Compound']:\n",
    "                block_seq.append(ASTNode(child))\n",
    "            get_blocks(child, block_seq)\n",
    "    elif name is 'Compound':\n",
    "        block_seq.append(ASTNode(name))\n",
    "        for _, child in node.children():\n",
    "            if child.__class__.__name__ not in ['If', 'For', 'While', 'DoWhile']:\n",
    "                block_seq.append(ASTNode(child))\n",
    "            get_blocks(child, block_seq)\n",
    "        block_seq.append(ASTNode('End'))\n",
    "    else:\n",
    "        for _, child in node.children():\n",
    "            get_blocks(child, block_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# pipeline.py\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self,  ratio, root, language):\n",
    "        self.ratio = ratio\n",
    "        self.root = root\n",
    "        self.language = language\n",
    "        self.sources = None\n",
    "        self.blocks = None\n",
    "        self.pairs = None\n",
    "        self.train_file_path = None\n",
    "        self.dev_file_path = None\n",
    "        self.test_file_path = None\n",
    "        self.size = None\n",
    "\n",
    "    # parse source code\n",
    "    def parse_source(self, output_file, option):\n",
    "        path = self.root+self.language+'/'+output_file\n",
    "        if os.path.exists(path) and option == 'existing':\n",
    "            source = pd.read_pickle(path)\n",
    "        else:\n",
    "            if self.language is 'c':\n",
    "                from pycparser import c_parser\n",
    "                parser = c_parser.CParser()\n",
    "                source = pd.read_pickle(self.root+self.language+'/programs.pkl')\n",
    "                source.columns = ['id', 'code', 'label']\n",
    "                source['code'] = source['code'].apply(parser.parse)\n",
    "                source.to_pickle(path)\n",
    "            else:\n",
    "                import javalang\n",
    "                def parse_program(func):\n",
    "                    tokens = javalang.tokenizer.tokenize(func)\n",
    "                    parser = javalang.parser.Parser(tokens)\n",
    "                    tree = parser.parse_member_declaration()\n",
    "                    return tree\n",
    "                source = pd.read_csv(self.root+self.language+'/bcb_funcs_all.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "                source.columns = ['id', 'code']\n",
    "                source['code'] = source['code'].apply(parse_program)\n",
    "                source.to_pickle(path)\n",
    "        self.sources = source\n",
    "        return source\n",
    "\n",
    "    # create clone pairs\n",
    "    def read_pairs(self, filename):\n",
    "        pairs = pd.read_pickle(self.root+self.language+'/'+filename)\n",
    "        self.pairs = pairs\n",
    "\n",
    "    # split data for training, developing and testing\n",
    "    def split_data(self):\n",
    "        data_path = self.root+self.language+'/'\n",
    "        data = self.pairs\n",
    "        data_num = len(data)\n",
    "        ratios = [int(r) for r in self.ratio.split(':')]\n",
    "        train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "        val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "\n",
    "        data = data.sample(frac=1, random_state=666)\n",
    "        train = data.iloc[:train_split]\n",
    "        dev = data.iloc[train_split:val_split]\n",
    "        test = data.iloc[val_split:]\n",
    "\n",
    "        def check_or_create(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "        train_path = data_path+'train/'\n",
    "        check_or_create(train_path)\n",
    "        self.train_file_path = train_path+'train_.pkl'\n",
    "        train.to_pickle(self.train_file_path)\n",
    "\n",
    "        dev_path = data_path+'dev/'\n",
    "        check_or_create(dev_path)\n",
    "        self.dev_file_path = dev_path+'dev_.pkl'\n",
    "        dev.to_pickle(self.dev_file_path)\n",
    "\n",
    "        test_path = data_path+'test/'\n",
    "        check_or_create(test_path)\n",
    "        self.test_file_path = test_path+'test_.pkl'\n",
    "        test.to_pickle(self.test_file_path)\n",
    "\n",
    "    # construct dictionary and train word embedding\n",
    "    def dictionary_and_embedding(self, input_file, size):\n",
    "        self.size = size\n",
    "        data_path = self.root+self.language+'/'\n",
    "        if not input_file:\n",
    "            input_file = self.train_file_path\n",
    "        pairs = pd.read_pickle(input_file)\n",
    "        train_ids = pairs['id1'].append(pairs['id2']).unique()\n",
    "\n",
    "        trees = self.sources.set_index('id',drop=False).loc[train_ids]\n",
    "        if not os.path.exists(data_path+'train/embedding'):\n",
    "            os.mkdir(data_path+'train/embedding')\n",
    "        if self.language is 'c':\n",
    "            sys.path.append('../')\n",
    "            from prepare_data import get_sequences as func\n",
    "        else:\n",
    "            from utils import get_sequence as func\n",
    "\n",
    "        def trans_to_sequences(ast):\n",
    "            sequence = []\n",
    "            func(ast, sequence)\n",
    "            return sequence\n",
    "        corpus = trees['code'].apply(trans_to_sequences)\n",
    "        str_corpus = [' '.join(c) for c in corpus]\n",
    "        trees['code'] = pd.Series(str_corpus)\n",
    "        # trees.to_csv(data_path+'train/programs_ns.tsv')\n",
    "\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, max_final_vocab=3000)\n",
    "        w2v.save(data_path+'train/embedding/node_w2v_' + str(size))\n",
    "\n",
    "    # generate block sequences with index representations\n",
    "    def generate_block_seqs(self):\n",
    "        if self.language is 'c':\n",
    "            from prepare_data import get_blocks as func\n",
    "        else:\n",
    "            from utils import get_blocks_v1 as func\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "        word2vec = Word2Vec.load(self.root+self.language+'/train/embedding/node_w2v_' + str(self.size)).wv\n",
    "        vocab = word2vec.vocab\n",
    "        max_token = word2vec.syn0.shape[0]\n",
    "\n",
    "        def tree_to_index(node):\n",
    "            token = node.token\n",
    "            result = [vocab[token].index if token in vocab else max_token]\n",
    "            children = node.children\n",
    "            for child in children:\n",
    "                result.append(tree_to_index(child))\n",
    "            return result\n",
    "\n",
    "        def trans2seq(r):\n",
    "            blocks = []\n",
    "            func(r, blocks)\n",
    "            tree = []\n",
    "            for b in blocks:\n",
    "                btree = tree_to_index(b)\n",
    "                tree.append(btree)\n",
    "            return tree\n",
    "        trees = pd.DataFrame(self.sources, copy=True)\n",
    "        trees['code'] = trees['code'].apply(trans2seq)\n",
    "        if 'label' in trees.columns:\n",
    "            trees.drop('label', axis=1, inplace=True)\n",
    "        self.blocks = trees\n",
    "\n",
    "    # merge pairs\n",
    "    def merge(self,data_path,part):\n",
    "        pairs = pd.read_pickle(data_path)\n",
    "        pairs['id1'] = pairs['id1'].astype(int)\n",
    "        pairs['id2'] = pairs['id2'].astype(int)\n",
    "        df = pd.merge(pairs, self.blocks, how='left', left_on='id1', right_on='id')\n",
    "        df = pd.merge(df, self.blocks, how='left', left_on='id2', right_on='id')\n",
    "        df.drop(['id_x', 'id_y'], axis=1,inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        df.to_pickle(self.root+self.language+'/'+part+'/blocks.pkl')\n",
    "\n",
    "    # run for processing data to train\n",
    "    def run(self):\n",
    "        print('parse source code...')\n",
    "        self.parse_source(output_file='ast.pkl',option='existing')\n",
    "        print('read id pairs...')\n",
    "        if self.language is 'c':\n",
    "            self.read_pairs('oj_clone_ids.pkl')\n",
    "        else:\n",
    "            self.read_pairs('bcb_pair_ids.pkl')\n",
    "        print('split data...')\n",
    "        self.split_data()\n",
    "        print('train word embedding...')\n",
    "        self.dictionary_and_embedding(None,128)\n",
    "        print('generate block sequences...')\n",
    "        self.generate_block_seqs()\n",
    "        print('merge pairs and blocks...')\n",
    "        self.merge(self.train_file_path, 'train')\n",
    "        self.merge(self.dev_file_path, 'dev')\n",
    "        self.merge(self.test_file_path, 'test')\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"Choose a dataset:[c|java]\")\n",
    "parser.add_argument('--lang')\n",
    "args = parser.parse_args()\n",
    "if not args.lang:\n",
    "    print(\"No specified dataset\")\n",
    "    exit(1)\n",
    "ppl = Pipeline('3:1:1', 'data/', str(args.lang))\n",
    "ppl.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
