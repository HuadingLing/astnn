{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T08:10:26.825080Z",
     "start_time": "2020-08-05T08:10:26.820093Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {'a': 13,\n",
    "               'b': {'datadir': '/tmp/minst',\n",
    "                     'batch_size': 1024,\n",
    "                     'num_workers': 4,\n",
    "                     'learning_rate': 0.01,\n",
    "                     'momentum': 0.5,\n",
    "                     'num_epochs': 20,\n",
    "                     'num_cores': 8,\n",
    "                     'log_steps': 20,\n",
    "                     'metrics_debug': False},\n",
    "               'c': [1, 2, 4]}\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4, separators=(',', ': '))\n",
    "#with open('result.json', 'r') as f:\n",
    "#    d = json.load(f)\n",
    "\n",
    "'''\n",
    "json.dumps: dict -> str\n",
    "json.dump: dict -> file\n",
    "json.loads: str -> dict\n",
    "json.load: file -> dict\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:19:26.407701Z",
     "start_time": "2020-08-05T11:19:26.401718Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
      "'{\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5}'\n",
      "{\n",
      "    \"a\": \"Runoob\",\n",
      "    \"b\": 7\n",
      "}\n",
      "{\n",
      "      \"a\":\"Runoob\",\n",
      "      \"b\":7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "data = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}\n",
    "print(data)\n",
    "pprint(data)\n",
    "data2 = json.dumps(data)\n",
    "pprint(data2)\n",
    "\n",
    "data2 = json.dumps({'a':'Runoob', 'b':7}, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "print(data2)\n",
    "data2 = json.dumps({'a':'Runoob', 'b':7}, sort_keys=True, indent=6, separators=(',', ':'))\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:33.306095Z",
     "start_time": "2020-09-21T06:38:29.576069Z"
    }
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramCC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.get_device_name(0) == 'GeForce GT 730':\n",
    "            device = 'cpu'\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return torch.device(device)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    x1, x2, labels = [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        x2.append(item['code_y'])\n",
    "        labels.append([item['label']])\n",
    "    return x1, x2, torch.FloatTensor(labels)\n",
    "\n",
    "def train(model, train_data, batch_size, device, epochs, epoch, optimizer):\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    while i < len(train_data):\n",
    "        train1_inputs, train2_inputs, train_labels = get_batch(train_data, i, batch_size)\n",
    "        i += len(train_labels)\n",
    "        train_labels = train_labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train1_inputs, train2_inputs)\n",
    "        #print(output.shape)\n",
    "\n",
    "        loss = loss_function(output, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # acc\n",
    "        total_loss += loss.item() * len(train_labels)\n",
    "        predicted = np.where(output.data.cpu().numpy()<0.5, 1, 0)\n",
    "        total_trues += (predicted==train_labels.cpu().numpy()).sum()\n",
    "        total_acc = total_trues / i\n",
    "\n",
    "        print('[Epoch:%3d/%3d] [data: %d/%d] Training Loss: %.4f Training Acc: %.4f%%'\n",
    "              % (epoch + 1, epochs, i, len(train_data), loss, total_acc*100))\n",
    "    return total_loss/len(train_data), total_acc\n",
    "\n",
    "def validation(model, validation_data, batch_size, device, epochs, epoch):\n",
    "    trues = []\n",
    "    total_trues = 0\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    with torch.no_grad():\n",
    "        while i < len(validation_data):\n",
    "            validation1_inputs, validation2_inputs, validation_labels = get_batch(validation_data, i, batch_size)\n",
    "            i += len(validation_labels)\n",
    "            test_labels = validation_labels.to(device)\n",
    "\n",
    "            model.batch_size = len(validation_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(validation1_inputs, validation2_inputs)\n",
    "\n",
    "            loss = loss_function(output, validation_labels)\n",
    "\n",
    "            # calc testing acc\n",
    "            predicted = (output.data < 0.5).cpu().numpy()\n",
    "            trues_ex = test_labels.cpu().numpy()\n",
    "            trues.extend(trues_ex)\n",
    "            total_trues += np.where(predicted==trues_ex, 1, 0).sum()\n",
    "            total_loss += loss.item() * len(validation_labels)\n",
    "    \n",
    "    total_loss /= len(validation_data)\n",
    "    total_acc = total_trues / len(validation_data)\n",
    "    print('[Epoch:%3d/%3d] Validation Loss: %.4f Validation Acc: %.4f%%'\n",
    "              % (epoch + 1, epochs, total_loss, total_acc*100))\n",
    "    return total_loss, total_acc\n",
    "\n",
    "def test(model, test_data, batch_size, device):\n",
    "    global precision, recall, f1\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_trues = 0\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    total_trues = 0\n",
    "    with torch.no_grad():\n",
    "        while i < len(test_data):\n",
    "            test1_inputs, test2_inputs, test_labels = get_batch(test_data, i, batch_size)\n",
    "            i += len(test_labels)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test1_inputs, test2_inputs)\n",
    "            loss = loss_function(output, test_labels)\n",
    "\n",
    "            # calc testing acc\n",
    "            predicted = (output.data < 0.5).cpu().numpy()\n",
    "            predicts.extend(predicted)\n",
    "            trues_ex = test_labels.cpu().numpy()\n",
    "            trues.extend(trues_ex)\n",
    "            total_trues += np.where(predicted==trues_ex, 1, 0).sum()\n",
    "            total_loss += loss.item() * len(test_labels)\n",
    "\n",
    "    total_acc = total_trues / len(test_data)\n",
    "    \n",
    "    if lang == 'java':\n",
    "        weights = [0, 0.005, 0.001, 0.002, 0.010, 0.982]\n",
    "        p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "        precision += weights[t] * p\n",
    "        recall += weights[t] * r\n",
    "        f1 += weights[t] * f\n",
    "        print(\"Type-\" + str(t) + \": \" + str(p) + \" \" + str(r) + \" \" + str(f))\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "        print(\"Testing results(P,R,F1):%.3f, %.3f, %.3f\" \n",
    "              % (precision, recall, f1))\n",
    "        \n",
    "    return total_loss, total_acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T15:37:12.074999Z",
     "start_time": "2020-08-12T15:37:06.344788Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "word2vec = Word2Vec.load(root+lang+\"/train/embedding/node_w2v_128\").wv\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:44:43.344345Z",
     "start_time": "2020-09-08T13:44:22.055733Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "#model= Word2Vec(min_count=4, size=200, workers=6, max_final_vocab=1000000)\n",
    "#model.load('./word2vec_Model/word2vec')\n",
    "word2vec = Word2Vec.load('./word2vec_Model/word2vec').wv\n",
    "\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:38.862354Z",
     "start_time": "2020-09-21T06:38:33.307090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  JAVA\n"
     ]
    }
   ],
   "source": [
    "root = 'data/'\n",
    "lang = 'java'\n",
    "categories = 1\n",
    "if lang == 'java':\n",
    "    categories = 5\n",
    "print(\"Train for \", str.upper(lang))\n",
    "train_data = pd.read_pickle(root+lang+'/train/blocks.pkl').sample(frac=1.0)\n",
    "validation_data = pd.read_pickle(root+lang+'/dev/blocks.pkl').sample(frac=1.0)\n",
    "test_data = pd.read_pickle(root+lang+'/test/blocks.pkl').sample(frac=1.0)\n",
    "\n",
    "word2vec = Word2Vec.load(root+lang+\"/train/embedding/node_w2v_128\").wv\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:38:38.869245Z",
     "start_time": "2020-09-21T06:38:38.864228Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, enclidean_distance, label):\n",
    "        #enclidean_distance = F.pairwise_distance(output[0], output[1])\n",
    "        loss_contrastive = torch.mean(label*torch.pow(enclidean_distance, 2) +\n",
    "                                     (1-label) * torch.pow(torch.clamp(self.margin - enclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:59:35.659925Z",
     "start_time": "2020-09-21T06:38:38.871210Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[Epoch:  1/  2] [data: 128/21362] Training Loss: 0.1335 Training Acc: 84.3750%\n",
      "[Epoch:  1/  2] [data: 256/21362] Training Loss: 0.0271 Training Acc: 92.1875%\n",
      "[Epoch:  1/  2] [data: 384/21362] Training Loss: 0.0038 Training Acc: 94.7917%\n",
      "[Epoch:  1/  2] [data: 512/21362] Training Loss: 0.0006 Training Acc: 96.0938%\n",
      "[Epoch:  1/  2] [data: 640/21362] Training Loss: 0.0037 Training Acc: 96.8750%\n",
      "[Epoch:  1/  2] [data: 768/21362] Training Loss: 0.0037 Training Acc: 97.3958%\n",
      "[Epoch:  1/  2] [data: 896/21362] Training Loss: 0.0002 Training Acc: 97.7679%\n",
      "[Epoch:  1/  2] [data: 1024/21362] Training Loss: 0.0001 Training Acc: 98.0469%\n",
      "[Epoch:  1/  2] [data: 1152/21362] Training Loss: 0.0001 Training Acc: 98.2639%\n",
      "[Epoch:  1/  2] [data: 1280/21362] Training Loss: 0.0000 Training Acc: 98.4375%\n",
      "[Epoch:  1/  2] [data: 1408/21362] Training Loss: 0.0001 Training Acc: 98.5795%\n",
      "[Epoch:  1/  2] [data: 1536/21362] Training Loss: 0.0027 Training Acc: 98.6979%\n",
      "[Epoch:  1/  2] [data: 1664/21362] Training Loss: 0.0001 Training Acc: 98.7981%\n",
      "[Epoch:  1/  2] [data: 1792/21362] Training Loss: 0.0004 Training Acc: 98.8839%\n",
      "[Epoch:  1/  2] [data: 1920/21362] Training Loss: 0.0002 Training Acc: 98.9583%\n",
      "[Epoch:  1/  2] [data: 2048/21362] Training Loss: 0.0001 Training Acc: 99.0234%\n",
      "[Epoch:  1/  2] [data: 2176/21362] Training Loss: 0.0001 Training Acc: 99.0809%\n",
      "[Epoch:  1/  2] [data: 2304/21362] Training Loss: 0.0000 Training Acc: 99.1319%\n",
      "[Epoch:  1/  2] [data: 2432/21362] Training Loss: 0.0000 Training Acc: 99.1776%\n",
      "[Epoch:  1/  2] [data: 2560/21362] Training Loss: 0.0001 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 2688/21362] Training Loss: 0.0000 Training Acc: 99.2560%\n",
      "[Epoch:  1/  2] [data: 2816/21362] Training Loss: 0.0004 Training Acc: 99.2898%\n",
      "[Epoch:  1/  2] [data: 2944/21362] Training Loss: 0.0000 Training Acc: 99.3207%\n",
      "[Epoch:  1/  2] [data: 3072/21362] Training Loss: 0.0002 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 3200/21362] Training Loss: 0.0005 Training Acc: 99.3750%\n",
      "[Epoch:  1/  2] [data: 3328/21362] Training Loss: 0.0003 Training Acc: 99.3990%\n",
      "[Epoch:  1/  2] [data: 3456/21362] Training Loss: 0.0006 Training Acc: 99.4213%\n",
      "[Epoch:  1/  2] [data: 3584/21362] Training Loss: 0.0000 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 3712/21362] Training Loss: 0.0000 Training Acc: 99.4612%\n",
      "[Epoch:  1/  2] [data: 3840/21362] Training Loss: 0.0001 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 3968/21362] Training Loss: 0.0000 Training Acc: 99.4960%\n",
      "[Epoch:  1/  2] [data: 4096/21362] Training Loss: 0.0000 Training Acc: 99.5117%\n",
      "[Epoch:  1/  2] [data: 4224/21362] Training Loss: 0.0000 Training Acc: 99.5265%\n",
      "[Epoch:  1/  2] [data: 4352/21362] Training Loss: 0.0000 Training Acc: 99.5404%\n",
      "[Epoch:  1/  2] [data: 4480/21362] Training Loss: 0.0003 Training Acc: 99.5536%\n",
      "[Epoch:  1/  2] [data: 4608/21362] Training Loss: 0.0001 Training Acc: 99.5660%\n",
      "[Epoch:  1/  2] [data: 4736/21362] Training Loss: 0.0000 Training Acc: 99.5777%\n",
      "[Epoch:  1/  2] [data: 4864/21362] Training Loss: 0.0000 Training Acc: 99.5888%\n",
      "[Epoch:  1/  2] [data: 4992/21362] Training Loss: 0.0001 Training Acc: 99.5994%\n",
      "[Epoch:  1/  2] [data: 5120/21362] Training Loss: 0.0003 Training Acc: 99.6094%\n",
      "[Epoch:  1/  2] [data: 5248/21362] Training Loss: 0.0001 Training Acc: 99.6189%\n",
      "[Epoch:  1/  2] [data: 5376/21362] Training Loss: 0.0001 Training Acc: 99.6280%\n",
      "[Epoch:  1/  2] [data: 5504/21362] Training Loss: 0.0003 Training Acc: 99.6366%\n",
      "[Epoch:  1/  2] [data: 5632/21362] Training Loss: 0.0001 Training Acc: 99.6449%\n",
      "[Epoch:  1/  2] [data: 5760/21362] Training Loss: 0.0000 Training Acc: 99.6528%\n",
      "[Epoch:  1/  2] [data: 5888/21362] Training Loss: 0.0000 Training Acc: 99.6603%\n",
      "[Epoch:  1/  2] [data: 6016/21362] Training Loss: 0.0000 Training Acc: 99.6676%\n",
      "[Epoch:  1/  2] [data: 6144/21362] Training Loss: 0.0000 Training Acc: 99.6745%\n",
      "[Epoch:  1/  2] [data: 6272/21362] Training Loss: 0.0000 Training Acc: 99.6811%\n",
      "[Epoch:  1/  2] [data: 6400/21362] Training Loss: 0.0000 Training Acc: 99.6875%\n",
      "[Epoch:  1/  2] [data: 6528/21362] Training Loss: 0.0000 Training Acc: 99.6936%\n",
      "[Epoch:  1/  2] [data: 6656/21362] Training Loss: 0.0000 Training Acc: 99.6995%\n",
      "[Epoch:  1/  2] [data: 6784/21362] Training Loss: 0.0000 Training Acc: 99.7052%\n",
      "[Epoch:  1/  2] [data: 6912/21362] Training Loss: 0.0000 Training Acc: 99.7106%\n",
      "[Epoch:  1/  2] [data: 7040/21362] Training Loss: 0.0000 Training Acc: 99.7159%\n",
      "[Epoch:  1/  2] [data: 7168/21362] Training Loss: 0.0000 Training Acc: 99.7210%\n",
      "[Epoch:  1/  2] [data: 7296/21362] Training Loss: 0.0000 Training Acc: 99.7259%\n",
      "[Epoch:  1/  2] [data: 7424/21362] Training Loss: 0.0000 Training Acc: 99.7306%\n",
      "[Epoch:  1/  2] [data: 7552/21362] Training Loss: 0.0000 Training Acc: 99.7352%\n",
      "[Epoch:  1/  2] [data: 7680/21362] Training Loss: 0.0000 Training Acc: 99.7396%\n",
      "[Epoch:  1/  2] [data: 7808/21362] Training Loss: 0.0000 Training Acc: 99.7439%\n",
      "[Epoch:  1/  2] [data: 7936/21362] Training Loss: 0.0001 Training Acc: 99.7480%\n",
      "[Epoch:  1/  2] [data: 8064/21362] Training Loss: 0.0000 Training Acc: 99.7520%\n",
      "[Epoch:  1/  2] [data: 8192/21362] Training Loss: 0.0000 Training Acc: 99.7559%\n",
      "[Epoch:  1/  2] [data: 8320/21362] Training Loss: 0.0000 Training Acc: 99.7596%\n",
      "[Epoch:  1/  2] [data: 8448/21362] Training Loss: 0.0000 Training Acc: 99.7633%\n",
      "[Epoch:  1/  2] [data: 8576/21362] Training Loss: 0.0000 Training Acc: 99.7668%\n",
      "[Epoch:  1/  2] [data: 8704/21362] Training Loss: 0.0000 Training Acc: 99.7702%\n",
      "[Epoch:  1/  2] [data: 8832/21362] Training Loss: 0.0000 Training Acc: 99.7736%\n",
      "[Epoch:  1/  2] [data: 8960/21362] Training Loss: 0.0000 Training Acc: 99.7768%\n",
      "[Epoch:  1/  2] [data: 9088/21362] Training Loss: 0.0000 Training Acc: 99.7799%\n",
      "[Epoch:  1/  2] [data: 9216/21362] Training Loss: 0.0000 Training Acc: 99.7830%\n",
      "[Epoch:  1/  2] [data: 9344/21362] Training Loss: 0.0000 Training Acc: 99.7860%\n",
      "[Epoch:  1/  2] [data: 9472/21362] Training Loss: 0.0000 Training Acc: 99.7889%\n",
      "[Epoch:  1/  2] [data: 9600/21362] Training Loss: 0.0000 Training Acc: 99.7917%\n",
      "[Epoch:  1/  2] [data: 9728/21362] Training Loss: 0.0000 Training Acc: 99.7944%\n",
      "[Epoch:  1/  2] [data: 9856/21362] Training Loss: 0.0000 Training Acc: 99.7971%\n",
      "[Epoch:  1/  2] [data: 9984/21362] Training Loss: 0.0000 Training Acc: 99.7997%\n",
      "[Epoch:  1/  2] [data: 10112/21362] Training Loss: 0.0000 Training Acc: 99.8022%\n",
      "[Epoch:  1/  2] [data: 10240/21362] Training Loss: 0.0000 Training Acc: 99.8047%\n",
      "[Epoch:  1/  2] [data: 10368/21362] Training Loss: 0.0000 Training Acc: 99.8071%\n",
      "[Epoch:  1/  2] [data: 10496/21362] Training Loss: 0.0053 Training Acc: 99.8095%\n",
      "[Epoch:  1/  2] [data: 10624/21362] Training Loss: 0.0000 Training Acc: 99.8117%\n",
      "[Epoch:  1/  2] [data: 10752/21362] Training Loss: 0.0000 Training Acc: 99.8140%\n",
      "[Epoch:  1/  2] [data: 10880/21362] Training Loss: 0.0000 Training Acc: 99.8162%\n",
      "[Epoch:  1/  2] [data: 11008/21362] Training Loss: 0.0000 Training Acc: 99.8183%\n",
      "[Epoch:  1/  2] [data: 11136/21362] Training Loss: 0.0001 Training Acc: 99.8204%\n",
      "[Epoch:  1/  2] [data: 11264/21362] Training Loss: 0.0000 Training Acc: 99.8224%\n",
      "[Epoch:  1/  2] [data: 11392/21362] Training Loss: 0.0000 Training Acc: 99.8244%\n",
      "[Epoch:  1/  2] [data: 11520/21362] Training Loss: 0.0000 Training Acc: 99.8264%\n",
      "[Epoch:  1/  2] [data: 11648/21362] Training Loss: 0.0000 Training Acc: 99.8283%\n",
      "[Epoch:  1/  2] [data: 11776/21362] Training Loss: 0.0000 Training Acc: 99.8302%\n",
      "[Epoch:  1/  2] [data: 11904/21362] Training Loss: 0.0000 Training Acc: 99.8320%\n",
      "[Epoch:  1/  2] [data: 12032/21362] Training Loss: 0.0000 Training Acc: 99.8338%\n",
      "[Epoch:  1/  2] [data: 12160/21362] Training Loss: 0.0000 Training Acc: 99.8355%\n",
      "[Epoch:  1/  2] [data: 12288/21362] Training Loss: 0.0000 Training Acc: 99.8372%\n",
      "[Epoch:  1/  2] [data: 12416/21362] Training Loss: 0.0000 Training Acc: 99.8389%\n",
      "[Epoch:  1/  2] [data: 12544/21362] Training Loss: 0.0000 Training Acc: 99.8406%\n",
      "[Epoch:  1/  2] [data: 12672/21362] Training Loss: 0.0000 Training Acc: 99.8422%\n",
      "[Epoch:  1/  2] [data: 12800/21362] Training Loss: 0.0000 Training Acc: 99.8438%\n",
      "[Epoch:  1/  2] [data: 12928/21362] Training Loss: 0.0002 Training Acc: 99.8453%\n",
      "[Epoch:  1/  2] [data: 13056/21362] Training Loss: 0.0000 Training Acc: 99.8468%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 13184/21362] Training Loss: 0.0000 Training Acc: 99.8483%\n",
      "[Epoch:  1/  2] [data: 13312/21362] Training Loss: 0.0000 Training Acc: 99.8498%\n",
      "[Epoch:  1/  2] [data: 13440/21362] Training Loss: 0.0000 Training Acc: 99.8512%\n",
      "[Epoch:  1/  2] [data: 13568/21362] Training Loss: 0.0000 Training Acc: 99.8526%\n",
      "[Epoch:  1/  2] [data: 13696/21362] Training Loss: 0.0000 Training Acc: 99.8540%\n",
      "[Epoch:  1/  2] [data: 13824/21362] Training Loss: 0.0000 Training Acc: 99.8553%\n",
      "[Epoch:  1/  2] [data: 13952/21362] Training Loss: 0.0000 Training Acc: 99.8567%\n",
      "[Epoch:  1/  2] [data: 14080/21362] Training Loss: 0.0000 Training Acc: 99.8580%\n",
      "[Epoch:  1/  2] [data: 14208/21362] Training Loss: 0.0006 Training Acc: 99.8592%\n",
      "[Epoch:  1/  2] [data: 14336/21362] Training Loss: 0.0000 Training Acc: 99.8605%\n",
      "[Epoch:  1/  2] [data: 14464/21362] Training Loss: 0.0000 Training Acc: 99.8617%\n",
      "[Epoch:  1/  2] [data: 14592/21362] Training Loss: 0.0000 Training Acc: 99.8629%\n",
      "[Epoch:  1/  2] [data: 14720/21362] Training Loss: 0.0000 Training Acc: 99.8641%\n",
      "[Epoch:  1/  2] [data: 14848/21362] Training Loss: 0.0000 Training Acc: 99.8653%\n",
      "[Epoch:  1/  2] [data: 14976/21362] Training Loss: 0.0000 Training Acc: 99.8665%\n",
      "[Epoch:  1/  2] [data: 15104/21362] Training Loss: 0.0000 Training Acc: 99.8676%\n",
      "[Epoch:  1/  2] [data: 15232/21362] Training Loss: 0.0095 Training Acc: 99.8687%\n",
      "[Epoch:  1/  2] [data: 15360/21362] Training Loss: 0.0000 Training Acc: 99.8698%\n",
      "[Epoch:  1/  2] [data: 15488/21362] Training Loss: 0.0007 Training Acc: 99.8709%\n",
      "[Epoch:  1/  2] [data: 15616/21362] Training Loss: 0.0000 Training Acc: 99.8719%\n",
      "[Epoch:  1/  2] [data: 15744/21362] Training Loss: 0.0000 Training Acc: 99.8730%\n",
      "[Epoch:  1/  2] [data: 15872/21362] Training Loss: 0.0040 Training Acc: 99.8740%\n",
      "[Epoch:  1/  2] [data: 16000/21362] Training Loss: 0.0000 Training Acc: 99.8750%\n",
      "[Epoch:  1/  2] [data: 16128/21362] Training Loss: 0.0000 Training Acc: 99.8760%\n",
      "[Epoch:  1/  2] [data: 16256/21362] Training Loss: 0.0000 Training Acc: 99.8770%\n",
      "[Epoch:  1/  2] [data: 16384/21362] Training Loss: 0.0000 Training Acc: 99.8779%\n",
      "[Epoch:  1/  2] [data: 16512/21362] Training Loss: 0.0000 Training Acc: 99.8789%\n",
      "[Epoch:  1/  2] [data: 16640/21362] Training Loss: 0.0000 Training Acc: 99.8798%\n",
      "[Epoch:  1/  2] [data: 16768/21362] Training Loss: 0.0000 Training Acc: 99.8807%\n",
      "[Epoch:  1/  2] [data: 16896/21362] Training Loss: 0.0000 Training Acc: 99.8816%\n",
      "[Epoch:  1/  2] [data: 17024/21362] Training Loss: 0.0000 Training Acc: 99.8825%\n",
      "[Epoch:  1/  2] [data: 17152/21362] Training Loss: 0.0000 Training Acc: 99.8834%\n",
      "[Epoch:  1/  2] [data: 17280/21362] Training Loss: 0.0000 Training Acc: 99.8843%\n",
      "[Epoch:  1/  2] [data: 17408/21362] Training Loss: 0.0000 Training Acc: 99.8851%\n",
      "[Epoch:  1/  2] [data: 17536/21362] Training Loss: 0.0000 Training Acc: 99.8859%\n",
      "[Epoch:  1/  2] [data: 17664/21362] Training Loss: 0.0000 Training Acc: 99.8868%\n",
      "[Epoch:  1/  2] [data: 17792/21362] Training Loss: 0.0000 Training Acc: 99.8876%\n",
      "[Epoch:  1/  2] [data: 17920/21362] Training Loss: 0.0000 Training Acc: 99.8884%\n",
      "[Epoch:  1/  2] [data: 18048/21362] Training Loss: 0.0000 Training Acc: 99.8892%\n",
      "[Epoch:  1/  2] [data: 18176/21362] Training Loss: 0.0002 Training Acc: 99.8900%\n",
      "[Epoch:  1/  2] [data: 18304/21362] Training Loss: 0.0001 Training Acc: 99.8907%\n",
      "[Epoch:  1/  2] [data: 18432/21362] Training Loss: 0.0000 Training Acc: 99.8915%\n",
      "[Epoch:  1/  2] [data: 18560/21362] Training Loss: 0.0000 Training Acc: 99.8922%\n",
      "[Epoch:  1/  2] [data: 18688/21362] Training Loss: 0.0000 Training Acc: 99.8930%\n",
      "[Epoch:  1/  2] [data: 18816/21362] Training Loss: 0.0000 Training Acc: 99.8937%\n",
      "[Epoch:  1/  2] [data: 18944/21362] Training Loss: 0.0000 Training Acc: 99.8944%\n",
      "[Epoch:  1/  2] [data: 19072/21362] Training Loss: 0.0000 Training Acc: 99.8951%\n",
      "[Epoch:  1/  2] [data: 19200/21362] Training Loss: 0.0008 Training Acc: 99.8958%\n",
      "[Epoch:  1/  2] [data: 19328/21362] Training Loss: 0.0000 Training Acc: 99.8965%\n",
      "[Epoch:  1/  2] [data: 19456/21362] Training Loss: 0.0000 Training Acc: 99.8972%\n",
      "[Epoch:  1/  2] [data: 19584/21362] Training Loss: 0.0000 Training Acc: 99.8979%\n",
      "[Epoch:  1/  2] [data: 19712/21362] Training Loss: 0.0000 Training Acc: 99.8985%\n",
      "[Epoch:  1/  2] [data: 19840/21362] Training Loss: 0.0000 Training Acc: 99.8992%\n",
      "[Epoch:  1/  2] [data: 19968/21362] Training Loss: 0.0000 Training Acc: 99.8998%\n",
      "[Epoch:  1/  2] [data: 20096/21362] Training Loss: 0.0000 Training Acc: 99.9005%\n",
      "[Epoch:  1/  2] [data: 20224/21362] Training Loss: 0.0000 Training Acc: 99.9011%\n",
      "[Epoch:  1/  2] [data: 20352/21362] Training Loss: 0.0000 Training Acc: 99.9017%\n",
      "[Epoch:  1/  2] [data: 20480/21362] Training Loss: 0.0000 Training Acc: 99.9023%\n",
      "[Epoch:  1/  2] [data: 20608/21362] Training Loss: 0.0000 Training Acc: 99.9030%\n",
      "[Epoch:  1/  2] [data: 20736/21362] Training Loss: 0.0000 Training Acc: 99.9035%\n",
      "[Epoch:  1/  2] [data: 20864/21362] Training Loss: 0.0000 Training Acc: 99.9041%\n",
      "[Epoch:  1/  2] [data: 20992/21362] Training Loss: 0.0000 Training Acc: 99.9047%\n",
      "[Epoch:  1/  2] [data: 21120/21362] Training Loss: 0.0000 Training Acc: 99.9053%\n",
      "[Epoch:  1/  2] [data: 21248/21362] Training Loss: 0.0000 Training Acc: 99.9059%\n",
      "[Epoch:  1/  2] [data: 21362/21362] Training Loss: 0.0000 Training Acc: 99.9064%\n",
      "Testing-1...\n",
      "[Epoch:  1/  2] Validation Loss: 0.0000 Validation Acc: 100.0000%\n",
      "Time used: 1251.2459571361542s\n",
      "[Epoch:  2/  2] [data: 128/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 256/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 384/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 512/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 640/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 768/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 896/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1024/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1152/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1280/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1408/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1536/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1664/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1792/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 1920/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2048/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2176/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2304/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2432/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2560/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2688/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2816/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 2944/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3072/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3200/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3328/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3456/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3584/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3712/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3840/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 3968/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4096/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4224/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4352/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4480/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 4608/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4736/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4864/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 4992/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5120/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5248/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5376/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5504/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5632/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5760/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 5888/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6016/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6144/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6272/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6400/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6528/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6656/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6784/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 6912/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7040/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7168/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7296/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7424/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7552/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7680/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7808/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 7936/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8064/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8192/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8320/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8448/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8576/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8704/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8832/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 8960/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9088/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9216/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9344/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9472/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9600/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9728/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9856/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 9984/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10112/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10240/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10368/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10496/21362] Training Loss: 0.0014 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10624/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10752/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 10880/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11008/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11136/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11264/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11392/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11520/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11648/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11776/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 11904/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12032/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12160/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12288/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12416/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12544/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12672/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12800/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 12928/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13056/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13184/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13312/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13440/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13568/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13696/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13824/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 13952/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14080/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14208/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14336/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14464/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14592/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14720/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14848/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 14976/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15104/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15232/21362] Training Loss: 0.0066 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15360/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15488/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15616/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15744/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 15872/21362] Training Loss: 0.0008 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16000/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16128/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16256/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16384/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16512/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16640/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16768/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 16896/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17024/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17152/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17280/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17408/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 17536/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17664/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17792/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 17920/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18048/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18176/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18304/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18432/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18560/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18688/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18816/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 18944/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19072/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19200/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19328/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19456/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19584/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19712/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19840/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 19968/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20096/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20224/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20352/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20480/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20608/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20736/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20864/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 20992/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21120/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21248/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 21362/21362] Training Loss: 0.0000 Training Acc: 100.0000%\n",
      "Testing-1...\n",
      "[Epoch:  2/  2] Validation Loss: 0.0000 Validation Acc: 100.0000%\n",
      "Time used: 1307.4331023693085s\n",
      "Type-1: 1.0 1.0 1.0\n",
      "[Epoch:  1/  2] [data: 128/14329] Training Loss: 0.0373 Training Acc: 98.4375%\n",
      "[Epoch:  1/  2] [data: 256/14329] Training Loss: 0.0165 Training Acc: 98.8281%\n",
      "[Epoch:  1/  2] [data: 384/14329] Training Loss: 0.0142 Training Acc: 98.9583%\n",
      "[Epoch:  1/  2] [data: 512/14329] Training Loss: 0.0069 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 640/14329] Training Loss: 0.0122 Training Acc: 99.2188%\n",
      "[Epoch:  1/  2] [data: 768/14329] Training Loss: 0.0028 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 896/14329] Training Loss: 0.0004 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 1024/14329] Training Loss: 0.0029 Training Acc: 99.5117%\n",
      "[Epoch:  1/  2] [data: 1152/14329] Training Loss: 0.0222 Training Acc: 99.3056%\n",
      "[Epoch:  1/  2] [data: 1280/14329] Training Loss: 0.0117 Training Acc: 99.2969%\n",
      "[Epoch:  1/  2] [data: 1408/14329] Training Loss: 0.0044 Training Acc: 99.3608%\n",
      "[Epoch:  1/  2] [data: 1536/14329] Training Loss: 0.0110 Training Acc: 99.3490%\n",
      "[Epoch:  1/  2] [data: 1664/14329] Training Loss: 0.0027 Training Acc: 99.3990%\n",
      "[Epoch:  1/  2] [data: 1792/14329] Training Loss: 0.0115 Training Acc: 99.3862%\n",
      "[Epoch:  1/  2] [data: 1920/14329] Training Loss: 0.0073 Training Acc: 99.3750%\n",
      "[Epoch:  1/  2] [data: 2048/14329] Training Loss: 0.0060 Training Acc: 99.3652%\n",
      "[Epoch:  1/  2] [data: 2176/14329] Training Loss: 0.0032 Training Acc: 99.4026%\n",
      "[Epoch:  1/  2] [data: 2304/14329] Training Loss: 0.0066 Training Acc: 99.3924%\n",
      "[Epoch:  1/  2] [data: 2432/14329] Training Loss: 0.0140 Training Acc: 99.3832%\n",
      "[Epoch:  1/  2] [data: 2560/14329] Training Loss: 0.0086 Training Acc: 99.3359%\n",
      "[Epoch:  1/  2] [data: 2688/14329] Training Loss: 0.0016 Training Acc: 99.3676%\n",
      "[Epoch:  1/  2] [data: 2816/14329] Training Loss: 0.0014 Training Acc: 99.3963%\n",
      "[Epoch:  1/  2] [data: 2944/14329] Training Loss: 0.0053 Training Acc: 99.4226%\n",
      "[Epoch:  1/  2] [data: 3072/14329] Training Loss: 0.0013 Training Acc: 99.4466%\n",
      "[Epoch:  1/  2] [data: 3200/14329] Training Loss: 0.0049 Training Acc: 99.4375%\n",
      "[Epoch:  1/  2] [data: 3328/14329] Training Loss: 0.0049 Training Acc: 99.4291%\n",
      "[Epoch:  1/  2] [data: 3456/14329] Training Loss: 0.0057 Training Acc: 99.4213%\n",
      "[Epoch:  1/  2] [data: 3584/14329] Training Loss: 0.0018 Training Acc: 99.4420%\n",
      "[Epoch:  1/  2] [data: 3712/14329] Training Loss: 0.0020 Training Acc: 99.4612%\n",
      "[Epoch:  1/  2] [data: 3840/14329] Training Loss: 0.0017 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 3968/14329] Training Loss: 0.0012 Training Acc: 99.4960%\n",
      "[Epoch:  1/  2] [data: 4096/14329] Training Loss: 0.0170 Training Acc: 99.4629%\n",
      "[Epoch:  1/  2] [data: 4224/14329] Training Loss: 0.0006 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 4352/14329] Training Loss: 0.0024 Training Acc: 99.4715%\n",
      "[Epoch:  1/  2] [data: 4480/14329] Training Loss: 0.0097 Training Acc: 99.4196%\n",
      "[Epoch:  1/  2] [data: 4608/14329] Training Loss: 0.0014 Training Acc: 99.4358%\n",
      "[Epoch:  1/  2] [data: 4736/14329] Training Loss: 0.0008 Training Acc: 99.4510%\n",
      "[Epoch:  1/  2] [data: 4864/14329] Training Loss: 0.0018 Training Acc: 99.4655%\n",
      "[Epoch:  1/  2] [data: 4992/14329] Training Loss: 0.0004 Training Acc: 99.4792%\n",
      "[Epoch:  1/  2] [data: 5120/14329] Training Loss: 0.0082 Training Acc: 99.4727%\n",
      "[Epoch:  1/  2] [data: 5248/14329] Training Loss: 0.0107 Training Acc: 99.4474%\n",
      "[Epoch:  1/  2] [data: 5376/14329] Training Loss: 0.0019 Training Acc: 99.4606%\n",
      "[Epoch:  1/  2] [data: 5504/14329] Training Loss: 0.0087 Training Acc: 99.4368%\n",
      "[Epoch:  1/  2] [data: 5632/14329] Training Loss: 0.0004 Training Acc: 99.4496%\n",
      "[Epoch:  1/  2] [data: 5760/14329] Training Loss: 0.0008 Training Acc: 99.4618%\n",
      "[Epoch:  1/  2] [data: 5888/14329] Training Loss: 0.0108 Training Acc: 99.4395%\n",
      "[Epoch:  1/  2] [data: 6016/14329] Training Loss: 0.0090 Training Acc: 99.4348%\n",
      "[Epoch:  1/  2] [data: 6144/14329] Training Loss: 0.0035 Training Acc: 99.4466%\n",
      "[Epoch:  1/  2] [data: 6272/14329] Training Loss: 0.0024 Training Acc: 99.4579%\n",
      "[Epoch:  1/  2] [data: 6400/14329] Training Loss: 0.0006 Training Acc: 99.4688%\n",
      "[Epoch:  1/  2] [data: 6528/14329] Training Loss: 0.0041 Training Acc: 99.4638%\n",
      "[Epoch:  1/  2] [data: 6656/14329] Training Loss: 0.0052 Training Acc: 99.4742%\n",
      "[Epoch:  1/  2] [data: 6784/14329] Training Loss: 0.0049 Training Acc: 99.4546%\n",
      "[Epoch:  1/  2] [data: 6912/14329] Training Loss: 0.0028 Training Acc: 99.4647%\n",
      "[Epoch:  1/  2] [data: 7040/14329] Training Loss: 0.0030 Training Acc: 99.4744%\n",
      "[Epoch:  1/  2] [data: 7168/14329] Training Loss: 0.0001 Training Acc: 99.4838%\n",
      "[Epoch:  1/  2] [data: 7296/14329] Training Loss: 0.0033 Training Acc: 99.4929%\n",
      "[Epoch:  1/  2] [data: 7424/14329] Training Loss: 0.0047 Training Acc: 99.5016%\n",
      "[Epoch:  1/  2] [data: 7552/14329] Training Loss: 0.0019 Training Acc: 99.5101%\n",
      "[Epoch:  1/  2] [data: 7680/14329] Training Loss: 0.0013 Training Acc: 99.5182%\n",
      "[Epoch:  1/  2] [data: 7808/14329] Training Loss: 0.0044 Training Acc: 99.5133%\n",
      "[Epoch:  1/  2] [data: 7936/14329] Training Loss: 0.0061 Training Acc: 99.5086%\n",
      "[Epoch:  1/  2] [data: 8064/14329] Training Loss: 0.0012 Training Acc: 99.5164%\n",
      "[Epoch:  1/  2] [data: 8192/14329] Training Loss: 0.0018 Training Acc: 99.5239%\n",
      "[Epoch:  1/  2] [data: 8320/14329] Training Loss: 0.0011 Training Acc: 99.5312%\n",
      "[Epoch:  1/  2] [data: 8448/14329] Training Loss: 0.0030 Training Acc: 99.5384%\n",
      "[Epoch:  1/  2] [data: 8576/14329] Training Loss: 0.0041 Training Acc: 99.5336%\n",
      "[Epoch:  1/  2] [data: 8704/14329] Training Loss: 0.0015 Training Acc: 99.5404%\n",
      "[Epoch:  1/  2] [data: 8832/14329] Training Loss: 0.0051 Training Acc: 99.5358%\n",
      "[Epoch:  1/  2] [data: 8960/14329] Training Loss: 0.0012 Training Acc: 99.5424%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 9088/14329] Training Loss: 0.0025 Training Acc: 99.5489%\n",
      "[Epoch:  1/  2] [data: 9216/14329] Training Loss: 0.0007 Training Acc: 99.5551%\n",
      "[Epoch:  1/  2] [data: 9344/14329] Training Loss: 0.0006 Training Acc: 99.5612%\n",
      "[Epoch:  1/  2] [data: 9472/14329] Training Loss: 0.0075 Training Acc: 99.5566%\n",
      "[Epoch:  1/  2] [data: 9600/14329] Training Loss: 0.0006 Training Acc: 99.5625%\n",
      "[Epoch:  1/  2] [data: 9728/14329] Training Loss: 0.0031 Training Acc: 99.5683%\n",
      "[Epoch:  1/  2] [data: 9856/14329] Training Loss: 0.0014 Training Acc: 99.5739%\n",
      "[Epoch:  1/  2] [data: 9984/14329] Training Loss: 0.0048 Training Acc: 99.5693%\n",
      "[Epoch:  1/  2] [data: 10112/14329] Training Loss: 0.0016 Training Acc: 99.5748%\n",
      "[Epoch:  1/  2] [data: 10240/14329] Training Loss: 0.0104 Training Acc: 99.5801%\n",
      "[Epoch:  1/  2] [data: 10368/14329] Training Loss: 0.0049 Training Acc: 99.5756%\n",
      "[Epoch:  1/  2] [data: 10496/14329] Training Loss: 0.0016 Training Acc: 99.5808%\n",
      "[Epoch:  1/  2] [data: 10624/14329] Training Loss: 0.0077 Training Acc: 99.5764%\n",
      "[Epoch:  1/  2] [data: 10752/14329] Training Loss: 0.0029 Training Acc: 99.5815%\n",
      "[Epoch:  1/  2] [data: 10880/14329] Training Loss: 0.0029 Training Acc: 99.5864%\n",
      "[Epoch:  1/  2] [data: 11008/14329] Training Loss: 0.0030 Training Acc: 99.5912%\n",
      "[Epoch:  1/  2] [data: 11136/14329] Training Loss: 0.0010 Training Acc: 99.5959%\n",
      "[Epoch:  1/  2] [data: 11264/14329] Training Loss: 0.0008 Training Acc: 99.6005%\n",
      "[Epoch:  1/  2] [data: 11392/14329] Training Loss: 0.0001 Training Acc: 99.6050%\n",
      "[Epoch:  1/  2] [data: 11520/14329] Training Loss: 0.0003 Training Acc: 99.6094%\n",
      "[Epoch:  1/  2] [data: 11648/14329] Training Loss: 0.0000 Training Acc: 99.6137%\n",
      "[Epoch:  1/  2] [data: 11776/14329] Training Loss: 0.0009 Training Acc: 99.6179%\n",
      "[Epoch:  1/  2] [data: 11904/14329] Training Loss: 0.0001 Training Acc: 99.6220%\n",
      "[Epoch:  1/  2] [data: 12032/14329] Training Loss: 0.0008 Training Acc: 99.6260%\n",
      "[Epoch:  1/  2] [data: 12160/14329] Training Loss: 0.0039 Training Acc: 99.6217%\n",
      "[Epoch:  1/  2] [data: 12288/14329] Training Loss: 0.0029 Training Acc: 99.6257%\n",
      "[Epoch:  1/  2] [data: 12416/14329] Training Loss: 0.0022 Training Acc: 99.6295%\n",
      "[Epoch:  1/  2] [data: 12544/14329] Training Loss: 0.0015 Training Acc: 99.6333%\n",
      "[Epoch:  1/  2] [data: 12672/14329] Training Loss: 0.0006 Training Acc: 99.6370%\n",
      "[Epoch:  1/  2] [data: 12800/14329] Training Loss: 0.0030 Training Acc: 99.6328%\n",
      "[Epoch:  1/  2] [data: 12928/14329] Training Loss: 0.0034 Training Acc: 99.6364%\n",
      "[Epoch:  1/  2] [data: 13056/14329] Training Loss: 0.0072 Training Acc: 99.6324%\n",
      "[Epoch:  1/  2] [data: 13184/14329] Training Loss: 0.0065 Training Acc: 99.6283%\n",
      "[Epoch:  1/  2] [data: 13312/14329] Training Loss: 0.0041 Training Acc: 99.6244%\n",
      "[Epoch:  1/  2] [data: 13440/14329] Training Loss: 0.0008 Training Acc: 99.6280%\n",
      "[Epoch:  1/  2] [data: 13568/14329] Training Loss: 0.0016 Training Acc: 99.6315%\n",
      "[Epoch:  1/  2] [data: 13696/14329] Training Loss: 0.0005 Training Acc: 99.6349%\n",
      "[Epoch:  1/  2] [data: 13824/14329] Training Loss: 0.0049 Training Acc: 99.6383%\n",
      "[Epoch:  1/  2] [data: 13952/14329] Training Loss: 0.0005 Training Acc: 99.6416%\n",
      "[Epoch:  1/  2] [data: 14080/14329] Training Loss: 0.0012 Training Acc: 99.6449%\n",
      "[Epoch:  1/  2] [data: 14208/14329] Training Loss: 0.0001 Training Acc: 99.6481%\n",
      "[Epoch:  1/  2] [data: 14329/14329] Training Loss: 0.0003 Training Acc: 99.6511%\n",
      "Testing-2...\n",
      "[Epoch:  1/  2] Validation Loss: 0.0021 Validation Acc: 99.8052%\n",
      "Time used: 835.2273042201996s\n",
      "[Epoch:  2/  2] [data: 128/14329] Training Loss: 0.0017 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 256/14329] Training Loss: 0.0012 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 384/14329] Training Loss: 0.0005 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 512/14329] Training Loss: 0.0003 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 640/14329] Training Loss: 0.0010 Training Acc: 100.0000%\n",
      "[Epoch:  2/  2] [data: 768/14329] Training Loss: 0.0021 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 896/14329] Training Loss: 0.0000 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 1024/14329] Training Loss: 0.0001 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 1152/14329] Training Loss: 0.0026 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 1280/14329] Training Loss: 0.0025 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 1408/14329] Training Loss: 0.0008 Training Acc: 99.8580%\n",
      "[Epoch:  2/  2] [data: 1536/14329] Training Loss: 0.0013 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 1664/14329] Training Loss: 0.0002 Training Acc: 99.8798%\n",
      "[Epoch:  2/  2] [data: 1792/14329] Training Loss: 0.0026 Training Acc: 99.8326%\n",
      "[Epoch:  2/  2] [data: 1920/14329] Training Loss: 0.0012 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 2048/14329] Training Loss: 0.0021 Training Acc: 99.8535%\n",
      "[Epoch:  2/  2] [data: 2176/14329] Training Loss: 0.0008 Training Acc: 99.8621%\n",
      "[Epoch:  2/  2] [data: 2304/14329] Training Loss: 0.0019 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 2432/14329] Training Loss: 0.0027 Training Acc: 99.8355%\n",
      "[Epoch:  2/  2] [data: 2560/14329] Training Loss: 0.0011 Training Acc: 99.8438%\n",
      "[Epoch:  2/  2] [data: 2688/14329] Training Loss: 0.0002 Training Acc: 99.8512%\n",
      "[Epoch:  2/  2] [data: 2816/14329] Training Loss: 0.0003 Training Acc: 99.8580%\n",
      "[Epoch:  2/  2] [data: 2944/14329] Training Loss: 0.0013 Training Acc: 99.8641%\n",
      "[Epoch:  2/  2] [data: 3072/14329] Training Loss: 0.0003 Training Acc: 99.8698%\n",
      "[Epoch:  2/  2] [data: 3200/14329] Training Loss: 0.0014 Training Acc: 99.8750%\n",
      "[Epoch:  2/  2] [data: 3328/14329] Training Loss: 0.0012 Training Acc: 99.8798%\n",
      "[Epoch:  2/  2] [data: 3456/14329] Training Loss: 0.0016 Training Acc: 99.8843%\n",
      "[Epoch:  2/  2] [data: 3584/14329] Training Loss: 0.0004 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 3712/14329] Training Loss: 0.0001 Training Acc: 99.8922%\n",
      "[Epoch:  2/  2] [data: 3840/14329] Training Loss: 0.0002 Training Acc: 99.8958%\n",
      "[Epoch:  2/  2] [data: 3968/14329] Training Loss: 0.0002 Training Acc: 99.8992%\n",
      "[Epoch:  2/  2] [data: 4096/14329] Training Loss: 0.0057 Training Acc: 99.8779%\n",
      "[Epoch:  2/  2] [data: 4224/14329] Training Loss: 0.0003 Training Acc: 99.8816%\n",
      "[Epoch:  2/  2] [data: 4352/14329] Training Loss: 0.0005 Training Acc: 99.8851%\n",
      "[Epoch:  2/  2] [data: 4480/14329] Training Loss: 0.0030 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 4608/14329] Training Loss: 0.0007 Training Acc: 99.8915%\n",
      "[Epoch:  2/  2] [data: 4736/14329] Training Loss: 0.0002 Training Acc: 99.8944%\n",
      "[Epoch:  2/  2] [data: 4864/14329] Training Loss: 0.0004 Training Acc: 99.8972%\n",
      "[Epoch:  2/  2] [data: 4992/14329] Training Loss: 0.0002 Training Acc: 99.8998%\n",
      "[Epoch:  2/  2] [data: 5120/14329] Training Loss: 0.0031 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 5248/14329] Training Loss: 0.0041 Training Acc: 99.9047%\n",
      "[Epoch:  2/  2] [data: 5376/14329] Training Loss: 0.0006 Training Acc: 99.9070%\n",
      "[Epoch:  2/  2] [data: 5504/14329] Training Loss: 0.0033 Training Acc: 99.9092%\n",
      "[Epoch:  2/  2] [data: 5632/14329] Training Loss: 0.0001 Training Acc: 99.9112%\n",
      "[Epoch:  2/  2] [data: 5760/14329] Training Loss: 0.0003 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 5888/14329] Training Loss: 0.0045 Training Acc: 99.8981%\n",
      "[Epoch:  2/  2] [data: 6016/14329] Training Loss: 0.0036 Training Acc: 99.8836%\n",
      "[Epoch:  2/  2] [data: 6144/14329] Training Loss: 0.0012 Training Acc: 99.8861%\n",
      "[Epoch:  2/  2] [data: 6272/14329] Training Loss: 0.0009 Training Acc: 99.8884%\n",
      "[Epoch:  2/  2] [data: 6400/14329] Training Loss: 0.0002 Training Acc: 99.8906%\n",
      "[Epoch:  2/  2] [data: 6528/14329] Training Loss: 0.0013 Training Acc: 99.8928%\n",
      "[Epoch:  2/  2] [data: 6656/14329] Training Loss: 0.0019 Training Acc: 99.8948%\n",
      "[Epoch:  2/  2] [data: 6784/14329] Training Loss: 0.0017 Training Acc: 99.8968%\n",
      "[Epoch:  2/  2] [data: 6912/14329] Training Loss: 0.0007 Training Acc: 99.8987%\n",
      "[Epoch:  2/  2] [data: 7040/14329] Training Loss: 0.0010 Training Acc: 99.9006%\n",
      "[Epoch:  2/  2] [data: 7168/14329] Training Loss: 0.0000 Training Acc: 99.9023%\n",
      "[Epoch:  2/  2] [data: 7296/14329] Training Loss: 0.0004 Training Acc: 99.9041%\n",
      "[Epoch:  2/  2] [data: 7424/14329] Training Loss: 0.0020 Training Acc: 99.9057%\n",
      "[Epoch:  2/  2] [data: 7552/14329] Training Loss: 0.0011 Training Acc: 99.9073%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 7680/14329] Training Loss: 0.0001 Training Acc: 99.9089%\n",
      "[Epoch:  2/  2] [data: 7808/14329] Training Loss: 0.0019 Training Acc: 99.9103%\n",
      "[Epoch:  2/  2] [data: 7936/14329] Training Loss: 0.0018 Training Acc: 99.9118%\n",
      "[Epoch:  2/  2] [data: 8064/14329] Training Loss: 0.0007 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 8192/14329] Training Loss: 0.0007 Training Acc: 99.9146%\n",
      "[Epoch:  2/  2] [data: 8320/14329] Training Loss: 0.0002 Training Acc: 99.9159%\n",
      "[Epoch:  2/  2] [data: 8448/14329] Training Loss: 0.0014 Training Acc: 99.9171%\n",
      "[Epoch:  2/  2] [data: 8576/14329] Training Loss: 0.0020 Training Acc: 99.9184%\n",
      "[Epoch:  2/  2] [data: 8704/14329] Training Loss: 0.0003 Training Acc: 99.9196%\n",
      "[Epoch:  2/  2] [data: 8832/14329] Training Loss: 0.0026 Training Acc: 99.9094%\n",
      "[Epoch:  2/  2] [data: 8960/14329] Training Loss: 0.0004 Training Acc: 99.9107%\n",
      "[Epoch:  2/  2] [data: 9088/14329] Training Loss: 0.0009 Training Acc: 99.9120%\n",
      "[Epoch:  2/  2] [data: 9216/14329] Training Loss: 0.0003 Training Acc: 99.9132%\n",
      "[Epoch:  2/  2] [data: 9344/14329] Training Loss: 0.0003 Training Acc: 99.9144%\n",
      "[Epoch:  2/  2] [data: 9472/14329] Training Loss: 0.0031 Training Acc: 99.9155%\n",
      "[Epoch:  2/  2] [data: 9600/14329] Training Loss: 0.0003 Training Acc: 99.9167%\n",
      "[Epoch:  2/  2] [data: 9728/14329] Training Loss: 0.0012 Training Acc: 99.9178%\n",
      "[Epoch:  2/  2] [data: 9856/14329] Training Loss: 0.0004 Training Acc: 99.9188%\n",
      "[Epoch:  2/  2] [data: 9984/14329] Training Loss: 0.0024 Training Acc: 99.9199%\n",
      "[Epoch:  2/  2] [data: 10112/14329] Training Loss: 0.0008 Training Acc: 99.9209%\n",
      "[Epoch:  2/  2] [data: 10240/14329] Training Loss: 0.0080 Training Acc: 99.9219%\n",
      "[Epoch:  2/  2] [data: 10368/14329] Training Loss: 0.0018 Training Acc: 99.9228%\n",
      "[Epoch:  2/  2] [data: 10496/14329] Training Loss: 0.0006 Training Acc: 99.9238%\n",
      "[Epoch:  2/  2] [data: 10624/14329] Training Loss: 0.0036 Training Acc: 99.9247%\n",
      "[Epoch:  2/  2] [data: 10752/14329] Training Loss: 0.0007 Training Acc: 99.9256%\n",
      "[Epoch:  2/  2] [data: 10880/14329] Training Loss: 0.0011 Training Acc: 99.9265%\n",
      "[Epoch:  2/  2] [data: 11008/14329] Training Loss: 0.0013 Training Acc: 99.9273%\n",
      "[Epoch:  2/  2] [data: 11136/14329] Training Loss: 0.0005 Training Acc: 99.9282%\n",
      "[Epoch:  2/  2] [data: 11264/14329] Training Loss: 0.0004 Training Acc: 99.9290%\n",
      "[Epoch:  2/  2] [data: 11392/14329] Training Loss: 0.0000 Training Acc: 99.9298%\n",
      "[Epoch:  2/  2] [data: 11520/14329] Training Loss: 0.0001 Training Acc: 99.9306%\n",
      "[Epoch:  2/  2] [data: 11648/14329] Training Loss: 0.0000 Training Acc: 99.9313%\n",
      "[Epoch:  2/  2] [data: 11776/14329] Training Loss: 0.0004 Training Acc: 99.9321%\n",
      "[Epoch:  2/  2] [data: 11904/14329] Training Loss: 0.0000 Training Acc: 99.9328%\n",
      "[Epoch:  2/  2] [data: 12032/14329] Training Loss: 0.0004 Training Acc: 99.9335%\n",
      "[Epoch:  2/  2] [data: 12160/14329] Training Loss: 0.0017 Training Acc: 99.9342%\n",
      "[Epoch:  2/  2] [data: 12288/14329] Training Loss: 0.0008 Training Acc: 99.9349%\n",
      "[Epoch:  2/  2] [data: 12416/14329] Training Loss: 0.0009 Training Acc: 99.9356%\n",
      "[Epoch:  2/  2] [data: 12544/14329] Training Loss: 0.0008 Training Acc: 99.9362%\n",
      "[Epoch:  2/  2] [data: 12672/14329] Training Loss: 0.0003 Training Acc: 99.9369%\n",
      "[Epoch:  2/  2] [data: 12800/14329] Training Loss: 0.0018 Training Acc: 99.9375%\n",
      "[Epoch:  2/  2] [data: 12928/14329] Training Loss: 0.0009 Training Acc: 99.9381%\n",
      "[Epoch:  2/  2] [data: 13056/14329] Training Loss: 0.0036 Training Acc: 99.9387%\n",
      "[Epoch:  2/  2] [data: 13184/14329] Training Loss: 0.0036 Training Acc: 99.9317%\n",
      "[Epoch:  2/  2] [data: 13312/14329] Training Loss: 0.0022 Training Acc: 99.9324%\n",
      "[Epoch:  2/  2] [data: 13440/14329] Training Loss: 0.0003 Training Acc: 99.9330%\n",
      "[Epoch:  2/  2] [data: 13568/14329] Training Loss: 0.0008 Training Acc: 99.9337%\n",
      "[Epoch:  2/  2] [data: 13696/14329] Training Loss: 0.0003 Training Acc: 99.9343%\n",
      "[Epoch:  2/  2] [data: 13824/14329] Training Loss: 0.0036 Training Acc: 99.9349%\n",
      "[Epoch:  2/  2] [data: 13952/14329] Training Loss: 0.0002 Training Acc: 99.9355%\n",
      "[Epoch:  2/  2] [data: 14080/14329] Training Loss: 0.0004 Training Acc: 99.9361%\n",
      "[Epoch:  2/  2] [data: 14208/14329] Training Loss: 0.0000 Training Acc: 99.9367%\n",
      "[Epoch:  2/  2] [data: 14329/14329] Training Loss: 0.0000 Training Acc: 99.9372%\n",
      "Testing-2...\n",
      "[Epoch:  2/  2] Validation Loss: 0.0014 Validation Acc: 99.9784%\n",
      "Time used: 841.021595954895s\n",
      "Type-2: 1.0 0.9971671388101983 0.9985815602836879\n",
      "[Epoch:  1/  2] [data: 128/23054] Training Loss: 1.0165 Training Acc: 64.0625%\n",
      "[Epoch:  1/  2] [data: 256/23054] Training Loss: 1.4451 Training Acc: 62.1094%\n",
      "[Epoch:  1/  2] [data: 384/23054] Training Loss: 0.5614 Training Acc: 63.2812%\n",
      "[Epoch:  1/  2] [data: 512/23054] Training Loss: 0.3955 Training Acc: 65.2344%\n",
      "[Epoch:  1/  2] [data: 640/23054] Training Loss: 0.3626 Training Acc: 66.0938%\n",
      "[Epoch:  1/  2] [data: 768/23054] Training Loss: 0.4878 Training Acc: 66.1458%\n",
      "[Epoch:  1/  2] [data: 896/23054] Training Loss: 0.3838 Training Acc: 66.8527%\n",
      "[Epoch:  1/  2] [data: 1024/23054] Training Loss: 0.3044 Training Acc: 67.4805%\n",
      "[Epoch:  1/  2] [data: 1152/23054] Training Loss: 0.2568 Training Acc: 68.4028%\n",
      "[Epoch:  1/  2] [data: 1280/23054] Training Loss: 0.4614 Training Acc: 68.8281%\n",
      "[Epoch:  1/  2] [data: 1408/23054] Training Loss: 0.4440 Training Acc: 69.5312%\n",
      "[Epoch:  1/  2] [data: 1536/23054] Training Loss: 0.3214 Training Acc: 69.9219%\n",
      "[Epoch:  1/  2] [data: 1664/23054] Training Loss: 0.4465 Training Acc: 70.4928%\n",
      "[Epoch:  1/  2] [data: 1792/23054] Training Loss: 0.3554 Training Acc: 71.1496%\n",
      "[Epoch:  1/  2] [data: 1920/23054] Training Loss: 0.3414 Training Acc: 72.0833%\n",
      "[Epoch:  1/  2] [data: 2048/23054] Training Loss: 0.2620 Training Acc: 73.1445%\n",
      "[Epoch:  1/  2] [data: 2176/23054] Training Loss: 0.4254 Training Acc: 73.7592%\n",
      "[Epoch:  1/  2] [data: 2304/23054] Training Loss: 0.2502 Training Acc: 74.3056%\n",
      "[Epoch:  1/  2] [data: 2432/23054] Training Loss: 0.2038 Training Acc: 75.0822%\n",
      "[Epoch:  1/  2] [data: 2560/23054] Training Loss: 0.2510 Training Acc: 75.5469%\n",
      "[Epoch:  1/  2] [data: 2688/23054] Training Loss: 0.2903 Training Acc: 75.8557%\n",
      "[Epoch:  1/  2] [data: 2816/23054] Training Loss: 0.1820 Training Acc: 76.3494%\n",
      "[Epoch:  1/  2] [data: 2944/23054] Training Loss: 0.3004 Training Acc: 76.6304%\n",
      "[Epoch:  1/  2] [data: 3072/23054] Training Loss: 0.1494 Training Acc: 77.2135%\n",
      "[Epoch:  1/  2] [data: 3200/23054] Training Loss: 0.1865 Training Acc: 77.7812%\n",
      "[Epoch:  1/  2] [data: 3328/23054] Training Loss: 0.2344 Training Acc: 78.1550%\n",
      "[Epoch:  1/  2] [data: 3456/23054] Training Loss: 0.2261 Training Acc: 78.5590%\n",
      "[Epoch:  1/  2] [data: 3584/23054] Training Loss: 0.1781 Training Acc: 78.8504%\n",
      "[Epoch:  1/  2] [data: 3712/23054] Training Loss: 0.2042 Training Acc: 79.0140%\n",
      "[Epoch:  1/  2] [data: 3840/23054] Training Loss: 0.2530 Training Acc: 79.2708%\n",
      "[Epoch:  1/  2] [data: 3968/23054] Training Loss: 0.2150 Training Acc: 79.5363%\n",
      "[Epoch:  1/  2] [data: 4096/23054] Training Loss: 0.0988 Training Acc: 80.0049%\n",
      "[Epoch:  1/  2] [data: 4224/23054] Training Loss: 0.2632 Training Acc: 80.3504%\n",
      "[Epoch:  1/  2] [data: 4352/23054] Training Loss: 0.2006 Training Acc: 80.6756%\n",
      "[Epoch:  1/  2] [data: 4480/23054] Training Loss: 0.1335 Training Acc: 80.9821%\n",
      "[Epoch:  1/  2] [data: 4608/23054] Training Loss: 0.1663 Training Acc: 81.3151%\n",
      "[Epoch:  1/  2] [data: 4736/23054] Training Loss: 0.1677 Training Acc: 81.6090%\n",
      "[Epoch:  1/  2] [data: 4864/23054] Training Loss: 0.2985 Training Acc: 81.8257%\n",
      "[Epoch:  1/  2] [data: 4992/23054] Training Loss: 0.1037 Training Acc: 82.2115%\n",
      "[Epoch:  1/  2] [data: 5120/23054] Training Loss: 0.1200 Training Acc: 82.4805%\n",
      "[Epoch:  1/  2] [data: 5248/23054] Training Loss: 0.3476 Training Acc: 82.5838%\n",
      "[Epoch:  1/  2] [data: 5376/23054] Training Loss: 0.1485 Training Acc: 82.8683%\n",
      "[Epoch:  1/  2] [data: 5504/23054] Training Loss: 0.1347 Training Acc: 83.0850%\n",
      "[Epoch:  1/  2] [data: 5632/23054] Training Loss: 0.2637 Training Acc: 83.3097%\n",
      "[Epoch:  1/  2] [data: 5760/23054] Training Loss: 0.2655 Training Acc: 83.4896%\n",
      "[Epoch:  1/  2] [data: 5888/23054] Training Loss: 0.1296 Training Acc: 83.7466%\n",
      "[Epoch:  1/  2] [data: 6016/23054] Training Loss: 0.1190 Training Acc: 83.9927%\n",
      "[Epoch:  1/  2] [data: 6144/23054] Training Loss: 0.1326 Training Acc: 84.2285%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 6272/23054] Training Loss: 0.1925 Training Acc: 84.3591%\n",
      "[Epoch:  1/  2] [data: 6400/23054] Training Loss: 0.1908 Training Acc: 84.5781%\n",
      "[Epoch:  1/  2] [data: 6528/23054] Training Loss: 0.2214 Training Acc: 84.7426%\n",
      "[Epoch:  1/  2] [data: 6656/23054] Training Loss: 0.2975 Training Acc: 84.8708%\n",
      "[Epoch:  1/  2] [data: 6784/23054] Training Loss: 0.1214 Training Acc: 85.0678%\n",
      "[Epoch:  1/  2] [data: 6912/23054] Training Loss: 0.1964 Training Acc: 85.2141%\n",
      "[Epoch:  1/  2] [data: 7040/23054] Training Loss: 0.1474 Training Acc: 85.4261%\n",
      "[Epoch:  1/  2] [data: 7168/23054] Training Loss: 0.1078 Training Acc: 85.6445%\n",
      "[Epoch:  1/  2] [data: 7296/23054] Training Loss: 0.2490 Training Acc: 85.8004%\n",
      "[Epoch:  1/  2] [data: 7424/23054] Training Loss: 0.1400 Training Acc: 85.9914%\n",
      "[Epoch:  1/  2] [data: 7552/23054] Training Loss: 0.1672 Training Acc: 86.1494%\n",
      "[Epoch:  1/  2] [data: 7680/23054] Training Loss: 0.1472 Training Acc: 86.3151%\n",
      "[Epoch:  1/  2] [data: 7808/23054] Training Loss: 0.1477 Training Acc: 86.4370%\n",
      "[Epoch:  1/  2] [data: 7936/23054] Training Loss: 0.1685 Training Acc: 86.5675%\n",
      "[Epoch:  1/  2] [data: 8064/23054] Training Loss: 0.2518 Training Acc: 86.6567%\n",
      "[Epoch:  1/  2] [data: 8192/23054] Training Loss: 0.1747 Training Acc: 86.7798%\n",
      "[Epoch:  1/  2] [data: 8320/23054] Training Loss: 0.1292 Training Acc: 86.9231%\n",
      "[Epoch:  1/  2] [data: 8448/23054] Training Loss: 0.2554 Training Acc: 86.9792%\n",
      "[Epoch:  1/  2] [data: 8576/23054] Training Loss: 0.2554 Training Acc: 87.0569%\n",
      "[Epoch:  1/  2] [data: 8704/23054] Training Loss: 0.1921 Training Acc: 87.1898%\n",
      "[Epoch:  1/  2] [data: 8832/23054] Training Loss: 0.1617 Training Acc: 87.3188%\n",
      "[Epoch:  1/  2] [data: 8960/23054] Training Loss: 0.1536 Training Acc: 87.3884%\n",
      "[Epoch:  1/  2] [data: 9088/23054] Training Loss: 0.1431 Training Acc: 87.4890%\n",
      "[Epoch:  1/  2] [data: 9216/23054] Training Loss: 0.0753 Training Acc: 87.6411%\n",
      "[Epoch:  1/  2] [data: 9344/23054] Training Loss: 0.2094 Training Acc: 87.7247%\n",
      "[Epoch:  1/  2] [data: 9472/23054] Training Loss: 0.1525 Training Acc: 87.7956%\n",
      "[Epoch:  1/  2] [data: 9600/23054] Training Loss: 0.2112 Training Acc: 87.8646%\n",
      "[Epoch:  1/  2] [data: 9728/23054] Training Loss: 0.1366 Training Acc: 87.9626%\n",
      "[Epoch:  1/  2] [data: 9856/23054] Training Loss: 0.1430 Training Acc: 88.0682%\n",
      "[Epoch:  1/  2] [data: 9984/23054] Training Loss: 0.1374 Training Acc: 88.1611%\n",
      "[Epoch:  1/  2] [data: 10112/23054] Training Loss: 0.1694 Training Acc: 88.2417%\n",
      "[Epoch:  1/  2] [data: 10240/23054] Training Loss: 0.0924 Training Acc: 88.3594%\n",
      "[Epoch:  1/  2] [data: 10368/23054] Training Loss: 0.1374 Training Acc: 88.4356%\n",
      "[Epoch:  1/  2] [data: 10496/23054] Training Loss: 0.1570 Training Acc: 88.4813%\n",
      "[Epoch:  1/  2] [data: 10624/23054] Training Loss: 0.2133 Training Acc: 88.5354%\n",
      "[Epoch:  1/  2] [data: 10752/23054] Training Loss: 0.1275 Training Acc: 88.5975%\n",
      "[Epoch:  1/  2] [data: 10880/23054] Training Loss: 0.1057 Training Acc: 88.6765%\n",
      "[Epoch:  1/  2] [data: 11008/23054] Training Loss: 0.2206 Training Acc: 88.7445%\n",
      "[Epoch:  1/  2] [data: 11136/23054] Training Loss: 0.1645 Training Acc: 88.8290%\n",
      "[Epoch:  1/  2] [data: 11264/23054] Training Loss: 0.1738 Training Acc: 88.9205%\n",
      "[Epoch:  1/  2] [data: 11392/23054] Training Loss: 0.2292 Training Acc: 88.9484%\n",
      "[Epoch:  1/  2] [data: 11520/23054] Training Loss: 0.0954 Training Acc: 89.0451%\n",
      "[Epoch:  1/  2] [data: 11648/23054] Training Loss: 0.1438 Training Acc: 89.0968%\n",
      "[Epoch:  1/  2] [data: 11776/23054] Training Loss: 0.1834 Training Acc: 89.1814%\n",
      "[Epoch:  1/  2] [data: 11904/23054] Training Loss: 0.2747 Training Acc: 89.2221%\n",
      "[Epoch:  1/  2] [data: 12032/23054] Training Loss: 0.1864 Training Acc: 89.2703%\n",
      "[Epoch:  1/  2] [data: 12160/23054] Training Loss: 0.1741 Training Acc: 89.3010%\n",
      "[Epoch:  1/  2] [data: 12288/23054] Training Loss: 0.1670 Training Acc: 89.3473%\n",
      "[Epoch:  1/  2] [data: 12416/23054] Training Loss: 0.1556 Training Acc: 89.4249%\n",
      "[Epoch:  1/  2] [data: 12544/23054] Training Loss: 0.1642 Training Acc: 89.4850%\n",
      "[Epoch:  1/  2] [data: 12672/23054] Training Loss: 0.2284 Training Acc: 89.5281%\n",
      "[Epoch:  1/  2] [data: 12800/23054] Training Loss: 0.1624 Training Acc: 89.5859%\n",
      "[Epoch:  1/  2] [data: 12928/23054] Training Loss: 0.2079 Training Acc: 89.6194%\n",
      "[Epoch:  1/  2] [data: 13056/23054] Training Loss: 0.1535 Training Acc: 89.6599%\n",
      "[Epoch:  1/  2] [data: 13184/23054] Training Loss: 0.1789 Training Acc: 89.6996%\n",
      "[Epoch:  1/  2] [data: 13312/23054] Training Loss: 0.0897 Training Acc: 89.7686%\n",
      "[Epoch:  1/  2] [data: 13440/23054] Training Loss: 0.1242 Training Acc: 89.8289%\n",
      "[Epoch:  1/  2] [data: 13568/23054] Training Loss: 0.1795 Training Acc: 89.8585%\n",
      "[Epoch:  1/  2] [data: 13696/23054] Training Loss: 0.1976 Training Acc: 89.8949%\n",
      "[Epoch:  1/  2] [data: 13824/23054] Training Loss: 0.0863 Training Acc: 89.9740%\n",
      "[Epoch:  1/  2] [data: 13952/23054] Training Loss: 0.2018 Training Acc: 89.9799%\n",
      "[Epoch:  1/  2] [data: 14080/23054] Training Loss: 0.0815 Training Acc: 90.0497%\n",
      "[Epoch:  1/  2] [data: 14208/23054] Training Loss: 0.0538 Training Acc: 90.1253%\n",
      "[Epoch:  1/  2] [data: 14336/23054] Training Loss: 0.1297 Training Acc: 90.1925%\n",
      "[Epoch:  1/  2] [data: 14464/23054] Training Loss: 0.3713 Training Acc: 90.1825%\n",
      "[Epoch:  1/  2] [data: 14592/23054] Training Loss: 0.1255 Training Acc: 90.2275%\n",
      "[Epoch:  1/  2] [data: 14720/23054] Training Loss: 0.2239 Training Acc: 90.2785%\n",
      "[Epoch:  1/  2] [data: 14848/23054] Training Loss: 0.1597 Training Acc: 90.3219%\n",
      "[Epoch:  1/  2] [data: 14976/23054] Training Loss: 0.1946 Training Acc: 90.3646%\n",
      "[Epoch:  1/  2] [data: 15104/23054] Training Loss: 0.1195 Training Acc: 90.4131%\n",
      "[Epoch:  1/  2] [data: 15232/23054] Training Loss: 0.1322 Training Acc: 90.4412%\n",
      "[Epoch:  1/  2] [data: 15360/23054] Training Loss: 0.2076 Training Acc: 90.4557%\n",
      "[Epoch:  1/  2] [data: 15488/23054] Training Loss: 0.1381 Training Acc: 90.4959%\n",
      "[Epoch:  1/  2] [data: 15616/23054] Training Loss: 0.2404 Training Acc: 90.5225%\n",
      "[Epoch:  1/  2] [data: 15744/23054] Training Loss: 0.1216 Training Acc: 90.5678%\n",
      "[Epoch:  1/  2] [data: 15872/23054] Training Loss: 0.1721 Training Acc: 90.6187%\n",
      "[Epoch:  1/  2] [data: 16000/23054] Training Loss: 0.2096 Training Acc: 90.6500%\n",
      "[Epoch:  1/  2] [data: 16128/23054] Training Loss: 0.2503 Training Acc: 90.6684%\n",
      "[Epoch:  1/  2] [data: 16256/23054] Training Loss: 0.1285 Training Acc: 90.7050%\n",
      "[Epoch:  1/  2] [data: 16384/23054] Training Loss: 0.1969 Training Acc: 90.7166%\n",
      "[Epoch:  1/  2] [data: 16512/23054] Training Loss: 0.1111 Training Acc: 90.7643%\n",
      "[Epoch:  1/  2] [data: 16640/23054] Training Loss: 0.2310 Training Acc: 90.7812%\n",
      "[Epoch:  1/  2] [data: 16768/23054] Training Loss: 0.2468 Training Acc: 90.8039%\n",
      "[Epoch:  1/  2] [data: 16896/23054] Training Loss: 0.1112 Training Acc: 90.8440%\n",
      "[Epoch:  1/  2] [data: 17024/23054] Training Loss: 0.1285 Training Acc: 90.8717%\n",
      "[Epoch:  1/  2] [data: 17152/23054] Training Loss: 0.0677 Training Acc: 90.9223%\n",
      "[Epoch:  1/  2] [data: 17280/23054] Training Loss: 0.1714 Training Acc: 90.9491%\n",
      "[Epoch:  1/  2] [data: 17408/23054] Training Loss: 0.1140 Training Acc: 90.9754%\n",
      "[Epoch:  1/  2] [data: 17536/23054] Training Loss: 0.1126 Training Acc: 91.0014%\n",
      "[Epoch:  1/  2] [data: 17664/23054] Training Loss: 0.1442 Training Acc: 91.0326%\n",
      "[Epoch:  1/  2] [data: 17792/23054] Training Loss: 0.0860 Training Acc: 91.0690%\n",
      "[Epoch:  1/  2] [data: 17920/23054] Training Loss: 0.1657 Training Acc: 91.0993%\n",
      "[Epoch:  1/  2] [data: 18048/23054] Training Loss: 0.2334 Training Acc: 91.1015%\n",
      "[Epoch:  1/  2] [data: 18176/23054] Training Loss: 0.1640 Training Acc: 91.1422%\n",
      "[Epoch:  1/  2] [data: 18304/23054] Training Loss: 0.2051 Training Acc: 91.1713%\n",
      "[Epoch:  1/  2] [data: 18432/23054] Training Loss: 0.1022 Training Acc: 91.2164%\n",
      "[Epoch:  1/  2] [data: 18560/23054] Training Loss: 0.1115 Training Acc: 91.2446%\n",
      "[Epoch:  1/  2] [data: 18688/23054] Training Loss: 0.1201 Training Acc: 91.2832%\n",
      "[Epoch:  1/  2] [data: 18816/23054] Training Loss: 0.0937 Training Acc: 91.3318%\n",
      "[Epoch:  1/  2] [data: 18944/23054] Training Loss: 0.1606 Training Acc: 91.3587%\n",
      "[Epoch:  1/  2] [data: 19072/23054] Training Loss: 0.1677 Training Acc: 91.4010%\n",
      "[Epoch:  1/  2] [data: 19200/23054] Training Loss: 0.1779 Training Acc: 91.4115%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  2] [data: 19328/23054] Training Loss: 0.1859 Training Acc: 91.4114%\n",
      "[Epoch:  1/  2] [data: 19456/23054] Training Loss: 0.1312 Training Acc: 91.4474%\n",
      "[Epoch:  1/  2] [data: 19584/23054] Training Loss: 0.0989 Training Acc: 91.4828%\n",
      "[Epoch:  1/  2] [data: 19712/23054] Training Loss: 0.1727 Training Acc: 91.5128%\n",
      "[Epoch:  1/  2] [data: 19840/23054] Training Loss: 0.1093 Training Acc: 91.5373%\n",
      "[Epoch:  1/  2] [data: 19968/23054] Training Loss: 0.1740 Training Acc: 91.5715%\n",
      "[Epoch:  1/  2] [data: 20096/23054] Training Loss: 0.1011 Training Acc: 91.6103%\n",
      "[Epoch:  1/  2] [data: 20224/23054] Training Loss: 0.0915 Training Acc: 91.6436%\n",
      "[Epoch:  1/  2] [data: 20352/23054] Training Loss: 0.1668 Training Acc: 91.6618%\n",
      "[Epoch:  1/  2] [data: 20480/23054] Training Loss: 0.0916 Training Acc: 91.6846%\n",
      "[Epoch:  1/  2] [data: 20608/23054] Training Loss: 0.2088 Training Acc: 91.6974%\n",
      "[Epoch:  1/  2] [data: 20736/23054] Training Loss: 0.1076 Training Acc: 91.7149%\n",
      "[Epoch:  1/  2] [data: 20864/23054] Training Loss: 0.0750 Training Acc: 91.7418%\n",
      "[Epoch:  1/  2] [data: 20992/23054] Training Loss: 0.1400 Training Acc: 91.7540%\n",
      "[Epoch:  1/  2] [data: 21120/23054] Training Loss: 0.1580 Training Acc: 91.7756%\n",
      "[Epoch:  1/  2] [data: 21248/23054] Training Loss: 0.1107 Training Acc: 91.7969%\n",
      "[Epoch:  1/  2] [data: 21376/23054] Training Loss: 0.0913 Training Acc: 91.8132%\n",
      "[Epoch:  1/  2] [data: 21504/23054] Training Loss: 0.1775 Training Acc: 91.8248%\n",
      "[Epoch:  1/  2] [data: 21632/23054] Training Loss: 0.0943 Training Acc: 91.8593%\n",
      "[Epoch:  1/  2] [data: 21760/23054] Training Loss: 0.1569 Training Acc: 91.8704%\n",
      "[Epoch:  1/  2] [data: 21888/23054] Training Loss: 0.1671 Training Acc: 91.8951%\n",
      "[Epoch:  1/  2] [data: 22016/23054] Training Loss: 0.2012 Training Acc: 91.9013%\n",
      "[Epoch:  1/  2] [data: 22144/23054] Training Loss: 0.1487 Training Acc: 91.9211%\n",
      "[Epoch:  1/  2] [data: 22272/23054] Training Loss: 0.2009 Training Acc: 91.9406%\n",
      "[Epoch:  1/  2] [data: 22400/23054] Training Loss: 0.2159 Training Acc: 91.9509%\n",
      "[Epoch:  1/  2] [data: 22528/23054] Training Loss: 0.1806 Training Acc: 91.9744%\n",
      "[Epoch:  1/  2] [data: 22656/23054] Training Loss: 0.0786 Training Acc: 92.0109%\n",
      "[Epoch:  1/  2] [data: 22784/23054] Training Loss: 0.1069 Training Acc: 92.0207%\n",
      "[Epoch:  1/  2] [data: 22912/23054] Training Loss: 0.2399 Training Acc: 92.0304%\n",
      "[Epoch:  1/  2] [data: 23040/23054] Training Loss: 0.2503 Training Acc: 92.0226%\n",
      "[Epoch:  1/  2] [data: 23054/23054] Training Loss: 0.2963 Training Acc: 92.0231%\n",
      "Testing-3...\n",
      "[Epoch:  1/  2] Validation Loss: 0.1355 Validation Acc: 95.9232%\n",
      "Time used: 1154.5457317829132s\n",
      "[Epoch:  2/  2] [data: 128/23054] Training Loss: 0.1400 Training Acc: 94.5312%\n",
      "[Epoch:  2/  2] [data: 256/23054] Training Loss: 0.1760 Training Acc: 92.9688%\n",
      "[Epoch:  2/  2] [data: 384/23054] Training Loss: 0.1254 Training Acc: 94.2708%\n",
      "[Epoch:  2/  2] [data: 512/23054] Training Loss: 0.0641 Training Acc: 95.3125%\n",
      "[Epoch:  2/  2] [data: 640/23054] Training Loss: 0.1166 Training Acc: 95.7812%\n",
      "[Epoch:  2/  2] [data: 768/23054] Training Loss: 0.1379 Training Acc: 95.7031%\n",
      "[Epoch:  2/  2] [data: 896/23054] Training Loss: 0.0847 Training Acc: 95.9821%\n",
      "[Epoch:  2/  2] [data: 1024/23054] Training Loss: 0.0465 Training Acc: 96.2891%\n",
      "[Epoch:  2/  2] [data: 1152/23054] Training Loss: 0.1080 Training Acc: 96.4410%\n",
      "[Epoch:  2/  2] [data: 1280/23054] Training Loss: 0.2077 Training Acc: 96.1719%\n",
      "[Epoch:  2/  2] [data: 1408/23054] Training Loss: 0.1307 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 1536/23054] Training Loss: 0.1269 Training Acc: 96.0286%\n",
      "[Epoch:  2/  2] [data: 1664/23054] Training Loss: 0.2581 Training Acc: 95.9135%\n",
      "[Epoch:  2/  2] [data: 1792/23054] Training Loss: 0.1416 Training Acc: 95.8705%\n",
      "[Epoch:  2/  2] [data: 1920/23054] Training Loss: 0.1679 Training Acc: 95.6771%\n",
      "[Epoch:  2/  2] [data: 2048/23054] Training Loss: 0.1071 Training Acc: 95.8008%\n",
      "[Epoch:  2/  2] [data: 2176/23054] Training Loss: 0.1827 Training Acc: 95.7721%\n",
      "[Epoch:  2/  2] [data: 2304/23054] Training Loss: 0.0929 Training Acc: 95.9201%\n",
      "[Epoch:  2/  2] [data: 2432/23054] Training Loss: 0.1003 Training Acc: 95.9704%\n",
      "[Epoch:  2/  2] [data: 2560/23054] Training Loss: 0.1556 Training Acc: 95.8984%\n",
      "[Epoch:  2/  2] [data: 2688/23054] Training Loss: 0.1535 Training Acc: 95.9077%\n",
      "[Epoch:  2/  2] [data: 2816/23054] Training Loss: 0.0953 Training Acc: 95.9517%\n",
      "[Epoch:  2/  2] [data: 2944/23054] Training Loss: 0.1615 Training Acc: 95.8899%\n",
      "[Epoch:  2/  2] [data: 3072/23054] Training Loss: 0.0963 Training Acc: 95.9635%\n",
      "[Epoch:  2/  2] [data: 3200/23054] Training Loss: 0.1014 Training Acc: 96.0312%\n",
      "[Epoch:  2/  2] [data: 3328/23054] Training Loss: 0.1131 Training Acc: 96.0337%\n",
      "[Epoch:  2/  2] [data: 3456/23054] Training Loss: 0.1287 Training Acc: 96.0069%\n",
      "[Epoch:  2/  2] [data: 3584/23054] Training Loss: 0.0974 Training Acc: 96.0379%\n",
      "[Epoch:  2/  2] [data: 3712/23054] Training Loss: 0.1190 Training Acc: 96.0129%\n",
      "[Epoch:  2/  2] [data: 3840/23054] Training Loss: 0.1641 Training Acc: 95.9635%\n",
      "[Epoch:  2/  2] [data: 3968/23054] Training Loss: 0.1053 Training Acc: 95.9929%\n",
      "[Epoch:  2/  2] [data: 4096/23054] Training Loss: 0.0292 Training Acc: 96.0449%\n",
      "[Epoch:  2/  2] [data: 4224/23054] Training Loss: 0.1348 Training Acc: 96.0701%\n",
      "[Epoch:  2/  2] [data: 4352/23054] Training Loss: 0.1357 Training Acc: 96.0018%\n",
      "[Epoch:  2/  2] [data: 4480/23054] Training Loss: 0.0892 Training Acc: 96.0268%\n",
      "[Epoch:  2/  2] [data: 4608/23054] Training Loss: 0.1024 Training Acc: 96.0286%\n",
      "[Epoch:  2/  2] [data: 4736/23054] Training Loss: 0.1124 Training Acc: 96.0093%\n",
      "[Epoch:  2/  2] [data: 4864/23054] Training Loss: 0.2276 Training Acc: 95.9498%\n",
      "[Epoch:  2/  2] [data: 4992/23054] Training Loss: 0.0468 Training Acc: 96.0136%\n",
      "[Epoch:  2/  2] [data: 5120/23054] Training Loss: 0.0513 Training Acc: 96.0742%\n",
      "[Epoch:  2/  2] [data: 5248/23054] Training Loss: 0.2306 Training Acc: 96.0556%\n",
      "[Epoch:  2/  2] [data: 5376/23054] Training Loss: 0.0994 Training Acc: 96.0565%\n",
      "[Epoch:  2/  2] [data: 5504/23054] Training Loss: 0.0715 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 5632/23054] Training Loss: 0.1629 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 5760/23054] Training Loss: 0.1806 Training Acc: 96.0417%\n",
      "[Epoch:  2/  2] [data: 5888/23054] Training Loss: 0.0744 Training Acc: 96.0768%\n",
      "[Epoch:  2/  2] [data: 6016/23054] Training Loss: 0.0619 Training Acc: 96.1270%\n",
      "[Epoch:  2/  2] [data: 6144/23054] Training Loss: 0.0673 Training Acc: 96.1263%\n",
      "[Epoch:  2/  2] [data: 6272/23054] Training Loss: 0.1820 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6400/23054] Training Loss: 0.1245 Training Acc: 96.1094%\n",
      "[Epoch:  2/  2] [data: 6528/23054] Training Loss: 0.1610 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6656/23054] Training Loss: 0.1900 Training Acc: 96.0487%\n",
      "[Epoch:  2/  2] [data: 6784/23054] Training Loss: 0.0702 Training Acc: 96.0938%\n",
      "[Epoch:  2/  2] [data: 6912/23054] Training Loss: 0.1417 Training Acc: 96.0793%\n",
      "[Epoch:  2/  2] [data: 7040/23054] Training Loss: 0.0757 Training Acc: 96.1222%\n",
      "[Epoch:  2/  2] [data: 7168/23054] Training Loss: 0.0630 Training Acc: 96.1635%\n",
      "[Epoch:  2/  2] [data: 7296/23054] Training Loss: 0.1680 Training Acc: 96.1623%\n",
      "[Epoch:  2/  2] [data: 7424/23054] Training Loss: 0.0686 Training Acc: 96.1746%\n",
      "[Epoch:  2/  2] [data: 7552/23054] Training Loss: 0.1197 Training Acc: 96.1997%\n",
      "[Epoch:  2/  2] [data: 7680/23054] Training Loss: 0.0823 Training Acc: 96.2109%\n",
      "[Epoch:  2/  2] [data: 7808/23054] Training Loss: 0.0899 Training Acc: 96.2218%\n",
      "[Epoch:  2/  2] [data: 7936/23054] Training Loss: 0.1320 Training Acc: 96.2198%\n",
      "[Epoch:  2/  2] [data: 8064/23054] Training Loss: 0.1644 Training Acc: 96.2178%\n",
      "[Epoch:  2/  2] [data: 8192/23054] Training Loss: 0.1145 Training Acc: 96.2158%\n",
      "[Epoch:  2/  2] [data: 8320/23054] Training Loss: 0.1064 Training Acc: 96.2260%\n",
      "[Epoch:  2/  2] [data: 8448/23054] Training Loss: 0.1726 Training Acc: 96.2003%\n",
      "[Epoch:  2/  2] [data: 8576/23054] Training Loss: 0.1884 Training Acc: 96.1870%\n",
      "[Epoch:  2/  2] [data: 8704/23054] Training Loss: 0.1335 Training Acc: 96.1857%\n",
      "[Epoch:  2/  2] [data: 8832/23054] Training Loss: 0.1168 Training Acc: 96.1957%\n",
      "[Epoch:  2/  2] [data: 8960/23054] Training Loss: 0.1020 Training Acc: 96.1942%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 9088/23054] Training Loss: 0.1135 Training Acc: 96.1928%\n",
      "[Epoch:  2/  2] [data: 9216/23054] Training Loss: 0.0272 Training Acc: 96.2457%\n",
      "[Epoch:  2/  2] [data: 9344/23054] Training Loss: 0.1743 Training Acc: 96.2436%\n",
      "[Epoch:  2/  2] [data: 9472/23054] Training Loss: 0.1085 Training Acc: 96.2204%\n",
      "[Epoch:  2/  2] [data: 9600/23054] Training Loss: 0.1501 Training Acc: 96.2083%\n",
      "[Epoch:  2/  2] [data: 9728/23054] Training Loss: 0.0966 Training Acc: 96.2274%\n",
      "[Epoch:  2/  2] [data: 9856/23054] Training Loss: 0.0946 Training Acc: 96.2561%\n",
      "[Epoch:  2/  2] [data: 9984/23054] Training Loss: 0.0852 Training Acc: 96.2740%\n",
      "[Epoch:  2/  2] [data: 10112/23054] Training Loss: 0.1270 Training Acc: 96.2718%\n",
      "[Epoch:  2/  2] [data: 10240/23054] Training Loss: 0.0587 Training Acc: 96.2988%\n",
      "[Epoch:  2/  2] [data: 10368/23054] Training Loss: 0.0922 Training Acc: 96.3156%\n",
      "[Epoch:  2/  2] [data: 10496/23054] Training Loss: 0.1113 Training Acc: 96.3034%\n",
      "[Epoch:  2/  2] [data: 10624/23054] Training Loss: 0.1582 Training Acc: 96.2820%\n",
      "[Epoch:  2/  2] [data: 10752/23054] Training Loss: 0.1053 Training Acc: 96.2891%\n",
      "[Epoch:  2/  2] [data: 10880/23054] Training Loss: 0.0627 Training Acc: 96.2960%\n",
      "[Epoch:  2/  2] [data: 11008/23054] Training Loss: 0.2021 Training Acc: 96.2754%\n",
      "[Epoch:  2/  2] [data: 11136/23054] Training Loss: 0.1273 Training Acc: 96.2733%\n",
      "[Epoch:  2/  2] [data: 11264/23054] Training Loss: 0.1283 Training Acc: 96.2713%\n",
      "[Epoch:  2/  2] [data: 11392/23054] Training Loss: 0.1769 Training Acc: 96.2430%\n",
      "[Epoch:  2/  2] [data: 11520/23054] Training Loss: 0.0634 Training Acc: 96.2674%\n",
      "[Epoch:  2/  2] [data: 11648/23054] Training Loss: 0.1050 Training Acc: 96.2655%\n",
      "[Epoch:  2/  2] [data: 11776/23054] Training Loss: 0.1485 Training Acc: 96.2721%\n",
      "[Epoch:  2/  2] [data: 11904/23054] Training Loss: 0.2159 Training Acc: 96.2450%\n",
      "[Epoch:  2/  2] [data: 12032/23054] Training Loss: 0.1379 Training Acc: 96.2267%\n",
      "[Epoch:  2/  2] [data: 12160/23054] Training Loss: 0.1542 Training Acc: 96.2007%\n",
      "[Epoch:  2/  2] [data: 12288/23054] Training Loss: 0.1313 Training Acc: 96.1833%\n",
      "[Epoch:  2/  2] [data: 12416/23054] Training Loss: 0.0993 Training Acc: 96.1985%\n",
      "[Epoch:  2/  2] [data: 12544/23054] Training Loss: 0.1283 Training Acc: 96.1974%\n",
      "[Epoch:  2/  2] [data: 12672/23054] Training Loss: 0.2145 Training Acc: 96.1806%\n",
      "[Epoch:  2/  2] [data: 12800/23054] Training Loss: 0.1306 Training Acc: 96.1875%\n",
      "[Epoch:  2/  2] [data: 12928/23054] Training Loss: 0.1756 Training Acc: 96.1634%\n",
      "[Epoch:  2/  2] [data: 13056/23054] Training Loss: 0.1076 Training Acc: 96.1627%\n",
      "[Epoch:  2/  2] [data: 13184/23054] Training Loss: 0.1432 Training Acc: 96.1620%\n",
      "[Epoch:  2/  2] [data: 13312/23054] Training Loss: 0.0610 Training Acc: 96.1839%\n",
      "[Epoch:  2/  2] [data: 13440/23054] Training Loss: 0.0708 Training Acc: 96.1979%\n",
      "[Epoch:  2/  2] [data: 13568/23054] Training Loss: 0.1618 Training Acc: 96.1822%\n",
      "[Epoch:  2/  2] [data: 13696/23054] Training Loss: 0.1525 Training Acc: 96.1741%\n",
      "[Epoch:  2/  2] [data: 13824/23054] Training Loss: 0.0536 Training Acc: 96.1950%\n",
      "[Epoch:  2/  2] [data: 13952/23054] Training Loss: 0.1560 Training Acc: 96.1798%\n",
      "[Epoch:  2/  2] [data: 14080/23054] Training Loss: 0.0566 Training Acc: 96.2003%\n",
      "[Epoch:  2/  2] [data: 14208/23054] Training Loss: 0.0314 Training Acc: 96.2275%\n",
      "[Epoch:  2/  2] [data: 14336/23054] Training Loss: 0.1050 Training Acc: 96.2402%\n",
      "[Epoch:  2/  2] [data: 14464/23054] Training Loss: 0.3070 Training Acc: 96.2113%\n",
      "[Epoch:  2/  2] [data: 14592/23054] Training Loss: 0.0870 Training Acc: 96.2103%\n",
      "[Epoch:  2/  2] [data: 14720/23054] Training Loss: 0.1548 Training Acc: 96.2160%\n",
      "[Epoch:  2/  2] [data: 14848/23054] Training Loss: 0.1194 Training Acc: 96.2150%\n",
      "[Epoch:  2/  2] [data: 14976/23054] Training Loss: 0.1566 Training Acc: 96.2139%\n",
      "[Epoch:  2/  2] [data: 15104/23054] Training Loss: 0.0942 Training Acc: 96.2195%\n",
      "[Epoch:  2/  2] [data: 15232/23054] Training Loss: 0.1097 Training Acc: 96.2185%\n",
      "[Epoch:  2/  2] [data: 15360/23054] Training Loss: 0.1673 Training Acc: 96.1914%\n",
      "[Epoch:  2/  2] [data: 15488/23054] Training Loss: 0.1125 Training Acc: 96.1971%\n",
      "[Epoch:  2/  2] [data: 15616/23054] Training Loss: 0.1964 Training Acc: 96.1898%\n",
      "[Epoch:  2/  2] [data: 15744/23054] Training Loss: 0.1054 Training Acc: 96.2017%\n",
      "[Epoch:  2/  2] [data: 15872/23054] Training Loss: 0.1400 Training Acc: 96.2072%\n",
      "[Epoch:  2/  2] [data: 16000/23054] Training Loss: 0.2026 Training Acc: 96.1937%\n",
      "[Epoch:  2/  2] [data: 16128/23054] Training Loss: 0.2267 Training Acc: 96.1620%\n",
      "[Epoch:  2/  2] [data: 16256/23054] Training Loss: 0.0973 Training Acc: 96.1614%\n",
      "[Epoch:  2/  2] [data: 16384/23054] Training Loss: 0.1509 Training Acc: 96.1426%\n",
      "[Epoch:  2/  2] [data: 16512/23054] Training Loss: 0.0889 Training Acc: 96.1543%\n",
      "[Epoch:  2/  2] [data: 16640/23054] Training Loss: 0.1924 Training Acc: 96.1358%\n",
      "[Epoch:  2/  2] [data: 16768/23054] Training Loss: 0.2013 Training Acc: 96.1236%\n",
      "[Epoch:  2/  2] [data: 16896/23054] Training Loss: 0.0870 Training Acc: 96.1233%\n",
      "[Epoch:  2/  2] [data: 17024/23054] Training Loss: 0.0954 Training Acc: 96.1231%\n",
      "[Epoch:  2/  2] [data: 17152/23054] Training Loss: 0.0408 Training Acc: 96.1462%\n",
      "[Epoch:  2/  2] [data: 17280/23054] Training Loss: 0.1432 Training Acc: 96.1400%\n",
      "[Epoch:  2/  2] [data: 17408/23054] Training Loss: 0.0916 Training Acc: 96.1340%\n",
      "[Epoch:  2/  2] [data: 17536/23054] Training Loss: 0.0737 Training Acc: 96.1337%\n",
      "[Epoch:  2/  2] [data: 17664/23054] Training Loss: 0.1180 Training Acc: 96.1390%\n",
      "[Epoch:  2/  2] [data: 17792/23054] Training Loss: 0.0546 Training Acc: 96.1443%\n",
      "[Epoch:  2/  2] [data: 17920/23054] Training Loss: 0.1361 Training Acc: 96.1440%\n",
      "[Epoch:  2/  2] [data: 18048/23054] Training Loss: 0.2462 Training Acc: 96.1215%\n",
      "[Epoch:  2/  2] [data: 18176/23054] Training Loss: 0.1409 Training Acc: 96.1268%\n",
      "[Epoch:  2/  2] [data: 18304/23054] Training Loss: 0.1747 Training Acc: 96.1211%\n",
      "[Epoch:  2/  2] [data: 18432/23054] Training Loss: 0.0987 Training Acc: 96.1155%\n",
      "[Epoch:  2/  2] [data: 18560/23054] Training Loss: 0.0897 Training Acc: 96.1207%\n",
      "[Epoch:  2/  2] [data: 18688/23054] Training Loss: 0.1023 Training Acc: 96.1259%\n",
      "[Epoch:  2/  2] [data: 18816/23054] Training Loss: 0.0570 Training Acc: 96.1469%\n",
      "[Epoch:  2/  2] [data: 18944/23054] Training Loss: 0.1421 Training Acc: 96.1465%\n",
      "[Epoch:  2/  2] [data: 19072/23054] Training Loss: 0.1320 Training Acc: 96.1567%\n",
      "[Epoch:  2/  2] [data: 19200/23054] Training Loss: 0.1722 Training Acc: 96.1354%\n",
      "[Epoch:  2/  2] [data: 19328/23054] Training Loss: 0.1683 Training Acc: 96.1248%\n",
      "[Epoch:  2/  2] [data: 19456/23054] Training Loss: 0.1132 Training Acc: 96.1297%\n",
      "[Epoch:  2/  2] [data: 19584/23054] Training Loss: 0.0735 Training Acc: 96.1397%\n",
      "[Epoch:  2/  2] [data: 19712/23054] Training Loss: 0.1487 Training Acc: 96.1394%\n",
      "[Epoch:  2/  2] [data: 19840/23054] Training Loss: 0.1060 Training Acc: 96.1341%\n",
      "[Epoch:  2/  2] [data: 19968/23054] Training Loss: 0.1543 Training Acc: 96.1388%\n",
      "[Epoch:  2/  2] [data: 20096/23054] Training Loss: 0.0862 Training Acc: 96.1535%\n",
      "[Epoch:  2/  2] [data: 20224/23054] Training Loss: 0.0641 Training Acc: 96.1679%\n",
      "[Epoch:  2/  2] [data: 20352/23054] Training Loss: 0.1482 Training Acc: 96.1625%\n",
      "[Epoch:  2/  2] [data: 20480/23054] Training Loss: 0.0756 Training Acc: 96.1475%\n",
      "[Epoch:  2/  2] [data: 20608/23054] Training Loss: 0.1851 Training Acc: 96.1374%\n",
      "[Epoch:  2/  2] [data: 20736/23054] Training Loss: 0.0848 Training Acc: 96.1468%\n",
      "[Epoch:  2/  2] [data: 20864/23054] Training Loss: 0.0595 Training Acc: 96.1561%\n",
      "[Epoch:  2/  2] [data: 20992/23054] Training Loss: 0.1117 Training Acc: 96.1509%\n",
      "[Epoch:  2/  2] [data: 21120/23054] Training Loss: 0.1249 Training Acc: 96.1506%\n",
      "[Epoch:  2/  2] [data: 21248/23054] Training Loss: 0.0815 Training Acc: 96.1502%\n",
      "[Epoch:  2/  2] [data: 21376/23054] Training Loss: 0.0717 Training Acc: 96.1452%\n",
      "[Epoch:  2/  2] [data: 21504/23054] Training Loss: 0.1601 Training Acc: 96.1403%\n",
      "[Epoch:  2/  2] [data: 21632/23054] Training Loss: 0.0709 Training Acc: 96.1538%\n",
      "[Epoch:  2/  2] [data: 21760/23054] Training Loss: 0.1426 Training Acc: 96.1443%\n",
      "[Epoch:  2/  2] [data: 21888/23054] Training Loss: 0.1421 Training Acc: 96.1486%\n",
      "[Epoch:  2/  2] [data: 22016/23054] Training Loss: 0.1860 Training Acc: 96.1346%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  2] [data: 22144/23054] Training Loss: 0.1413 Training Acc: 96.1299%\n",
      "[Epoch:  2/  2] [data: 22272/23054] Training Loss: 0.1793 Training Acc: 96.1252%\n",
      "[Epoch:  2/  2] [data: 22400/23054] Training Loss: 0.1946 Training Acc: 96.1205%\n",
      "[Epoch:  2/  2] [data: 22528/23054] Training Loss: 0.1563 Training Acc: 96.1159%\n",
      "[Epoch:  2/  2] [data: 22656/23054] Training Loss: 0.0585 Training Acc: 96.1291%\n",
      "[Epoch:  2/  2] [data: 22784/23054] Training Loss: 0.0822 Training Acc: 96.1157%\n",
      "[Epoch:  2/  2] [data: 22912/23054] Training Loss: 0.2099 Training Acc: 96.1112%\n",
      "[Epoch:  2/  2] [data: 23040/23054] Training Loss: 0.2349 Training Acc: 96.0807%\n",
      "[Epoch:  2/  2] [data: 23054/23054] Training Loss: 0.2632 Training Acc: 96.0788%\n",
      "Testing-3...\n",
      "[Epoch:  2/  2] Validation Loss: 0.1205 Validation Acc: 96.4492%\n",
      "Time used: 1109.9475662708282s\n",
      "Type-3: 1.0 0.9126266776225691 0.9543176285264213\n",
      "[Epoch:  1/  5] [data: 128/24058] Training Loss: 0.2702 Training Acc: 85.1562%\n",
      "[Epoch:  1/  5] [data: 256/24058] Training Loss: 0.3907 Training Acc: 81.6406%\n",
      "[Epoch:  1/  5] [data: 384/24058] Training Loss: 0.3654 Training Acc: 80.4688%\n",
      "[Epoch:  1/  5] [data: 512/24058] Training Loss: 0.2389 Training Acc: 82.2266%\n",
      "[Epoch:  1/  5] [data: 640/24058] Training Loss: 0.3127 Training Acc: 82.1875%\n",
      "[Epoch:  1/  5] [data: 768/24058] Training Loss: 0.3018 Training Acc: 82.1615%\n",
      "[Epoch:  1/  5] [data: 896/24058] Training Loss: 0.1664 Training Acc: 82.3661%\n",
      "[Epoch:  1/  5] [data: 1024/24058] Training Loss: 0.2193 Training Acc: 83.0078%\n",
      "[Epoch:  1/  5] [data: 1152/24058] Training Loss: 0.2530 Training Acc: 83.1597%\n",
      "[Epoch:  1/  5] [data: 1280/24058] Training Loss: 0.5216 Training Acc: 82.4219%\n",
      "[Epoch:  1/  5] [data: 1408/24058] Training Loss: 0.2670 Training Acc: 83.0256%\n",
      "[Epoch:  1/  5] [data: 1536/24058] Training Loss: 0.1509 Training Acc: 83.4635%\n",
      "[Epoch:  1/  5] [data: 1664/24058] Training Loss: 0.3041 Training Acc: 83.8341%\n",
      "[Epoch:  1/  5] [data: 1792/24058] Training Loss: 0.3042 Training Acc: 84.0960%\n",
      "[Epoch:  1/  5] [data: 1920/24058] Training Loss: 0.1677 Training Acc: 84.6875%\n",
      "[Epoch:  1/  5] [data: 2048/24058] Training Loss: 0.2394 Training Acc: 85.1074%\n",
      "[Epoch:  1/  5] [data: 2176/24058] Training Loss: 0.1541 Training Acc: 85.7077%\n",
      "[Epoch:  1/  5] [data: 2304/24058] Training Loss: 0.3004 Training Acc: 85.8941%\n",
      "[Epoch:  1/  5] [data: 2432/24058] Training Loss: 0.1293 Training Acc: 86.2664%\n",
      "[Epoch:  1/  5] [data: 2560/24058] Training Loss: 0.2401 Training Acc: 86.4453%\n",
      "[Epoch:  1/  5] [data: 2688/24058] Training Loss: 0.1418 Training Acc: 86.7560%\n",
      "[Epoch:  1/  5] [data: 2816/24058] Training Loss: 0.2487 Training Acc: 86.9673%\n",
      "[Epoch:  1/  5] [data: 2944/24058] Training Loss: 0.1361 Training Acc: 87.2622%\n",
      "[Epoch:  1/  5] [data: 3072/24058] Training Loss: 0.3250 Training Acc: 87.3698%\n",
      "[Epoch:  1/  5] [data: 3200/24058] Training Loss: 0.2640 Training Acc: 87.4688%\n",
      "[Epoch:  1/  5] [data: 3328/24058] Training Loss: 0.1985 Training Acc: 87.6803%\n",
      "[Epoch:  1/  5] [data: 3456/24058] Training Loss: 0.1803 Training Acc: 87.8762%\n",
      "[Epoch:  1/  5] [data: 3584/24058] Training Loss: 0.1632 Training Acc: 88.0580%\n",
      "[Epoch:  1/  5] [data: 3712/24058] Training Loss: 0.3578 Training Acc: 87.9580%\n",
      "[Epoch:  1/  5] [data: 3840/24058] Training Loss: 0.1421 Training Acc: 88.2292%\n",
      "[Epoch:  1/  5] [data: 3968/24058] Training Loss: 0.1863 Training Acc: 88.4577%\n",
      "[Epoch:  1/  5] [data: 4096/24058] Training Loss: 0.2596 Training Acc: 88.4766%\n",
      "[Epoch:  1/  5] [data: 4224/24058] Training Loss: 0.1755 Training Acc: 88.6837%\n",
      "[Epoch:  1/  5] [data: 4352/24058] Training Loss: 0.1279 Training Acc: 88.7408%\n",
      "[Epoch:  1/  5] [data: 4480/24058] Training Loss: 0.2629 Training Acc: 88.6830%\n",
      "[Epoch:  1/  5] [data: 4608/24058] Training Loss: 0.1364 Training Acc: 88.7587%\n",
      "[Epoch:  1/  5] [data: 4736/24058] Training Loss: 0.2840 Training Acc: 88.7880%\n",
      "[Epoch:  1/  5] [data: 4864/24058] Training Loss: 0.1482 Training Acc: 88.9186%\n",
      "[Epoch:  1/  5] [data: 4992/24058] Training Loss: 0.1661 Training Acc: 88.9623%\n",
      "[Epoch:  1/  5] [data: 5120/24058] Training Loss: 0.2417 Training Acc: 88.9453%\n",
      "[Epoch:  1/  5] [data: 5248/24058] Training Loss: 0.2261 Training Acc: 88.9863%\n",
      "[Epoch:  1/  5] [data: 5376/24058] Training Loss: 0.2402 Training Acc: 89.0253%\n",
      "[Epoch:  1/  5] [data: 5504/24058] Training Loss: 0.1695 Training Acc: 89.2078%\n",
      "[Epoch:  1/  5] [data: 5632/24058] Training Loss: 0.2169 Training Acc: 89.3111%\n",
      "[Epoch:  1/  5] [data: 5760/24058] Training Loss: 0.1813 Training Acc: 89.4097%\n",
      "[Epoch:  1/  5] [data: 5888/24058] Training Loss: 0.1740 Training Acc: 89.4871%\n",
      "[Epoch:  1/  5] [data: 6016/24058] Training Loss: 0.2005 Training Acc: 89.5612%\n",
      "[Epoch:  1/  5] [data: 6144/24058] Training Loss: 0.2066 Training Acc: 89.6322%\n",
      "[Epoch:  1/  5] [data: 6272/24058] Training Loss: 0.2504 Training Acc: 89.6524%\n",
      "[Epoch:  1/  5] [data: 6400/24058] Training Loss: 0.2192 Training Acc: 89.7031%\n",
      "[Epoch:  1/  5] [data: 6528/24058] Training Loss: 0.1886 Training Acc: 89.7365%\n",
      "[Epoch:  1/  5] [data: 6656/24058] Training Loss: 0.2144 Training Acc: 89.7236%\n",
      "[Epoch:  1/  5] [data: 6784/24058] Training Loss: 0.1699 Training Acc: 89.8290%\n",
      "[Epoch:  1/  5] [data: 6912/24058] Training Loss: 0.2339 Training Acc: 89.8582%\n",
      "[Epoch:  1/  5] [data: 7040/24058] Training Loss: 0.1874 Training Acc: 89.9290%\n",
      "[Epoch:  1/  5] [data: 7168/24058] Training Loss: 0.2422 Training Acc: 89.9414%\n",
      "[Epoch:  1/  5] [data: 7296/24058] Training Loss: 0.1276 Training Acc: 90.0630%\n",
      "[Epoch:  1/  5] [data: 7424/24058] Training Loss: 0.1510 Training Acc: 90.1401%\n",
      "[Epoch:  1/  5] [data: 7552/24058] Training Loss: 0.1145 Training Acc: 90.2542%\n",
      "[Epoch:  1/  5] [data: 7680/24058] Training Loss: 0.2136 Training Acc: 90.2474%\n",
      "[Epoch:  1/  5] [data: 7808/24058] Training Loss: 0.2656 Training Acc: 90.2664%\n",
      "[Epoch:  1/  5] [data: 7936/24058] Training Loss: 0.3702 Training Acc: 90.2218%\n",
      "[Epoch:  1/  5] [data: 8064/24058] Training Loss: 0.3340 Training Acc: 90.1290%\n",
      "[Epoch:  1/  5] [data: 8192/24058] Training Loss: 0.1211 Training Acc: 90.2100%\n",
      "[Epoch:  1/  5] [data: 8320/24058] Training Loss: 0.2188 Training Acc: 90.2644%\n",
      "[Epoch:  1/  5] [data: 8448/24058] Training Loss: 0.2668 Training Acc: 90.2699%\n",
      "[Epoch:  1/  5] [data: 8576/24058] Training Loss: 0.1172 Training Acc: 90.3801%\n",
      "[Epoch:  1/  5] [data: 8704/24058] Training Loss: 0.2286 Training Acc: 90.4067%\n",
      "[Epoch:  1/  5] [data: 8832/24058] Training Loss: 0.2957 Training Acc: 90.4099%\n",
      "[Epoch:  1/  5] [data: 8960/24058] Training Loss: 0.3059 Training Acc: 90.4018%\n",
      "[Epoch:  1/  5] [data: 9088/24058] Training Loss: 0.2290 Training Acc: 90.4049%\n",
      "[Epoch:  1/  5] [data: 9216/24058] Training Loss: 0.1589 Training Acc: 90.4622%\n",
      "[Epoch:  1/  5] [data: 9344/24058] Training Loss: 0.2105 Training Acc: 90.4966%\n",
      "[Epoch:  1/  5] [data: 9472/24058] Training Loss: 0.2159 Training Acc: 90.5194%\n",
      "[Epoch:  1/  5] [data: 9600/24058] Training Loss: 0.2930 Training Acc: 90.5312%\n",
      "[Epoch:  1/  5] [data: 9728/24058] Training Loss: 0.1930 Training Acc: 90.5736%\n",
      "[Epoch:  1/  5] [data: 9856/24058] Training Loss: 0.2340 Training Acc: 90.6047%\n",
      "[Epoch:  1/  5] [data: 9984/24058] Training Loss: 0.1814 Training Acc: 90.6350%\n",
      "[Epoch:  1/  5] [data: 10112/24058] Training Loss: 0.2280 Training Acc: 90.6744%\n",
      "[Epoch:  1/  5] [data: 10240/24058] Training Loss: 0.1871 Training Acc: 90.6836%\n",
      "[Epoch:  1/  5] [data: 10368/24058] Training Loss: 0.1914 Training Acc: 90.7118%\n",
      "[Epoch:  1/  5] [data: 10496/24058] Training Loss: 0.1414 Training Acc: 90.7774%\n",
      "[Epoch:  1/  5] [data: 10624/24058] Training Loss: 0.1218 Training Acc: 90.8321%\n",
      "[Epoch:  1/  5] [data: 10752/24058] Training Loss: 0.1524 Training Acc: 90.8575%\n",
      "[Epoch:  1/  5] [data: 10880/24058] Training Loss: 0.2043 Training Acc: 90.8915%\n",
      "[Epoch:  1/  5] [data: 11008/24058] Training Loss: 0.2656 Training Acc: 90.8703%\n",
      "[Epoch:  1/  5] [data: 11136/24058] Training Loss: 0.2318 Training Acc: 90.8944%\n",
      "[Epoch:  1/  5] [data: 11264/24058] Training Loss: 0.2173 Training Acc: 90.9002%\n",
      "[Epoch:  1/  5] [data: 11392/24058] Training Loss: 0.1001 Training Acc: 90.9673%\n",
      "[Epoch:  1/  5] [data: 11520/24058] Training Loss: 0.1864 Training Acc: 90.9896%\n",
      "[Epoch:  1/  5] [data: 11648/24058] Training Loss: 0.2069 Training Acc: 91.0199%\n",
      "[Epoch:  1/  5] [data: 11776/24058] Training Loss: 0.3017 Training Acc: 91.0241%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  5] [data: 11904/24058] Training Loss: 0.1757 Training Acc: 91.0366%\n",
      "[Epoch:  1/  5] [data: 12032/24058] Training Loss: 0.1775 Training Acc: 91.0738%\n",
      "[Epoch:  1/  5] [data: 12160/24058] Training Loss: 0.1806 Training Acc: 91.1020%\n",
      "[Epoch:  1/  5] [data: 12288/24058] Training Loss: 0.1759 Training Acc: 91.1540%\n",
      "[Epoch:  1/  5] [data: 12416/24058] Training Loss: 0.2805 Training Acc: 91.1485%\n",
      "[Epoch:  1/  5] [data: 12544/24058] Training Loss: 0.1644 Training Acc: 91.1910%\n",
      "[Epoch:  1/  5] [data: 12672/24058] Training Loss: 0.2476 Training Acc: 91.1932%\n",
      "[Epoch:  1/  5] [data: 12800/24058] Training Loss: 0.2447 Training Acc: 91.1875%\n",
      "[Epoch:  1/  5] [data: 12928/24058] Training Loss: 0.1383 Training Acc: 91.2361%\n",
      "[Epoch:  1/  5] [data: 13056/24058] Training Loss: 0.2213 Training Acc: 91.2531%\n",
      "[Epoch:  1/  5] [data: 13184/24058] Training Loss: 0.1988 Training Acc: 91.2849%\n",
      "[Epoch:  1/  5] [data: 13312/24058] Training Loss: 0.1332 Training Acc: 91.3386%\n",
      "[Epoch:  1/  5] [data: 13440/24058] Training Loss: 0.1159 Training Acc: 91.3914%\n",
      "[Epoch:  1/  5] [data: 13568/24058] Training Loss: 0.2029 Training Acc: 91.4210%\n",
      "[Epoch:  1/  5] [data: 13696/24058] Training Loss: 0.2127 Training Acc: 91.4501%\n",
      "[Epoch:  1/  5] [data: 13824/24058] Training Loss: 0.1645 Training Acc: 91.4569%\n",
      "[Epoch:  1/  5] [data: 13952/24058] Training Loss: 0.2038 Training Acc: 91.4851%\n",
      "[Epoch:  1/  5] [data: 14080/24058] Training Loss: 0.1788 Training Acc: 91.5270%\n",
      "[Epoch:  1/  5] [data: 14208/24058] Training Loss: 0.2834 Training Acc: 91.5118%\n",
      "[Epoch:  1/  5] [data: 14336/24058] Training Loss: 0.1647 Training Acc: 91.5527%\n",
      "[Epoch:  1/  5] [data: 14464/24058] Training Loss: 0.1126 Training Acc: 91.5929%\n",
      "[Epoch:  1/  5] [data: 14592/24058] Training Loss: 0.1938 Training Acc: 91.6187%\n",
      "[Epoch:  1/  5] [data: 14720/24058] Training Loss: 0.0864 Training Acc: 91.6712%\n",
      "[Epoch:  1/  5] [data: 14848/24058] Training Loss: 0.1944 Training Acc: 91.6756%\n",
      "[Epoch:  1/  5] [data: 14976/24058] Training Loss: 0.1623 Training Acc: 91.7067%\n",
      "[Epoch:  1/  5] [data: 15104/24058] Training Loss: 0.1551 Training Acc: 91.7307%\n",
      "[Epoch:  1/  5] [data: 15232/24058] Training Loss: 0.1879 Training Acc: 91.7411%\n",
      "[Epoch:  1/  5] [data: 15360/24058] Training Loss: 0.1740 Training Acc: 91.7839%\n",
      "[Epoch:  1/  5] [data: 15488/24058] Training Loss: 0.1566 Training Acc: 91.8001%\n",
      "[Epoch:  1/  5] [data: 15616/24058] Training Loss: 0.1779 Training Acc: 91.8225%\n",
      "[Epoch:  1/  5] [data: 15744/24058] Training Loss: 0.2624 Training Acc: 91.8255%\n",
      "[Epoch:  1/  5] [data: 15872/24058] Training Loss: 0.1627 Training Acc: 91.8536%\n",
      "[Epoch:  1/  5] [data: 16000/24058] Training Loss: 0.2230 Training Acc: 91.8625%\n",
      "[Epoch:  1/  5] [data: 16128/24058] Training Loss: 0.1656 Training Acc: 91.8713%\n",
      "[Epoch:  1/  5] [data: 16256/24058] Training Loss: 0.2239 Training Acc: 91.8861%\n",
      "[Epoch:  1/  5] [data: 16384/24058] Training Loss: 0.2185 Training Acc: 91.8884%\n",
      "[Epoch:  1/  5] [data: 16512/24058] Training Loss: 0.2933 Training Acc: 91.8665%\n",
      "[Epoch:  1/  5] [data: 16640/24058] Training Loss: 0.1978 Training Acc: 91.8870%\n",
      "[Epoch:  1/  5] [data: 16768/24058] Training Loss: 0.1506 Training Acc: 91.9191%\n",
      "[Epoch:  1/  5] [data: 16896/24058] Training Loss: 0.2114 Training Acc: 91.9389%\n",
      "[Epoch:  1/  5] [data: 17024/24058] Training Loss: 0.2983 Training Acc: 91.9114%\n",
      "[Epoch:  1/  5] [data: 17152/24058] Training Loss: 0.1966 Training Acc: 91.9310%\n",
      "[Epoch:  1/  5] [data: 17280/24058] Training Loss: 0.2590 Training Acc: 91.9329%\n",
      "[Epoch:  1/  5] [data: 17408/24058] Training Loss: 0.2554 Training Acc: 91.9290%\n",
      "[Epoch:  1/  5] [data: 17536/24058] Training Loss: 0.1477 Training Acc: 91.9537%\n",
      "[Epoch:  1/  5] [data: 17664/24058] Training Loss: 0.1774 Training Acc: 91.9780%\n",
      "[Epoch:  1/  5] [data: 17792/24058] Training Loss: 0.1670 Training Acc: 91.9908%\n",
      "[Epoch:  1/  5] [data: 17920/24058] Training Loss: 0.2446 Training Acc: 91.9922%\n",
      "[Epoch:  1/  5] [data: 18048/24058] Training Loss: 0.1489 Training Acc: 92.0047%\n",
      "[Epoch:  1/  5] [data: 18176/24058] Training Loss: 0.2807 Training Acc: 91.9949%\n",
      "[Epoch:  1/  5] [data: 18304/24058] Training Loss: 0.2046 Training Acc: 92.0072%\n",
      "[Epoch:  1/  5] [data: 18432/24058] Training Loss: 0.1680 Training Acc: 92.0356%\n",
      "[Epoch:  1/  5] [data: 18560/24058] Training Loss: 0.1953 Training Acc: 92.0420%\n",
      "[Epoch:  1/  5] [data: 18688/24058] Training Loss: 0.1213 Training Acc: 92.0751%\n",
      "[Epoch:  1/  5] [data: 18816/24058] Training Loss: 0.1697 Training Acc: 92.0972%\n",
      "[Epoch:  1/  5] [data: 18944/24058] Training Loss: 0.1844 Training Acc: 92.1189%\n",
      "[Epoch:  1/  5] [data: 19072/24058] Training Loss: 0.1685 Training Acc: 92.1456%\n",
      "[Epoch:  1/  5] [data: 19200/24058] Training Loss: 0.1030 Training Acc: 92.1771%\n",
      "[Epoch:  1/  5] [data: 19328/24058] Training Loss: 0.2267 Training Acc: 92.1823%\n",
      "[Epoch:  1/  5] [data: 19456/24058] Training Loss: 0.1976 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 19584/24058] Training Loss: 0.1424 Training Acc: 92.2028%\n",
      "[Epoch:  1/  5] [data: 19712/24058] Training Loss: 0.1667 Training Acc: 92.2179%\n",
      "[Epoch:  1/  5] [data: 19840/24058] Training Loss: 0.1486 Training Acc: 92.2228%\n",
      "[Epoch:  1/  5] [data: 19968/24058] Training Loss: 0.2217 Training Acc: 92.2326%\n",
      "[Epoch:  1/  5] [data: 20096/24058] Training Loss: 0.3190 Training Acc: 92.2223%\n",
      "[Epoch:  1/  5] [data: 20224/24058] Training Loss: 0.2499 Training Acc: 92.2172%\n",
      "[Epoch:  1/  5] [data: 20352/24058] Training Loss: 0.1693 Training Acc: 92.2366%\n",
      "[Epoch:  1/  5] [data: 20480/24058] Training Loss: 0.1462 Training Acc: 92.2656%\n",
      "[Epoch:  1/  5] [data: 20608/24058] Training Loss: 0.2926 Training Acc: 92.2554%\n",
      "[Epoch:  1/  5] [data: 20736/24058] Training Loss: 0.1408 Training Acc: 92.2695%\n",
      "[Epoch:  1/  5] [data: 20864/24058] Training Loss: 0.1509 Training Acc: 92.2834%\n",
      "[Epoch:  1/  5] [data: 20992/24058] Training Loss: 0.2233 Training Acc: 92.2828%\n",
      "[Epoch:  1/  5] [data: 21120/24058] Training Loss: 0.2132 Training Acc: 92.2964%\n",
      "[Epoch:  1/  5] [data: 21248/24058] Training Loss: 0.1375 Training Acc: 92.3193%\n",
      "[Epoch:  1/  5] [data: 21376/24058] Training Loss: 0.1639 Training Acc: 92.3325%\n",
      "[Epoch:  1/  5] [data: 21504/24058] Training Loss: 0.1151 Training Acc: 92.3503%\n",
      "[Epoch:  1/  5] [data: 21632/24058] Training Loss: 0.2871 Training Acc: 92.3401%\n",
      "[Epoch:  1/  5] [data: 21760/24058] Training Loss: 0.1905 Training Acc: 92.3438%\n",
      "[Epoch:  1/  5] [data: 21888/24058] Training Loss: 0.2351 Training Acc: 92.3474%\n",
      "[Epoch:  1/  5] [data: 22016/24058] Training Loss: 0.2134 Training Acc: 92.3556%\n",
      "[Epoch:  1/  5] [data: 22144/24058] Training Loss: 0.2043 Training Acc: 92.3681%\n",
      "[Epoch:  1/  5] [data: 22272/24058] Training Loss: 0.1611 Training Acc: 92.3851%\n",
      "[Epoch:  1/  5] [data: 22400/24058] Training Loss: 0.1721 Training Acc: 92.3973%\n",
      "[Epoch:  1/  5] [data: 22528/24058] Training Loss: 0.2008 Training Acc: 92.4094%\n",
      "[Epoch:  1/  5] [data: 22656/24058] Training Loss: 0.2679 Training Acc: 92.4038%\n",
      "[Epoch:  1/  5] [data: 22784/24058] Training Loss: 0.1963 Training Acc: 92.4157%\n",
      "[Epoch:  1/  5] [data: 22912/24058] Training Loss: 0.2080 Training Acc: 92.4232%\n",
      "[Epoch:  1/  5] [data: 23040/24058] Training Loss: 0.2528 Training Acc: 92.4175%\n",
      "[Epoch:  1/  5] [data: 23168/24058] Training Loss: 0.1802 Training Acc: 92.4292%\n",
      "[Epoch:  1/  5] [data: 23296/24058] Training Loss: 0.2097 Training Acc: 92.4365%\n",
      "[Epoch:  1/  5] [data: 23424/24058] Training Loss: 0.1402 Training Acc: 92.4565%\n",
      "[Epoch:  1/  5] [data: 23552/24058] Training Loss: 0.1695 Training Acc: 92.4720%\n",
      "[Epoch:  1/  5] [data: 23680/24058] Training Loss: 0.2168 Training Acc: 92.4747%\n",
      "[Epoch:  1/  5] [data: 23808/24058] Training Loss: 0.0810 Training Acc: 92.4983%\n",
      "[Epoch:  1/  5] [data: 23936/24058] Training Loss: 0.1569 Training Acc: 92.5175%\n",
      "[Epoch:  1/  5] [data: 24058/24058] Training Loss: 0.1783 Training Acc: 92.5222%\n",
      "Testing-4...\n",
      "[Epoch:  1/  5] Validation Loss: 0.1979 Validation Acc: 93.5447%\n",
      "Time used: 1169.54776096344s\n",
      "[Epoch:  2/  5] [data: 128/24058] Training Loss: 0.2346 Training Acc: 92.1875%\n",
      "[Epoch:  2/  5] [data: 256/24058] Training Loss: 0.3126 Training Acc: 89.8438%\n",
      "[Epoch:  2/  5] [data: 384/24058] Training Loss: 0.3259 Training Acc: 90.3646%\n",
      "[Epoch:  2/  5] [data: 512/24058] Training Loss: 0.1158 Training Acc: 91.7969%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 640/24058] Training Loss: 0.2030 Training Acc: 92.0312%\n",
      "[Epoch:  2/  5] [data: 768/24058] Training Loss: 0.2306 Training Acc: 92.1875%\n",
      "[Epoch:  2/  5] [data: 896/24058] Training Loss: 0.0916 Training Acc: 92.9688%\n",
      "[Epoch:  2/  5] [data: 1024/24058] Training Loss: 0.1730 Training Acc: 93.2617%\n",
      "[Epoch:  2/  5] [data: 1152/24058] Training Loss: 0.1832 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 1280/24058] Training Loss: 0.4563 Training Acc: 92.7344%\n",
      "[Epoch:  2/  5] [data: 1408/24058] Training Loss: 0.1927 Training Acc: 92.8267%\n",
      "[Epoch:  2/  5] [data: 1536/24058] Training Loss: 0.0998 Training Acc: 93.1641%\n",
      "[Epoch:  2/  5] [data: 1664/24058] Training Loss: 0.2702 Training Acc: 93.0889%\n",
      "[Epoch:  2/  5] [data: 1792/24058] Training Loss: 0.2834 Training Acc: 92.9688%\n",
      "[Epoch:  2/  5] [data: 1920/24058] Training Loss: 0.1081 Training Acc: 93.2292%\n",
      "[Epoch:  2/  5] [data: 2048/24058] Training Loss: 0.2084 Training Acc: 93.3105%\n",
      "[Epoch:  2/  5] [data: 2176/24058] Training Loss: 0.0997 Training Acc: 93.5662%\n",
      "[Epoch:  2/  5] [data: 2304/24058] Training Loss: 0.2441 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 2432/24058] Training Loss: 0.0968 Training Acc: 93.6678%\n",
      "[Epoch:  2/  5] [data: 2560/24058] Training Loss: 0.2070 Training Acc: 93.5938%\n",
      "[Epoch:  2/  5] [data: 2688/24058] Training Loss: 0.0890 Training Acc: 93.7872%\n",
      "[Epoch:  2/  5] [data: 2816/24058] Training Loss: 0.1861 Training Acc: 93.7855%\n",
      "[Epoch:  2/  5] [data: 2944/24058] Training Loss: 0.0843 Training Acc: 93.9198%\n",
      "[Epoch:  2/  5] [data: 3072/24058] Training Loss: 0.2658 Training Acc: 93.8477%\n",
      "[Epoch:  2/  5] [data: 3200/24058] Training Loss: 0.2073 Training Acc: 93.8438%\n",
      "[Epoch:  2/  5] [data: 3328/24058] Training Loss: 0.1307 Training Acc: 93.9603%\n",
      "[Epoch:  2/  5] [data: 3456/24058] Training Loss: 0.1349 Training Acc: 94.0394%\n",
      "[Epoch:  2/  5] [data: 3584/24058] Training Loss: 0.1146 Training Acc: 94.1406%\n",
      "[Epoch:  2/  5] [data: 3712/24058] Training Loss: 0.3024 Training Acc: 93.9925%\n",
      "[Epoch:  2/  5] [data: 3840/24058] Training Loss: 0.0892 Training Acc: 94.1406%\n",
      "[Epoch:  2/  5] [data: 3968/24058] Training Loss: 0.1441 Training Acc: 94.2036%\n",
      "[Epoch:  2/  5] [data: 4096/24058] Training Loss: 0.2319 Training Acc: 94.1650%\n",
      "[Epoch:  2/  5] [data: 4224/24058] Training Loss: 0.1371 Training Acc: 94.2472%\n",
      "[Epoch:  2/  5] [data: 4352/24058] Training Loss: 0.0702 Training Acc: 94.3244%\n",
      "[Epoch:  2/  5] [data: 4480/24058] Training Loss: 0.2420 Training Acc: 94.2634%\n",
      "[Epoch:  2/  5] [data: 4608/24058] Training Loss: 0.0837 Training Acc: 94.3142%\n",
      "[Epoch:  2/  5] [data: 4736/24058] Training Loss: 0.2509 Training Acc: 94.2990%\n",
      "[Epoch:  2/  5] [data: 4864/24058] Training Loss: 0.1094 Training Acc: 94.3873%\n",
      "[Epoch:  2/  5] [data: 4992/24058] Training Loss: 0.1265 Training Acc: 94.4311%\n",
      "[Epoch:  2/  5] [data: 5120/24058] Training Loss: 0.1948 Training Acc: 94.3750%\n",
      "[Epoch:  2/  5] [data: 5248/24058] Training Loss: 0.1882 Training Acc: 94.3788%\n",
      "[Epoch:  2/  5] [data: 5376/24058] Training Loss: 0.2085 Training Acc: 94.3824%\n",
      "[Epoch:  2/  5] [data: 5504/24058] Training Loss: 0.1266 Training Acc: 94.4586%\n",
      "[Epoch:  2/  5] [data: 5632/24058] Training Loss: 0.1761 Training Acc: 94.4602%\n",
      "[Epoch:  2/  5] [data: 5760/24058] Training Loss: 0.1499 Training Acc: 94.4792%\n",
      "[Epoch:  2/  5] [data: 5888/24058] Training Loss: 0.1515 Training Acc: 94.4973%\n",
      "[Epoch:  2/  5] [data: 6016/24058] Training Loss: 0.1827 Training Acc: 94.4814%\n",
      "[Epoch:  2/  5] [data: 6144/24058] Training Loss: 0.1618 Training Acc: 94.4987%\n",
      "[Epoch:  2/  5] [data: 6272/24058] Training Loss: 0.2152 Training Acc: 94.4834%\n",
      "[Epoch:  2/  5] [data: 6400/24058] Training Loss: 0.2136 Training Acc: 94.4688%\n",
      "[Epoch:  2/  5] [data: 6528/24058] Training Loss: 0.1551 Training Acc: 94.5006%\n",
      "[Epoch:  2/  5] [data: 6656/24058] Training Loss: 0.1826 Training Acc: 94.5162%\n",
      "[Epoch:  2/  5] [data: 6784/24058] Training Loss: 0.1401 Training Acc: 94.5607%\n",
      "[Epoch:  2/  5] [data: 6912/24058] Training Loss: 0.2046 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 7040/24058] Training Loss: 0.1765 Training Acc: 94.5455%\n",
      "[Epoch:  2/  5] [data: 7168/24058] Training Loss: 0.1933 Training Acc: 94.5173%\n",
      "[Epoch:  2/  5] [data: 7296/24058] Training Loss: 0.0911 Training Acc: 94.5587%\n",
      "[Epoch:  2/  5] [data: 7424/24058] Training Loss: 0.1169 Training Acc: 94.5851%\n",
      "[Epoch:  2/  5] [data: 7552/24058] Training Loss: 0.0815 Training Acc: 94.6239%\n",
      "[Epoch:  2/  5] [data: 7680/24058] Training Loss: 0.2079 Training Acc: 94.5833%\n",
      "[Epoch:  2/  5] [data: 7808/24058] Training Loss: 0.2168 Training Acc: 94.5697%\n",
      "[Epoch:  2/  5] [data: 7936/24058] Training Loss: 0.3199 Training Acc: 94.4934%\n",
      "[Epoch:  2/  5] [data: 8064/24058] Training Loss: 0.2871 Training Acc: 94.4568%\n",
      "[Epoch:  2/  5] [data: 8192/24058] Training Loss: 0.0971 Training Acc: 94.5068%\n",
      "[Epoch:  2/  5] [data: 8320/24058] Training Loss: 0.1980 Training Acc: 94.5192%\n",
      "[Epoch:  2/  5] [data: 8448/24058] Training Loss: 0.2432 Training Acc: 94.4839%\n",
      "[Epoch:  2/  5] [data: 8576/24058] Training Loss: 0.1113 Training Acc: 94.5429%\n",
      "[Epoch:  2/  5] [data: 8704/24058] Training Loss: 0.2010 Training Acc: 94.5198%\n",
      "[Epoch:  2/  5] [data: 8832/24058] Training Loss: 0.2793 Training Acc: 94.4860%\n",
      "[Epoch:  2/  5] [data: 8960/24058] Training Loss: 0.2891 Training Acc: 94.4420%\n",
      "[Epoch:  2/  5] [data: 9088/24058] Training Loss: 0.1851 Training Acc: 94.4212%\n",
      "[Epoch:  2/  5] [data: 9216/24058] Training Loss: 0.1280 Training Acc: 94.4444%\n",
      "[Epoch:  2/  5] [data: 9344/24058] Training Loss: 0.1908 Training Acc: 94.4563%\n",
      "[Epoch:  2/  5] [data: 9472/24058] Training Loss: 0.2089 Training Acc: 94.4468%\n",
      "[Epoch:  2/  5] [data: 9600/24058] Training Loss: 0.2640 Training Acc: 94.4271%\n",
      "[Epoch:  2/  5] [data: 9728/24058] Training Loss: 0.1696 Training Acc: 94.4387%\n",
      "[Epoch:  2/  5] [data: 9856/24058] Training Loss: 0.2020 Training Acc: 94.4399%\n",
      "[Epoch:  2/  5] [data: 9984/24058] Training Loss: 0.1652 Training Acc: 94.4411%\n",
      "[Epoch:  2/  5] [data: 10112/24058] Training Loss: 0.2247 Training Acc: 94.4324%\n",
      "[Epoch:  2/  5] [data: 10240/24058] Training Loss: 0.1573 Training Acc: 94.4141%\n",
      "[Epoch:  2/  5] [data: 10368/24058] Training Loss: 0.1611 Training Acc: 94.4252%\n",
      "[Epoch:  2/  5] [data: 10496/24058] Training Loss: 0.1228 Training Acc: 94.4550%\n",
      "[Epoch:  2/  5] [data: 10624/24058] Training Loss: 0.0960 Training Acc: 94.4748%\n",
      "[Epoch:  2/  5] [data: 10752/24058] Training Loss: 0.1286 Training Acc: 94.4847%\n",
      "[Epoch:  2/  5] [data: 10880/24058] Training Loss: 0.1760 Training Acc: 94.4945%\n",
      "[Epoch:  2/  5] [data: 11008/24058] Training Loss: 0.2510 Training Acc: 94.4677%\n",
      "[Epoch:  2/  5] [data: 11136/24058] Training Loss: 0.2195 Training Acc: 94.4594%\n",
      "[Epoch:  2/  5] [data: 11264/24058] Training Loss: 0.2014 Training Acc: 94.4602%\n",
      "[Epoch:  2/  5] [data: 11392/24058] Training Loss: 0.0803 Training Acc: 94.5049%\n",
      "[Epoch:  2/  5] [data: 11520/24058] Training Loss: 0.1714 Training Acc: 94.4965%\n",
      "[Epoch:  2/  5] [data: 11648/24058] Training Loss: 0.1828 Training Acc: 94.4969%\n",
      "[Epoch:  2/  5] [data: 11776/24058] Training Loss: 0.2869 Training Acc: 94.4633%\n",
      "[Epoch:  2/  5] [data: 11904/24058] Training Loss: 0.1579 Training Acc: 94.4808%\n",
      "[Epoch:  2/  5] [data: 12032/24058] Training Loss: 0.1487 Training Acc: 94.4814%\n",
      "[Epoch:  2/  5] [data: 12160/24058] Training Loss: 0.1734 Training Acc: 94.4901%\n",
      "[Epoch:  2/  5] [data: 12288/24058] Training Loss: 0.1604 Training Acc: 94.5068%\n",
      "[Epoch:  2/  5] [data: 12416/24058] Training Loss: 0.2704 Training Acc: 94.4668%\n",
      "[Epoch:  2/  5] [data: 12544/24058] Training Loss: 0.1491 Training Acc: 94.4834%\n",
      "[Epoch:  2/  5] [data: 12672/24058] Training Loss: 0.2300 Training Acc: 94.4681%\n",
      "[Epoch:  2/  5] [data: 12800/24058] Training Loss: 0.2269 Training Acc: 94.4531%\n",
      "[Epoch:  2/  5] [data: 12928/24058] Training Loss: 0.1241 Training Acc: 94.4771%\n",
      "[Epoch:  2/  5] [data: 13056/24058] Training Loss: 0.2079 Training Acc: 94.4700%\n",
      "[Epoch:  2/  5] [data: 13184/24058] Training Loss: 0.1812 Training Acc: 94.4782%\n",
      "[Epoch:  2/  5] [data: 13312/24058] Training Loss: 0.1160 Training Acc: 94.5012%\n",
      "[Epoch:  2/  5] [data: 13440/24058] Training Loss: 0.0961 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 13568/24058] Training Loss: 0.1883 Training Acc: 94.5386%\n",
      "[Epoch:  2/  5] [data: 13696/24058] Training Loss: 0.1931 Training Acc: 94.5386%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 13824/24058] Training Loss: 0.1355 Training Acc: 94.5530%\n",
      "[Epoch:  2/  5] [data: 13952/24058] Training Loss: 0.1779 Training Acc: 94.5528%\n",
      "[Epoch:  2/  5] [data: 14080/24058] Training Loss: 0.1615 Training Acc: 94.5668%\n",
      "[Epoch:  2/  5] [data: 14208/24058] Training Loss: 0.2702 Training Acc: 94.5312%\n",
      "[Epoch:  2/  5] [data: 14336/24058] Training Loss: 0.1309 Training Acc: 94.5522%\n",
      "[Epoch:  2/  5] [data: 14464/24058] Training Loss: 0.0772 Training Acc: 94.5796%\n",
      "[Epoch:  2/  5] [data: 14592/24058] Training Loss: 0.1787 Training Acc: 94.5861%\n",
      "[Epoch:  2/  5] [data: 14720/24058] Training Loss: 0.0760 Training Acc: 94.6196%\n",
      "[Epoch:  2/  5] [data: 14848/24058] Training Loss: 0.1731 Training Acc: 94.6188%\n",
      "[Epoch:  2/  5] [data: 14976/24058] Training Loss: 0.1530 Training Acc: 94.6314%\n",
      "[Epoch:  2/  5] [data: 15104/24058] Training Loss: 0.1283 Training Acc: 94.6438%\n",
      "[Epoch:  2/  5] [data: 15232/24058] Training Loss: 0.1581 Training Acc: 94.6560%\n",
      "[Epoch:  2/  5] [data: 15360/24058] Training Loss: 0.1479 Training Acc: 94.6745%\n",
      "[Epoch:  2/  5] [data: 15488/24058] Training Loss: 0.1504 Training Acc: 94.6733%\n",
      "[Epoch:  2/  5] [data: 15616/24058] Training Loss: 0.1774 Training Acc: 94.6785%\n",
      "[Epoch:  2/  5] [data: 15744/24058] Training Loss: 0.2556 Training Acc: 94.6710%\n",
      "[Epoch:  2/  5] [data: 15872/24058] Training Loss: 0.1369 Training Acc: 94.6888%\n",
      "[Epoch:  2/  5] [data: 16000/24058] Training Loss: 0.1984 Training Acc: 94.6937%\n",
      "[Epoch:  2/  5] [data: 16128/24058] Training Loss: 0.1485 Training Acc: 94.6987%\n",
      "[Epoch:  2/  5] [data: 16256/24058] Training Loss: 0.2059 Training Acc: 94.6973%\n",
      "[Epoch:  2/  5] [data: 16384/24058] Training Loss: 0.1963 Training Acc: 94.6960%\n",
      "[Epoch:  2/  5] [data: 16512/24058] Training Loss: 0.2579 Training Acc: 94.6766%\n",
      "[Epoch:  2/  5] [data: 16640/24058] Training Loss: 0.1863 Training Acc: 94.6815%\n",
      "[Epoch:  2/  5] [data: 16768/24058] Training Loss: 0.1356 Training Acc: 94.6982%\n",
      "[Epoch:  2/  5] [data: 16896/24058] Training Loss: 0.1931 Training Acc: 94.7029%\n",
      "[Epoch:  2/  5] [data: 17024/24058] Training Loss: 0.2933 Training Acc: 94.6664%\n",
      "[Epoch:  2/  5] [data: 17152/24058] Training Loss: 0.1791 Training Acc: 94.6712%\n",
      "[Epoch:  2/  5] [data: 17280/24058] Training Loss: 0.2536 Training Acc: 94.6586%\n",
      "[Epoch:  2/  5] [data: 17408/24058] Training Loss: 0.2375 Training Acc: 94.6461%\n",
      "[Epoch:  2/  5] [data: 17536/24058] Training Loss: 0.1276 Training Acc: 94.6510%\n",
      "[Epoch:  2/  5] [data: 17664/24058] Training Loss: 0.1675 Training Acc: 94.6558%\n",
      "[Epoch:  2/  5] [data: 17792/24058] Training Loss: 0.1596 Training Acc: 94.6661%\n",
      "[Epoch:  2/  5] [data: 17920/24058] Training Loss: 0.2267 Training Acc: 94.6596%\n",
      "[Epoch:  2/  5] [data: 18048/24058] Training Loss: 0.1442 Training Acc: 94.6753%\n",
      "[Epoch:  2/  5] [data: 18176/24058] Training Loss: 0.2667 Training Acc: 94.6523%\n",
      "[Epoch:  2/  5] [data: 18304/24058] Training Loss: 0.1822 Training Acc: 94.6569%\n",
      "[Epoch:  2/  5] [data: 18432/24058] Training Loss: 0.1544 Training Acc: 94.6669%\n",
      "[Epoch:  2/  5] [data: 18560/24058] Training Loss: 0.1894 Training Acc: 94.6659%\n",
      "[Epoch:  2/  5] [data: 18688/24058] Training Loss: 0.1136 Training Acc: 94.6811%\n",
      "[Epoch:  2/  5] [data: 18816/24058] Training Loss: 0.1603 Training Acc: 94.6854%\n",
      "[Epoch:  2/  5] [data: 18944/24058] Training Loss: 0.1866 Training Acc: 94.6843%\n",
      "[Epoch:  2/  5] [data: 19072/24058] Training Loss: 0.1367 Training Acc: 94.6938%\n",
      "[Epoch:  2/  5] [data: 19200/24058] Training Loss: 0.0946 Training Acc: 94.7135%\n",
      "[Epoch:  2/  5] [data: 19328/24058] Training Loss: 0.2156 Training Acc: 94.7020%\n",
      "[Epoch:  2/  5] [data: 19456/24058] Training Loss: 0.1776 Training Acc: 94.7060%\n",
      "[Epoch:  2/  5] [data: 19584/24058] Training Loss: 0.1198 Training Acc: 94.7151%\n",
      "[Epoch:  2/  5] [data: 19712/24058] Training Loss: 0.1546 Training Acc: 94.7240%\n",
      "[Epoch:  2/  5] [data: 19840/24058] Training Loss: 0.1334 Training Acc: 94.7278%\n",
      "[Epoch:  2/  5] [data: 19968/24058] Training Loss: 0.2117 Training Acc: 94.7216%\n",
      "[Epoch:  2/  5] [data: 20096/24058] Training Loss: 0.3157 Training Acc: 94.6905%\n",
      "[Epoch:  2/  5] [data: 20224/24058] Training Loss: 0.2465 Training Acc: 94.6746%\n",
      "[Epoch:  2/  5] [data: 20352/24058] Training Loss: 0.1450 Training Acc: 94.6836%\n",
      "[Epoch:  2/  5] [data: 20480/24058] Training Loss: 0.1067 Training Acc: 94.7021%\n",
      "[Epoch:  2/  5] [data: 20608/24058] Training Loss: 0.2784 Training Acc: 94.6817%\n",
      "[Epoch:  2/  5] [data: 20736/24058] Training Loss: 0.1285 Training Acc: 94.6904%\n",
      "[Epoch:  2/  5] [data: 20864/24058] Training Loss: 0.1273 Training Acc: 94.6990%\n",
      "[Epoch:  2/  5] [data: 20992/24058] Training Loss: 0.1979 Training Acc: 94.6837%\n",
      "[Epoch:  2/  5] [data: 21120/24058] Training Loss: 0.1968 Training Acc: 94.6828%\n",
      "[Epoch:  2/  5] [data: 21248/24058] Training Loss: 0.1239 Training Acc: 94.6960%\n",
      "[Epoch:  2/  5] [data: 21376/24058] Training Loss: 0.1610 Training Acc: 94.6997%\n",
      "[Epoch:  2/  5] [data: 21504/24058] Training Loss: 0.0938 Training Acc: 94.7080%\n",
      "[Epoch:  2/  5] [data: 21632/24058] Training Loss: 0.2851 Training Acc: 94.6884%\n",
      "[Epoch:  2/  5] [data: 21760/24058] Training Loss: 0.1766 Training Acc: 94.6921%\n",
      "[Epoch:  2/  5] [data: 21888/24058] Training Loss: 0.2271 Training Acc: 94.6820%\n",
      "[Epoch:  2/  5] [data: 22016/24058] Training Loss: 0.1871 Training Acc: 94.6766%\n",
      "[Epoch:  2/  5] [data: 22144/24058] Training Loss: 0.2033 Training Acc: 94.6758%\n",
      "[Epoch:  2/  5] [data: 22272/24058] Training Loss: 0.1421 Training Acc: 94.6884%\n",
      "[Epoch:  2/  5] [data: 22400/24058] Training Loss: 0.1871 Training Acc: 94.6830%\n",
      "[Epoch:  2/  5] [data: 22528/24058] Training Loss: 0.1944 Training Acc: 94.6822%\n",
      "[Epoch:  2/  5] [data: 22656/24058] Training Loss: 0.2442 Training Acc: 94.6769%\n",
      "[Epoch:  2/  5] [data: 22784/24058] Training Loss: 0.1750 Training Acc: 94.6805%\n",
      "[Epoch:  2/  5] [data: 22912/24058] Training Loss: 0.1952 Training Acc: 94.6753%\n",
      "[Epoch:  2/  5] [data: 23040/24058] Training Loss: 0.2405 Training Acc: 94.6658%\n",
      "[Epoch:  2/  5] [data: 23168/24058] Training Loss: 0.1563 Training Acc: 94.6694%\n",
      "[Epoch:  2/  5] [data: 23296/24058] Training Loss: 0.2029 Training Acc: 94.6643%\n",
      "[Epoch:  2/  5] [data: 23424/24058] Training Loss: 0.1224 Training Acc: 94.6764%\n",
      "[Epoch:  2/  5] [data: 23552/24058] Training Loss: 0.1553 Training Acc: 94.6799%\n",
      "[Epoch:  2/  5] [data: 23680/24058] Training Loss: 0.2090 Training Acc: 94.6748%\n",
      "[Epoch:  2/  5] [data: 23808/24058] Training Loss: 0.0720 Training Acc: 94.6951%\n",
      "[Epoch:  2/  5] [data: 23936/24058] Training Loss: 0.1492 Training Acc: 94.7025%\n",
      "[Epoch:  2/  5] [data: 24058/24058] Training Loss: 0.1777 Training Acc: 94.7045%\n",
      "Testing-4...\n",
      "[Epoch:  2/  5] Validation Loss: 0.1832 Validation Acc: 94.5048%\n",
      "Time used: 1151.8869507312775s\n",
      "[Epoch:  3/  5] [data: 128/24058] Training Loss: 0.2119 Training Acc: 92.1875%\n",
      "[Epoch:  3/  5] [data: 256/24058] Training Loss: 0.2851 Training Acc: 90.6250%\n",
      "[Epoch:  3/  5] [data: 384/24058] Training Loss: 0.2969 Training Acc: 91.1458%\n",
      "[Epoch:  3/  5] [data: 512/24058] Training Loss: 0.1058 Training Acc: 92.5781%\n",
      "[Epoch:  3/  5] [data: 640/24058] Training Loss: 0.1923 Training Acc: 92.9688%\n",
      "[Epoch:  3/  5] [data: 768/24058] Training Loss: 0.2170 Training Acc: 93.0990%\n",
      "[Epoch:  3/  5] [data: 896/24058] Training Loss: 0.0610 Training Acc: 93.9732%\n",
      "[Epoch:  3/  5] [data: 1024/24058] Training Loss: 0.1591 Training Acc: 94.2383%\n",
      "[Epoch:  3/  5] [data: 1152/24058] Training Loss: 0.1794 Training Acc: 94.3576%\n",
      "[Epoch:  3/  5] [data: 1280/24058] Training Loss: 0.4454 Training Acc: 93.5156%\n",
      "[Epoch:  3/  5] [data: 1408/24058] Training Loss: 0.1624 Training Acc: 93.6080%\n",
      "[Epoch:  3/  5] [data: 1536/24058] Training Loss: 0.0990 Training Acc: 93.9453%\n",
      "[Epoch:  3/  5] [data: 1664/24058] Training Loss: 0.2558 Training Acc: 93.8702%\n",
      "[Epoch:  3/  5] [data: 1792/24058] Training Loss: 0.2670 Training Acc: 93.6942%\n",
      "[Epoch:  3/  5] [data: 1920/24058] Training Loss: 0.1048 Training Acc: 93.9583%\n",
      "[Epoch:  3/  5] [data: 2048/24058] Training Loss: 0.2026 Training Acc: 93.9941%\n",
      "[Epoch:  3/  5] [data: 2176/24058] Training Loss: 0.0906 Training Acc: 94.2096%\n",
      "[Epoch:  3/  5] [data: 2304/24058] Training Loss: 0.2416 Training Acc: 94.0972%\n",
      "[Epoch:  3/  5] [data: 2432/24058] Training Loss: 0.0725 Training Acc: 94.2434%\n",
      "[Epoch:  3/  5] [data: 2560/24058] Training Loss: 0.2004 Training Acc: 94.1797%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  3/  5] [data: 2688/24058] Training Loss: 0.0788 Training Acc: 94.3824%\n",
      "[Epoch:  3/  5] [data: 2816/24058] Training Loss: 0.1706 Training Acc: 94.3537%\n",
      "[Epoch:  3/  5] [data: 2944/24058] Training Loss: 0.0776 Training Acc: 94.4973%\n",
      "[Epoch:  3/  5] [data: 3072/24058] Training Loss: 0.2595 Training Acc: 94.4010%\n",
      "[Epoch:  3/  5] [data: 3200/24058] Training Loss: 0.2038 Training Acc: 94.3750%\n",
      "[Epoch:  3/  5] [data: 3328/24058] Training Loss: 0.1209 Training Acc: 94.4712%\n",
      "[Epoch:  3/  5] [data: 3456/24058] Training Loss: 0.1252 Training Acc: 94.5602%\n",
      "[Epoch:  3/  5] [data: 3584/24058] Training Loss: 0.1066 Training Acc: 94.6429%\n",
      "[Epoch:  3/  5] [data: 3712/24058] Training Loss: 0.2971 Training Acc: 94.5043%\n",
      "[Epoch:  3/  5] [data: 3840/24058] Training Loss: 0.0775 Training Acc: 94.6094%\n",
      "[Epoch:  3/  5] [data: 3968/24058] Training Loss: 0.1269 Training Acc: 94.6825%\n",
      "[Epoch:  3/  5] [data: 4096/24058] Training Loss: 0.2168 Training Acc: 94.6533%\n",
      "[Epoch:  3/  5] [data: 4224/24058] Training Loss: 0.1266 Training Acc: 94.7206%\n",
      "[Epoch:  3/  5] [data: 4352/24058] Training Loss: 0.0654 Training Acc: 94.8070%\n",
      "[Epoch:  3/  5] [data: 4480/24058] Training Loss: 0.2200 Training Acc: 94.7321%\n",
      "[Epoch:  3/  5] [data: 4608/24058] Training Loss: 0.0723 Training Acc: 94.8134%\n",
      "[Epoch:  3/  5] [data: 4736/24058] Training Loss: 0.2327 Training Acc: 94.7635%\n",
      "[Epoch:  3/  5] [data: 4864/24058] Training Loss: 0.0997 Training Acc: 94.8396%\n",
      "[Epoch:  3/  5] [data: 4992/24058] Training Loss: 0.1112 Training Acc: 94.8718%\n",
      "[Epoch:  3/  5] [data: 5120/24058] Training Loss: 0.1869 Training Acc: 94.8438%\n",
      "[Epoch:  3/  5] [data: 5248/24058] Training Loss: 0.1769 Training Acc: 94.8552%\n",
      "[Epoch:  3/  5] [data: 5376/24058] Training Loss: 0.1986 Training Acc: 94.8475%\n",
      "[Epoch:  3/  5] [data: 5504/24058] Training Loss: 0.1146 Training Acc: 94.9128%\n",
      "[Epoch:  3/  5] [data: 5632/24058] Training Loss: 0.1712 Training Acc: 94.9041%\n",
      "[Epoch:  3/  5] [data: 5760/24058] Training Loss: 0.1498 Training Acc: 94.9306%\n",
      "[Epoch:  3/  5] [data: 5888/24058] Training Loss: 0.1488 Training Acc: 94.9558%\n",
      "[Epoch:  3/  5] [data: 6016/24058] Training Loss: 0.1686 Training Acc: 94.9468%\n",
      "[Epoch:  3/  5] [data: 6144/24058] Training Loss: 0.1459 Training Acc: 94.9544%\n",
      "[Epoch:  3/  5] [data: 6272/24058] Training Loss: 0.2089 Training Acc: 94.9298%\n",
      "[Epoch:  3/  5] [data: 6400/24058] Training Loss: 0.2118 Training Acc: 94.9062%\n",
      "[Epoch:  3/  5] [data: 6528/24058] Training Loss: 0.1515 Training Acc: 94.9295%\n",
      "[Epoch:  3/  5] [data: 6656/24058] Training Loss: 0.1747 Training Acc: 94.9369%\n",
      "[Epoch:  3/  5] [data: 6784/24058] Training Loss: 0.1342 Training Acc: 94.9735%\n",
      "[Epoch:  3/  5] [data: 6912/24058] Training Loss: 0.1993 Training Acc: 94.9653%\n",
      "[Epoch:  3/  5] [data: 7040/24058] Training Loss: 0.1700 Training Acc: 94.9716%\n",
      "[Epoch:  3/  5] [data: 7168/24058] Training Loss: 0.1829 Training Acc: 94.9777%\n",
      "[Epoch:  3/  5] [data: 7296/24058] Training Loss: 0.0748 Training Acc: 95.0247%\n",
      "[Epoch:  3/  5] [data: 7424/24058] Training Loss: 0.1061 Training Acc: 95.0566%\n",
      "[Epoch:  3/  5] [data: 7552/24058] Training Loss: 0.0759 Training Acc: 95.1006%\n",
      "[Epoch:  3/  5] [data: 7680/24058] Training Loss: 0.1908 Training Acc: 95.0911%\n",
      "[Epoch:  3/  5] [data: 7808/24058] Training Loss: 0.2015 Training Acc: 95.0820%\n",
      "[Epoch:  3/  5] [data: 7936/24058] Training Loss: 0.3122 Training Acc: 95.0353%\n",
      "[Epoch:  3/  5] [data: 8064/24058] Training Loss: 0.2746 Training Acc: 94.9901%\n",
      "[Epoch:  3/  5] [data: 8192/24058] Training Loss: 0.0814 Training Acc: 95.0317%\n",
      "[Epoch:  3/  5] [data: 8320/24058] Training Loss: 0.1829 Training Acc: 95.0361%\n",
      "[Epoch:  3/  5] [data: 8448/24058] Training Loss: 0.2381 Training Acc: 94.9929%\n",
      "[Epoch:  3/  5] [data: 8576/24058] Training Loss: 0.1039 Training Acc: 95.0443%\n",
      "[Epoch:  3/  5] [data: 8704/24058] Training Loss: 0.1946 Training Acc: 95.0253%\n",
      "[Epoch:  3/  5] [data: 8832/24058] Training Loss: 0.2755 Training Acc: 94.9841%\n",
      "[Epoch:  3/  5] [data: 8960/24058] Training Loss: 0.2734 Training Acc: 94.9442%\n",
      "[Epoch:  3/  5] [data: 9088/24058] Training Loss: 0.1805 Training Acc: 94.9384%\n",
      "[Epoch:  3/  5] [data: 9216/24058] Training Loss: 0.1258 Training Acc: 94.9544%\n",
      "[Epoch:  3/  5] [data: 9344/24058] Training Loss: 0.1803 Training Acc: 94.9593%\n",
      "[Epoch:  3/  5] [data: 9472/24058] Training Loss: 0.2178 Training Acc: 94.9219%\n",
      "[Epoch:  3/  5] [data: 9600/24058] Training Loss: 0.2693 Training Acc: 94.8854%\n",
      "[Epoch:  3/  5] [data: 9728/24058] Training Loss: 0.1716 Training Acc: 94.8910%\n",
      "[Epoch:  3/  5] [data: 9856/24058] Training Loss: 0.1905 Training Acc: 94.8864%\n",
      "[Epoch:  3/  5] [data: 9984/24058] Training Loss: 0.1617 Training Acc: 94.8818%\n",
      "[Epoch:  3/  5] [data: 10112/24058] Training Loss: 0.2270 Training Acc: 94.8675%\n",
      "[Epoch:  3/  5] [data: 10240/24058] Training Loss: 0.1449 Training Acc: 94.8535%\n",
      "[Epoch:  3/  5] [data: 10368/24058] Training Loss: 0.1542 Training Acc: 94.8592%\n",
      "[Epoch:  3/  5] [data: 10496/24058] Training Loss: 0.1247 Training Acc: 94.8838%\n",
      "[Epoch:  3/  5] [data: 10624/24058] Training Loss: 0.0906 Training Acc: 94.9078%\n",
      "[Epoch:  3/  5] [data: 10752/24058] Training Loss: 0.1265 Training Acc: 94.9126%\n",
      "[Epoch:  3/  5] [data: 10880/24058] Training Loss: 0.1674 Training Acc: 94.9173%\n",
      "[Epoch:  3/  5] [data: 11008/24058] Training Loss: 0.2435 Training Acc: 94.8855%\n",
      "[Epoch:  3/  5] [data: 11136/24058] Training Loss: 0.2141 Training Acc: 94.8725%\n",
      "[Epoch:  3/  5] [data: 11264/24058] Training Loss: 0.1908 Training Acc: 94.8686%\n",
      "[Epoch:  3/  5] [data: 11392/24058] Training Loss: 0.0718 Training Acc: 94.9087%\n",
      "[Epoch:  3/  5] [data: 11520/24058] Training Loss: 0.1619 Training Acc: 94.9045%\n",
      "[Epoch:  3/  5] [data: 11648/24058] Training Loss: 0.1800 Training Acc: 94.9004%\n",
      "[Epoch:  3/  5] [data: 11776/24058] Training Loss: 0.2775 Training Acc: 94.8624%\n",
      "[Epoch:  3/  5] [data: 11904/24058] Training Loss: 0.1546 Training Acc: 94.8757%\n",
      "[Epoch:  3/  5] [data: 12032/24058] Training Loss: 0.1346 Training Acc: 94.8803%\n",
      "[Epoch:  3/  5] [data: 12160/24058] Training Loss: 0.1635 Training Acc: 94.8849%\n",
      "[Epoch:  3/  5] [data: 12288/24058] Training Loss: 0.1605 Training Acc: 94.8975%\n",
      "[Epoch:  3/  5] [data: 12416/24058] Training Loss: 0.2617 Training Acc: 94.8615%\n",
      "[Epoch:  3/  5] [data: 12544/24058] Training Loss: 0.1478 Training Acc: 94.8740%\n",
      "[Epoch:  3/  5] [data: 12672/24058] Training Loss: 0.2260 Training Acc: 94.8548%\n",
      "[Epoch:  3/  5] [data: 12800/24058] Training Loss: 0.2256 Training Acc: 94.8438%\n",
      "[Epoch:  3/  5] [data: 12928/24058] Training Loss: 0.1237 Training Acc: 94.8639%\n",
      "[Epoch:  3/  5] [data: 13056/24058] Training Loss: 0.2065 Training Acc: 94.8529%\n",
      "[Epoch:  3/  5] [data: 13184/24058] Training Loss: 0.1801 Training Acc: 94.8574%\n",
      "[Epoch:  3/  5] [data: 13312/24058] Training Loss: 0.1040 Training Acc: 94.8768%\n",
      "[Epoch:  3/  5] [data: 13440/24058] Training Loss: 0.0796 Training Acc: 94.9107%\n",
      "[Epoch:  3/  5] [data: 13568/24058] Training Loss: 0.1818 Training Acc: 94.9145%\n",
      "[Epoch:  3/  5] [data: 13696/24058] Training Loss: 0.1847 Training Acc: 94.9109%\n",
      "[Epoch:  3/  5] [data: 13824/24058] Training Loss: 0.1296 Training Acc: 94.9146%\n",
      "[Epoch:  3/  5] [data: 13952/24058] Training Loss: 0.1684 Training Acc: 94.9183%\n",
      "[Epoch:  3/  5] [data: 14080/24058] Training Loss: 0.1553 Training Acc: 94.9290%\n",
      "[Epoch:  3/  5] [data: 14208/24058] Training Loss: 0.2655 Training Acc: 94.9113%\n",
      "[Epoch:  3/  5] [data: 14336/24058] Training Loss: 0.1270 Training Acc: 94.9289%\n",
      "[Epoch:  3/  5] [data: 14464/24058] Training Loss: 0.0720 Training Acc: 94.9599%\n",
      "[Epoch:  3/  5] [data: 14592/24058] Training Loss: 0.1728 Training Acc: 94.9630%\n",
      "[Epoch:  3/  5] [data: 14720/24058] Training Loss: 0.0715 Training Acc: 94.9932%\n",
      "[Epoch:  3/  5] [data: 14848/24058] Training Loss: 0.1735 Training Acc: 94.9892%\n",
      "[Epoch:  3/  5] [data: 14976/24058] Training Loss: 0.1502 Training Acc: 94.9920%\n",
      "[Epoch:  3/  5] [data: 15104/24058] Training Loss: 0.1285 Training Acc: 95.0013%\n",
      "[Epoch:  3/  5] [data: 15232/24058] Training Loss: 0.1488 Training Acc: 95.0105%\n",
      "[Epoch:  3/  5] [data: 15360/24058] Training Loss: 0.1322 Training Acc: 95.0260%\n",
      "[Epoch:  3/  5] [data: 15488/24058] Training Loss: 0.1477 Training Acc: 95.0284%\n",
      "[Epoch:  3/  5] [data: 15616/24058] Training Loss: 0.1750 Training Acc: 95.0307%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  3/  5] [data: 15744/24058] Training Loss: 0.2417 Training Acc: 95.0203%\n",
      "[Epoch:  3/  5] [data: 15872/24058] Training Loss: 0.1313 Training Acc: 95.0353%\n",
      "[Epoch:  3/  5] [data: 16000/24058] Training Loss: 0.1893 Training Acc: 95.0375%\n",
      "[Epoch:  3/  5] [data: 16128/24058] Training Loss: 0.1408 Training Acc: 95.0459%\n",
      "[Epoch:  3/  5] [data: 16256/24058] Training Loss: 0.2029 Training Acc: 95.0418%\n",
      "[Epoch:  3/  5] [data: 16384/24058] Training Loss: 0.1933 Training Acc: 95.0439%\n",
      "[Epoch:  3/  5] [data: 16512/24058] Training Loss: 0.2543 Training Acc: 95.0218%\n",
      "[Epoch:  3/  5] [data: 16640/24058] Training Loss: 0.1802 Training Acc: 95.0180%\n",
      "[Epoch:  3/  5] [data: 16768/24058] Training Loss: 0.1325 Training Acc: 95.0322%\n",
      "[Epoch:  3/  5] [data: 16896/24058] Training Loss: 0.1857 Training Acc: 95.0343%\n",
      "[Epoch:  3/  5] [data: 17024/24058] Training Loss: 0.2842 Training Acc: 95.0012%\n",
      "[Epoch:  3/  5] [data: 17152/24058] Training Loss: 0.1778 Training Acc: 94.9977%\n",
      "[Epoch:  3/  5] [data: 17280/24058] Training Loss: 0.2524 Training Acc: 94.9826%\n",
      "[Epoch:  3/  5] [data: 17408/24058] Training Loss: 0.2291 Training Acc: 94.9736%\n",
      "[Epoch:  3/  5] [data: 17536/24058] Training Loss: 0.1159 Training Acc: 94.9875%\n",
      "[Epoch:  3/  5] [data: 17664/24058] Training Loss: 0.1560 Training Acc: 94.9955%\n",
      "[Epoch:  3/  5] [data: 17792/24058] Training Loss: 0.1478 Training Acc: 95.0034%\n",
      "[Epoch:  3/  5] [data: 17920/24058] Training Loss: 0.2192 Training Acc: 94.9944%\n",
      "[Epoch:  3/  5] [data: 18048/24058] Training Loss: 0.1275 Training Acc: 95.0022%\n",
      "[Epoch:  3/  5] [data: 18176/24058] Training Loss: 0.2799 Training Acc: 94.9769%\n",
      "[Epoch:  3/  5] [data: 18304/24058] Training Loss: 0.1773 Training Acc: 94.9792%\n",
      "[Epoch:  3/  5] [data: 18432/24058] Training Loss: 0.1374 Training Acc: 94.9870%\n",
      "[Epoch:  3/  5] [data: 18560/24058] Training Loss: 0.1816 Training Acc: 94.9838%\n",
      "[Epoch:  3/  5] [data: 18688/24058] Training Loss: 0.1020 Training Acc: 94.9968%\n",
      "[Epoch:  3/  5] [data: 18816/24058] Training Loss: 0.1474 Training Acc: 94.9989%\n",
      "[Epoch:  3/  5] [data: 18944/24058] Training Loss: 0.1800 Training Acc: 95.0011%\n",
      "[Epoch:  3/  5] [data: 19072/24058] Training Loss: 0.1357 Training Acc: 95.0084%\n",
      "[Epoch:  3/  5] [data: 19200/24058] Training Loss: 0.0910 Training Acc: 95.0260%\n",
      "[Epoch:  3/  5] [data: 19328/24058] Training Loss: 0.2113 Training Acc: 95.0124%\n",
      "[Epoch:  3/  5] [data: 19456/24058] Training Loss: 0.1725 Training Acc: 95.0144%\n",
      "[Epoch:  3/  5] [data: 19584/24058] Training Loss: 0.1117 Training Acc: 95.0266%\n",
      "[Epoch:  3/  5] [data: 19712/24058] Training Loss: 0.1514 Training Acc: 95.0335%\n",
      "[Epoch:  3/  5] [data: 19840/24058] Training Loss: 0.1306 Training Acc: 95.0403%\n",
      "[Epoch:  3/  5] [data: 19968/24058] Training Loss: 0.2089 Training Acc: 95.0321%\n",
      "[Epoch:  3/  5] [data: 20096/24058] Training Loss: 0.3211 Training Acc: 95.0040%\n",
      "[Epoch:  3/  5] [data: 20224/24058] Training Loss: 0.2336 Training Acc: 94.9862%\n",
      "[Epoch:  3/  5] [data: 20352/24058] Training Loss: 0.1347 Training Acc: 94.9931%\n",
      "[Epoch:  3/  5] [data: 20480/24058] Training Loss: 0.0900 Training Acc: 95.0146%\n",
      "[Epoch:  3/  5] [data: 20608/24058] Training Loss: 0.2749 Training Acc: 94.9874%\n",
      "[Epoch:  3/  5] [data: 20736/24058] Training Loss: 0.1266 Training Acc: 94.9942%\n",
      "[Epoch:  3/  5] [data: 20864/24058] Training Loss: 0.1176 Training Acc: 95.0010%\n",
      "[Epoch:  3/  5] [data: 20992/24058] Training Loss: 0.1924 Training Acc: 94.9886%\n",
      "[Epoch:  3/  5] [data: 21120/24058] Training Loss: 0.1956 Training Acc: 94.9763%\n",
      "[Epoch:  3/  5] [data: 21248/24058] Training Loss: 0.1239 Training Acc: 94.9878%\n",
      "[Epoch:  3/  5] [data: 21376/24058] Training Loss: 0.1519 Training Acc: 94.9897%\n",
      "[Epoch:  3/  5] [data: 21504/24058] Training Loss: 0.0798 Training Acc: 95.0056%\n",
      "[Epoch:  3/  5] [data: 21632/24058] Training Loss: 0.2724 Training Acc: 94.9889%\n",
      "[Epoch:  3/  5] [data: 21760/24058] Training Loss: 0.1737 Training Acc: 94.9908%\n",
      "[Epoch:  3/  5] [data: 21888/24058] Training Loss: 0.2257 Training Acc: 94.9790%\n",
      "[Epoch:  3/  5] [data: 22016/24058] Training Loss: 0.1855 Training Acc: 94.9764%\n",
      "[Epoch:  3/  5] [data: 22144/24058] Training Loss: 0.1974 Training Acc: 94.9738%\n",
      "[Epoch:  3/  5] [data: 22272/24058] Training Loss: 0.1449 Training Acc: 94.9847%\n",
      "[Epoch:  3/  5] [data: 22400/24058] Training Loss: 0.2002 Training Acc: 94.9821%\n",
      "[Epoch:  3/  5] [data: 22528/24058] Training Loss: 0.1878 Training Acc: 94.9796%\n",
      "[Epoch:  3/  5] [data: 22656/24058] Training Loss: 0.2353 Training Acc: 94.9726%\n",
      "[Epoch:  3/  5] [data: 22784/24058] Training Loss: 0.1678 Training Acc: 94.9745%\n",
      "[Epoch:  3/  5] [data: 22912/24058] Training Loss: 0.2000 Training Acc: 94.9721%\n",
      "[Epoch:  3/  5] [data: 23040/24058] Training Loss: 0.2348 Training Acc: 94.9609%\n",
      "[Epoch:  3/  5] [data: 23168/24058] Training Loss: 0.1529 Training Acc: 94.9629%\n",
      "[Epoch:  3/  5] [data: 23296/24058] Training Loss: 0.2072 Training Acc: 94.9605%\n",
      "[Epoch:  3/  5] [data: 23424/24058] Training Loss: 0.1102 Training Acc: 94.9710%\n",
      "[Epoch:  3/  5] [data: 23552/24058] Training Loss: 0.1512 Training Acc: 94.9728%\n",
      "[Epoch:  3/  5] [data: 23680/24058] Training Loss: 0.2034 Training Acc: 94.9662%\n",
      "[Epoch:  3/  5] [data: 23808/24058] Training Loss: 0.0630 Training Acc: 94.9891%\n",
      "[Epoch:  3/  5] [data: 23936/24058] Training Loss: 0.1463 Training Acc: 94.9950%\n",
      "[Epoch:  3/  5] [data: 24058/24058] Training Loss: 0.1695 Training Acc: 94.9996%\n",
      "Testing-4...\n",
      "[Epoch:  3/  5] Validation Loss: 0.1795 Validation Acc: 94.7575%\n",
      "Time used: 1174.888810634613s\n",
      "[Epoch:  4/  5] [data: 128/24058] Training Loss: 0.2075 Training Acc: 92.9688%\n",
      "[Epoch:  4/  5] [data: 256/24058] Training Loss: 0.2728 Training Acc: 91.4062%\n",
      "[Epoch:  4/  5] [data: 384/24058] Training Loss: 0.2898 Training Acc: 91.6667%\n",
      "[Epoch:  4/  5] [data: 512/24058] Training Loss: 0.0964 Training Acc: 93.1641%\n",
      "[Epoch:  4/  5] [data: 640/24058] Training Loss: 0.1872 Training Acc: 93.4375%\n",
      "[Epoch:  4/  5] [data: 768/24058] Training Loss: 0.2164 Training Acc: 93.4896%\n",
      "[Epoch:  4/  5] [data: 896/24058] Training Loss: 0.0580 Training Acc: 94.3080%\n",
      "[Epoch:  4/  5] [data: 1024/24058] Training Loss: 0.1542 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 1152/24058] Training Loss: 0.1763 Training Acc: 94.6181%\n",
      "[Epoch:  4/  5] [data: 1280/24058] Training Loss: 0.4469 Training Acc: 93.8281%\n",
      "[Epoch:  4/  5] [data: 1408/24058] Training Loss: 0.1568 Training Acc: 93.8920%\n",
      "[Epoch:  4/  5] [data: 1536/24058] Training Loss: 0.0937 Training Acc: 94.2057%\n",
      "[Epoch:  4/  5] [data: 1664/24058] Training Loss: 0.2480 Training Acc: 94.0505%\n",
      "[Epoch:  4/  5] [data: 1792/24058] Training Loss: 0.2615 Training Acc: 93.8616%\n",
      "[Epoch:  4/  5] [data: 1920/24058] Training Loss: 0.1061 Training Acc: 94.0625%\n",
      "[Epoch:  4/  5] [data: 2048/24058] Training Loss: 0.2001 Training Acc: 94.0918%\n",
      "[Epoch:  4/  5] [data: 2176/24058] Training Loss: 0.0861 Training Acc: 94.3015%\n",
      "[Epoch:  4/  5] [data: 2304/24058] Training Loss: 0.2362 Training Acc: 94.2274%\n",
      "[Epoch:  4/  5] [data: 2432/24058] Training Loss: 0.0709 Training Acc: 94.3668%\n",
      "[Epoch:  4/  5] [data: 2560/24058] Training Loss: 0.1933 Training Acc: 94.3359%\n",
      "[Epoch:  4/  5] [data: 2688/24058] Training Loss: 0.0733 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 2816/24058] Training Loss: 0.1610 Training Acc: 94.5312%\n",
      "[Epoch:  4/  5] [data: 2944/24058] Training Loss: 0.0721 Training Acc: 94.7011%\n",
      "[Epoch:  4/  5] [data: 3072/24058] Training Loss: 0.2575 Training Acc: 94.5964%\n",
      "[Epoch:  4/  5] [data: 3200/24058] Training Loss: 0.2011 Training Acc: 94.5938%\n",
      "[Epoch:  4/  5] [data: 3328/24058] Training Loss: 0.1164 Training Acc: 94.6815%\n",
      "[Epoch:  4/  5] [data: 3456/24058] Training Loss: 0.1235 Training Acc: 94.7627%\n",
      "[Epoch:  4/  5] [data: 3584/24058] Training Loss: 0.1083 Training Acc: 94.8382%\n",
      "[Epoch:  4/  5] [data: 3712/24058] Training Loss: 0.2863 Training Acc: 94.6929%\n",
      "[Epoch:  4/  5] [data: 3840/24058] Training Loss: 0.0798 Training Acc: 94.7917%\n",
      "[Epoch:  4/  5] [data: 3968/24058] Training Loss: 0.1241 Training Acc: 94.8589%\n",
      "[Epoch:  4/  5] [data: 4096/24058] Training Loss: 0.2153 Training Acc: 94.8242%\n",
      "[Epoch:  4/  5] [data: 4224/24058] Training Loss: 0.1240 Training Acc: 94.8864%\n",
      "[Epoch:  4/  5] [data: 4352/24058] Training Loss: 0.0617 Training Acc: 94.9678%\n",
      "[Epoch:  4/  5] [data: 4480/24058] Training Loss: 0.2080 Training Acc: 94.8884%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 4608/24058] Training Loss: 0.0657 Training Acc: 94.9653%\n",
      "[Epoch:  4/  5] [data: 4736/24058] Training Loss: 0.2302 Training Acc: 94.9324%\n",
      "[Epoch:  4/  5] [data: 4864/24058] Training Loss: 0.0980 Training Acc: 95.0041%\n",
      "[Epoch:  4/  5] [data: 4992/24058] Training Loss: 0.1058 Training Acc: 95.0321%\n",
      "[Epoch:  4/  5] [data: 5120/24058] Training Loss: 0.1800 Training Acc: 95.0000%\n",
      "[Epoch:  4/  5] [data: 5248/24058] Training Loss: 0.1736 Training Acc: 95.0076%\n",
      "[Epoch:  4/  5] [data: 5376/24058] Training Loss: 0.2000 Training Acc: 94.9963%\n",
      "[Epoch:  4/  5] [data: 5504/24058] Training Loss: 0.1123 Training Acc: 95.0581%\n",
      "[Epoch:  4/  5] [data: 5632/24058] Training Loss: 0.1724 Training Acc: 95.0639%\n",
      "[Epoch:  4/  5] [data: 5760/24058] Training Loss: 0.1544 Training Acc: 95.0694%\n",
      "[Epoch:  4/  5] [data: 5888/24058] Training Loss: 0.1507 Training Acc: 95.0917%\n",
      "[Epoch:  4/  5] [data: 6016/24058] Training Loss: 0.1678 Training Acc: 95.0798%\n",
      "[Epoch:  4/  5] [data: 6144/24058] Training Loss: 0.1387 Training Acc: 95.1009%\n",
      "[Epoch:  4/  5] [data: 6272/24058] Training Loss: 0.2085 Training Acc: 95.0733%\n",
      "[Epoch:  4/  5] [data: 6400/24058] Training Loss: 0.2108 Training Acc: 95.0469%\n",
      "[Epoch:  4/  5] [data: 6528/24058] Training Loss: 0.1494 Training Acc: 95.0674%\n",
      "[Epoch:  4/  5] [data: 6656/24058] Training Loss: 0.1751 Training Acc: 95.0721%\n",
      "[Epoch:  4/  5] [data: 6784/24058] Training Loss: 0.1268 Training Acc: 95.1061%\n",
      "[Epoch:  4/  5] [data: 6912/24058] Training Loss: 0.1936 Training Acc: 95.0955%\n",
      "[Epoch:  4/  5] [data: 7040/24058] Training Loss: 0.1658 Training Acc: 95.0994%\n",
      "[Epoch:  4/  5] [data: 7168/24058] Training Loss: 0.1812 Training Acc: 95.1032%\n",
      "[Epoch:  4/  5] [data: 7296/24058] Training Loss: 0.0774 Training Acc: 95.1617%\n",
      "[Epoch:  4/  5] [data: 7424/24058] Training Loss: 0.1049 Training Acc: 95.1913%\n",
      "[Epoch:  4/  5] [data: 7552/24058] Training Loss: 0.0727 Training Acc: 95.2331%\n",
      "[Epoch:  4/  5] [data: 7680/24058] Training Loss: 0.1860 Training Acc: 95.2214%\n",
      "[Epoch:  4/  5] [data: 7808/24058] Training Loss: 0.1970 Training Acc: 95.2100%\n",
      "[Epoch:  4/  5] [data: 7936/24058] Training Loss: 0.3061 Training Acc: 95.1613%\n",
      "[Epoch:  4/  5] [data: 8064/24058] Training Loss: 0.2725 Training Acc: 95.1141%\n",
      "[Epoch:  4/  5] [data: 8192/24058] Training Loss: 0.0770 Training Acc: 95.1538%\n",
      "[Epoch:  4/  5] [data: 8320/24058] Training Loss: 0.1793 Training Acc: 95.1562%\n",
      "[Epoch:  4/  5] [data: 8448/24058] Training Loss: 0.2371 Training Acc: 95.1113%\n",
      "[Epoch:  4/  5] [data: 8576/24058] Training Loss: 0.0982 Training Acc: 95.1609%\n",
      "[Epoch:  4/  5] [data: 8704/24058] Training Loss: 0.1882 Training Acc: 95.1402%\n",
      "[Epoch:  4/  5] [data: 8832/24058] Training Loss: 0.2708 Training Acc: 95.0974%\n",
      "[Epoch:  4/  5] [data: 8960/24058] Training Loss: 0.2714 Training Acc: 95.0558%\n",
      "[Epoch:  4/  5] [data: 9088/24058] Training Loss: 0.1691 Training Acc: 95.0484%\n",
      "[Epoch:  4/  5] [data: 9216/24058] Training Loss: 0.1277 Training Acc: 95.0629%\n",
      "[Epoch:  4/  5] [data: 9344/24058] Training Loss: 0.1837 Training Acc: 95.0664%\n",
      "[Epoch:  4/  5] [data: 9472/24058] Training Loss: 0.2141 Training Acc: 95.0486%\n",
      "[Epoch:  4/  5] [data: 9600/24058] Training Loss: 0.2693 Training Acc: 95.0104%\n",
      "[Epoch:  4/  5] [data: 9728/24058] Training Loss: 0.1765 Training Acc: 95.0144%\n",
      "[Epoch:  4/  5] [data: 9856/24058] Training Loss: 0.1888 Training Acc: 95.0081%\n",
      "[Epoch:  4/  5] [data: 9984/24058] Training Loss: 0.1587 Training Acc: 95.0020%\n",
      "[Epoch:  4/  5] [data: 10112/24058] Training Loss: 0.2259 Training Acc: 94.9862%\n",
      "[Epoch:  4/  5] [data: 10240/24058] Training Loss: 0.1463 Training Acc: 94.9609%\n",
      "[Epoch:  4/  5] [data: 10368/24058] Training Loss: 0.1521 Training Acc: 94.9653%\n",
      "[Epoch:  4/  5] [data: 10496/24058] Training Loss: 0.1241 Training Acc: 94.9886%\n",
      "[Epoch:  4/  5] [data: 10624/24058] Training Loss: 0.0875 Training Acc: 95.0113%\n",
      "[Epoch:  4/  5] [data: 10752/24058] Training Loss: 0.1209 Training Acc: 95.0335%\n",
      "[Epoch:  4/  5] [data: 10880/24058] Training Loss: 0.1695 Training Acc: 95.0368%\n",
      "[Epoch:  4/  5] [data: 11008/24058] Training Loss: 0.2448 Training Acc: 95.0036%\n",
      "[Epoch:  4/  5] [data: 11136/24058] Training Loss: 0.2063 Training Acc: 94.9892%\n",
      "[Epoch:  4/  5] [data: 11264/24058] Training Loss: 0.1941 Training Acc: 94.9751%\n",
      "[Epoch:  4/  5] [data: 11392/24058] Training Loss: 0.0710 Training Acc: 95.0140%\n",
      "[Epoch:  4/  5] [data: 11520/24058] Training Loss: 0.1557 Training Acc: 95.0087%\n",
      "[Epoch:  4/  5] [data: 11648/24058] Training Loss: 0.1821 Training Acc: 95.0034%\n",
      "[Epoch:  4/  5] [data: 11776/24058] Training Loss: 0.2770 Training Acc: 94.9643%\n",
      "[Epoch:  4/  5] [data: 11904/24058] Training Loss: 0.1555 Training Acc: 94.9765%\n",
      "[Epoch:  4/  5] [data: 12032/24058] Training Loss: 0.1364 Training Acc: 94.9801%\n",
      "[Epoch:  4/  5] [data: 12160/24058] Training Loss: 0.1586 Training Acc: 94.9836%\n",
      "[Epoch:  4/  5] [data: 12288/24058] Training Loss: 0.1591 Training Acc: 94.9951%\n",
      "[Epoch:  4/  5] [data: 12416/24058] Training Loss: 0.2605 Training Acc: 94.9581%\n",
      "[Epoch:  4/  5] [data: 12544/24058] Training Loss: 0.1449 Training Acc: 94.9697%\n",
      "[Epoch:  4/  5] [data: 12672/24058] Training Loss: 0.2152 Training Acc: 94.9495%\n",
      "[Epoch:  4/  5] [data: 12800/24058] Training Loss: 0.2236 Training Acc: 94.9375%\n",
      "[Epoch:  4/  5] [data: 12928/24058] Training Loss: 0.1231 Training Acc: 94.9567%\n",
      "[Epoch:  4/  5] [data: 13056/24058] Training Loss: 0.2000 Training Acc: 94.9525%\n",
      "[Epoch:  4/  5] [data: 13184/24058] Training Loss: 0.1768 Training Acc: 94.9560%\n",
      "[Epoch:  4/  5] [data: 13312/24058] Training Loss: 0.0998 Training Acc: 94.9820%\n",
      "[Epoch:  4/  5] [data: 13440/24058] Training Loss: 0.0785 Training Acc: 95.0149%\n",
      "[Epoch:  4/  5] [data: 13568/24058] Training Loss: 0.1789 Training Acc: 95.0177%\n",
      "[Epoch:  4/  5] [data: 13696/24058] Training Loss: 0.1815 Training Acc: 95.0131%\n",
      "[Epoch:  4/  5] [data: 13824/24058] Training Loss: 0.1308 Training Acc: 95.0231%\n",
      "[Epoch:  4/  5] [data: 13952/24058] Training Loss: 0.1655 Training Acc: 95.0258%\n",
      "[Epoch:  4/  5] [data: 14080/24058] Training Loss: 0.1554 Training Acc: 95.0355%\n",
      "[Epoch:  4/  5] [data: 14208/24058] Training Loss: 0.2600 Training Acc: 95.0169%\n",
      "[Epoch:  4/  5] [data: 14336/24058] Training Loss: 0.1236 Training Acc: 95.0335%\n",
      "[Epoch:  4/  5] [data: 14464/24058] Training Loss: 0.0674 Training Acc: 95.0636%\n",
      "[Epoch:  4/  5] [data: 14592/24058] Training Loss: 0.1685 Training Acc: 95.0658%\n",
      "[Epoch:  4/  5] [data: 14720/24058] Training Loss: 0.0694 Training Acc: 95.0951%\n",
      "[Epoch:  4/  5] [data: 14848/24058] Training Loss: 0.1725 Training Acc: 95.0902%\n",
      "[Epoch:  4/  5] [data: 14976/24058] Training Loss: 0.1494 Training Acc: 95.0988%\n",
      "[Epoch:  4/  5] [data: 15104/24058] Training Loss: 0.1174 Training Acc: 95.1073%\n",
      "[Epoch:  4/  5] [data: 15232/24058] Training Loss: 0.1483 Training Acc: 95.1155%\n",
      "[Epoch:  4/  5] [data: 15360/24058] Training Loss: 0.1269 Training Acc: 95.1302%\n",
      "[Epoch:  4/  5] [data: 15488/24058] Training Loss: 0.1369 Training Acc: 95.1317%\n",
      "[Epoch:  4/  5] [data: 15616/24058] Training Loss: 0.1737 Training Acc: 95.1332%\n",
      "[Epoch:  4/  5] [data: 15744/24058] Training Loss: 0.2376 Training Acc: 95.1220%\n",
      "[Epoch:  4/  5] [data: 15872/24058] Training Loss: 0.1314 Training Acc: 95.1361%\n",
      "[Epoch:  4/  5] [data: 16000/24058] Training Loss: 0.1889 Training Acc: 95.1375%\n",
      "[Epoch:  4/  5] [data: 16128/24058] Training Loss: 0.1325 Training Acc: 95.1451%\n",
      "[Epoch:  4/  5] [data: 16256/24058] Training Loss: 0.1971 Training Acc: 95.1403%\n",
      "[Epoch:  4/  5] [data: 16384/24058] Training Loss: 0.1831 Training Acc: 95.1416%\n",
      "[Epoch:  4/  5] [data: 16512/24058] Training Loss: 0.2489 Training Acc: 95.1187%\n",
      "[Epoch:  4/  5] [data: 16640/24058] Training Loss: 0.1785 Training Acc: 95.1142%\n",
      "[Epoch:  4/  5] [data: 16768/24058] Training Loss: 0.1314 Training Acc: 95.1276%\n",
      "[Epoch:  4/  5] [data: 16896/24058] Training Loss: 0.1841 Training Acc: 95.1290%\n",
      "[Epoch:  4/  5] [data: 17024/24058] Training Loss: 0.2840 Training Acc: 95.1069%\n",
      "[Epoch:  4/  5] [data: 17152/24058] Training Loss: 0.1765 Training Acc: 95.1026%\n",
      "[Epoch:  4/  5] [data: 17280/24058] Training Loss: 0.2518 Training Acc: 95.0868%\n",
      "[Epoch:  4/  5] [data: 17408/24058] Training Loss: 0.2288 Training Acc: 95.0770%\n",
      "[Epoch:  4/  5] [data: 17536/24058] Training Loss: 0.1058 Training Acc: 95.0901%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 17664/24058] Training Loss: 0.1527 Training Acc: 95.0974%\n",
      "[Epoch:  4/  5] [data: 17792/24058] Training Loss: 0.1440 Training Acc: 95.1045%\n",
      "[Epoch:  4/  5] [data: 17920/24058] Training Loss: 0.2131 Training Acc: 95.0949%\n",
      "[Epoch:  4/  5] [data: 18048/24058] Training Loss: 0.1250 Training Acc: 95.1075%\n",
      "[Epoch:  4/  5] [data: 18176/24058] Training Loss: 0.2821 Training Acc: 95.0814%\n",
      "[Epoch:  4/  5] [data: 18304/24058] Training Loss: 0.1741 Training Acc: 95.0830%\n",
      "[Epoch:  4/  5] [data: 18432/24058] Training Loss: 0.1328 Training Acc: 95.0901%\n",
      "[Epoch:  4/  5] [data: 18560/24058] Training Loss: 0.1826 Training Acc: 95.0862%\n",
      "[Epoch:  4/  5] [data: 18688/24058] Training Loss: 0.0976 Training Acc: 95.1038%\n",
      "[Epoch:  4/  5] [data: 18816/24058] Training Loss: 0.1512 Training Acc: 95.1052%\n",
      "[Epoch:  4/  5] [data: 18944/24058] Training Loss: 0.1781 Training Acc: 95.1066%\n",
      "[Epoch:  4/  5] [data: 19072/24058] Training Loss: 0.1286 Training Acc: 95.1133%\n",
      "[Epoch:  4/  5] [data: 19200/24058] Training Loss: 0.0934 Training Acc: 95.1302%\n",
      "[Epoch:  4/  5] [data: 19328/24058] Training Loss: 0.2073 Training Acc: 95.1159%\n",
      "[Epoch:  4/  5] [data: 19456/24058] Training Loss: 0.1709 Training Acc: 95.1172%\n",
      "[Epoch:  4/  5] [data: 19584/24058] Training Loss: 0.1063 Training Acc: 95.1287%\n",
      "[Epoch:  4/  5] [data: 19712/24058] Training Loss: 0.1498 Training Acc: 95.1349%\n",
      "[Epoch:  4/  5] [data: 19840/24058] Training Loss: 0.1280 Training Acc: 95.1411%\n",
      "[Epoch:  4/  5] [data: 19968/24058] Training Loss: 0.2047 Training Acc: 95.1322%\n",
      "[Epoch:  4/  5] [data: 20096/24058] Training Loss: 0.3153 Training Acc: 95.1035%\n",
      "[Epoch:  4/  5] [data: 20224/24058] Training Loss: 0.2312 Training Acc: 95.0850%\n",
      "[Epoch:  4/  5] [data: 20352/24058] Training Loss: 0.1294 Training Acc: 95.0914%\n",
      "[Epoch:  4/  5] [data: 20480/24058] Training Loss: 0.0846 Training Acc: 95.1123%\n",
      "[Epoch:  4/  5] [data: 20608/24058] Training Loss: 0.2636 Training Acc: 95.0844%\n",
      "[Epoch:  4/  5] [data: 20736/24058] Training Loss: 0.1223 Training Acc: 95.0907%\n",
      "[Epoch:  4/  5] [data: 20864/24058] Training Loss: 0.1090 Training Acc: 95.0968%\n",
      "[Epoch:  4/  5] [data: 20992/24058] Training Loss: 0.1846 Training Acc: 95.0886%\n",
      "[Epoch:  4/  5] [data: 21120/24058] Training Loss: 0.1892 Training Acc: 95.0852%\n",
      "[Epoch:  4/  5] [data: 21248/24058] Training Loss: 0.1222 Training Acc: 95.0960%\n",
      "[Epoch:  4/  5] [data: 21376/24058] Training Loss: 0.1494 Training Acc: 95.0973%\n",
      "[Epoch:  4/  5] [data: 21504/24058] Training Loss: 0.0719 Training Acc: 95.1172%\n",
      "[Epoch:  4/  5] [data: 21632/24058] Training Loss: 0.2693 Training Acc: 95.0999%\n",
      "[Epoch:  4/  5] [data: 21760/24058] Training Loss: 0.1717 Training Acc: 95.1011%\n",
      "[Epoch:  4/  5] [data: 21888/24058] Training Loss: 0.2264 Training Acc: 95.0932%\n",
      "[Epoch:  4/  5] [data: 22016/24058] Training Loss: 0.1821 Training Acc: 95.0899%\n",
      "[Epoch:  4/  5] [data: 22144/24058] Training Loss: 0.1962 Training Acc: 95.0867%\n",
      "[Epoch:  4/  5] [data: 22272/24058] Training Loss: 0.1420 Training Acc: 95.0970%\n",
      "[Epoch:  4/  5] [data: 22400/24058] Training Loss: 0.2018 Training Acc: 95.0938%\n",
      "[Epoch:  4/  5] [data: 22528/24058] Training Loss: 0.1829 Training Acc: 95.0906%\n",
      "[Epoch:  4/  5] [data: 22656/24058] Training Loss: 0.2298 Training Acc: 95.0830%\n",
      "[Epoch:  4/  5] [data: 22784/24058] Training Loss: 0.1596 Training Acc: 95.0843%\n",
      "[Epoch:  4/  5] [data: 22912/24058] Training Loss: 0.1931 Training Acc: 95.0812%\n",
      "[Epoch:  4/  5] [data: 23040/24058] Training Loss: 0.2235 Training Acc: 95.0738%\n",
      "[Epoch:  4/  5] [data: 23168/24058] Training Loss: 0.1377 Training Acc: 95.0751%\n",
      "[Epoch:  4/  5] [data: 23296/24058] Training Loss: 0.2040 Training Acc: 95.0678%\n",
      "[Epoch:  4/  5] [data: 23424/24058] Training Loss: 0.1066 Training Acc: 95.0777%\n",
      "[Epoch:  4/  5] [data: 23552/24058] Training Loss: 0.1459 Training Acc: 95.0790%\n",
      "[Epoch:  4/  5] [data: 23680/24058] Training Loss: 0.1934 Training Acc: 95.0718%\n",
      "[Epoch:  4/  5] [data: 23808/24058] Training Loss: 0.0548 Training Acc: 95.0941%\n",
      "[Epoch:  4/  5] [data: 23936/24058] Training Loss: 0.1463 Training Acc: 95.0994%\n",
      "[Epoch:  4/  5] [data: 24058/24058] Training Loss: 0.1692 Training Acc: 95.1035%\n",
      "Testing-4...\n",
      "[Epoch:  4/  5] Validation Loss: 0.1756 Validation Acc: 94.9090%\n",
      "Time used: 1216.7156257629395s\n",
      "[Epoch:  5/  5] [data: 128/24058] Training Loss: 0.1982 Training Acc: 92.9688%\n",
      "[Epoch:  5/  5] [data: 256/24058] Training Loss: 0.2634 Training Acc: 91.4062%\n",
      "[Epoch:  5/  5] [data: 384/24058] Training Loss: 0.2814 Training Acc: 91.6667%\n",
      "[Epoch:  5/  5] [data: 512/24058] Training Loss: 0.1007 Training Acc: 93.1641%\n",
      "[Epoch:  5/  5] [data: 640/24058] Training Loss: 0.1799 Training Acc: 93.4375%\n",
      "[Epoch:  5/  5] [data: 768/24058] Training Loss: 0.2146 Training Acc: 93.4896%\n",
      "[Epoch:  5/  5] [data: 896/24058] Training Loss: 0.0519 Training Acc: 94.3080%\n",
      "[Epoch:  5/  5] [data: 1024/24058] Training Loss: 0.1520 Training Acc: 94.5312%\n",
      "[Epoch:  5/  5] [data: 1152/24058] Training Loss: 0.1755 Training Acc: 94.6181%\n",
      "[Epoch:  5/  5] [data: 1280/24058] Training Loss: 0.4457 Training Acc: 93.8281%\n",
      "[Epoch:  5/  5] [data: 1408/24058] Training Loss: 0.1531 Training Acc: 94.0341%\n",
      "[Epoch:  5/  5] [data: 1536/24058] Training Loss: 0.0886 Training Acc: 94.3359%\n",
      "[Epoch:  5/  5] [data: 1664/24058] Training Loss: 0.2449 Training Acc: 94.1707%\n",
      "[Epoch:  5/  5] [data: 1792/24058] Training Loss: 0.2628 Training Acc: 93.9732%\n",
      "[Epoch:  5/  5] [data: 1920/24058] Training Loss: 0.1027 Training Acc: 94.1667%\n",
      "[Epoch:  5/  5] [data: 2048/24058] Training Loss: 0.1978 Training Acc: 94.1895%\n",
      "[Epoch:  5/  5] [data: 2176/24058] Training Loss: 0.0847 Training Acc: 94.3934%\n",
      "[Epoch:  5/  5] [data: 2304/24058] Training Loss: 0.2282 Training Acc: 94.3142%\n",
      "[Epoch:  5/  5] [data: 2432/24058] Training Loss: 0.0749 Training Acc: 94.4490%\n",
      "[Epoch:  5/  5] [data: 2560/24058] Training Loss: 0.1849 Training Acc: 94.4531%\n",
      "[Epoch:  5/  5] [data: 2688/24058] Training Loss: 0.0726 Training Acc: 94.6429%\n",
      "[Epoch:  5/  5] [data: 2816/24058] Training Loss: 0.1580 Training Acc: 94.6733%\n",
      "[Epoch:  5/  5] [data: 2944/24058] Training Loss: 0.0682 Training Acc: 94.8370%\n",
      "[Epoch:  5/  5] [data: 3072/24058] Training Loss: 0.2611 Training Acc: 94.7266%\n",
      "[Epoch:  5/  5] [data: 3200/24058] Training Loss: 0.2010 Training Acc: 94.7188%\n",
      "[Epoch:  5/  5] [data: 3328/24058] Training Loss: 0.1141 Training Acc: 94.8017%\n",
      "[Epoch:  5/  5] [data: 3456/24058] Training Loss: 0.1238 Training Acc: 94.8785%\n",
      "[Epoch:  5/  5] [data: 3584/24058] Training Loss: 0.1061 Training Acc: 94.9498%\n",
      "[Epoch:  5/  5] [data: 3712/24058] Training Loss: 0.2785 Training Acc: 94.8006%\n",
      "[Epoch:  5/  5] [data: 3840/24058] Training Loss: 0.0791 Training Acc: 94.9219%\n",
      "[Epoch:  5/  5] [data: 3968/24058] Training Loss: 0.1203 Training Acc: 94.9849%\n",
      "[Epoch:  5/  5] [data: 4096/24058] Training Loss: 0.2142 Training Acc: 94.9463%\n",
      "[Epoch:  5/  5] [data: 4224/24058] Training Loss: 0.1204 Training Acc: 95.0047%\n",
      "[Epoch:  5/  5] [data: 4352/24058] Training Loss: 0.0600 Training Acc: 95.1057%\n",
      "[Epoch:  5/  5] [data: 4480/24058] Training Loss: 0.1909 Training Acc: 95.0223%\n",
      "[Epoch:  5/  5] [data: 4608/24058] Training Loss: 0.0643 Training Acc: 95.1172%\n",
      "[Epoch:  5/  5] [data: 4736/24058] Training Loss: 0.2282 Training Acc: 95.1014%\n",
      "[Epoch:  5/  5] [data: 4864/24058] Training Loss: 0.0969 Training Acc: 95.1686%\n",
      "[Epoch:  5/  5] [data: 4992/24058] Training Loss: 0.1038 Training Acc: 95.1923%\n",
      "[Epoch:  5/  5] [data: 5120/24058] Training Loss: 0.1794 Training Acc: 95.1953%\n",
      "[Epoch:  5/  5] [data: 5248/24058] Training Loss: 0.1727 Training Acc: 95.1982%\n",
      "[Epoch:  5/  5] [data: 5376/24058] Training Loss: 0.1979 Training Acc: 95.1823%\n",
      "[Epoch:  5/  5] [data: 5504/24058] Training Loss: 0.1073 Training Acc: 95.2398%\n",
      "[Epoch:  5/  5] [data: 5632/24058] Training Loss: 0.1684 Training Acc: 95.2592%\n",
      "[Epoch:  5/  5] [data: 5760/24058] Training Loss: 0.1494 Training Acc: 95.2778%\n",
      "[Epoch:  5/  5] [data: 5888/24058] Training Loss: 0.1494 Training Acc: 95.2955%\n",
      "[Epoch:  5/  5] [data: 6016/24058] Training Loss: 0.1568 Training Acc: 95.2959%\n",
      "[Epoch:  5/  5] [data: 6144/24058] Training Loss: 0.1326 Training Acc: 95.3125%\n",
      "[Epoch:  5/  5] [data: 6272/24058] Training Loss: 0.2038 Training Acc: 95.2806%\n",
      "[Epoch:  5/  5] [data: 6400/24058] Training Loss: 0.2064 Training Acc: 95.2500%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 6528/24058] Training Loss: 0.1483 Training Acc: 95.2665%\n",
      "[Epoch:  5/  5] [data: 6656/24058] Training Loss: 0.1728 Training Acc: 95.2674%\n",
      "[Epoch:  5/  5] [data: 6784/24058] Training Loss: 0.1244 Training Acc: 95.2978%\n",
      "[Epoch:  5/  5] [data: 6912/24058] Training Loss: 0.1958 Training Acc: 95.2836%\n",
      "[Epoch:  5/  5] [data: 7040/24058] Training Loss: 0.1636 Training Acc: 95.2841%\n",
      "[Epoch:  5/  5] [data: 7168/24058] Training Loss: 0.1840 Training Acc: 95.2846%\n",
      "[Epoch:  5/  5] [data: 7296/24058] Training Loss: 0.0757 Training Acc: 95.3399%\n",
      "[Epoch:  5/  5] [data: 7424/24058] Training Loss: 0.1062 Training Acc: 95.3664%\n",
      "[Epoch:  5/  5] [data: 7552/24058] Training Loss: 0.0714 Training Acc: 95.4052%\n",
      "[Epoch:  5/  5] [data: 7680/24058] Training Loss: 0.1809 Training Acc: 95.3906%\n",
      "[Epoch:  5/  5] [data: 7808/24058] Training Loss: 0.1895 Training Acc: 95.3765%\n",
      "[Epoch:  5/  5] [data: 7936/24058] Training Loss: 0.2968 Training Acc: 95.3251%\n",
      "[Epoch:  5/  5] [data: 8064/24058] Training Loss: 0.2629 Training Acc: 95.2877%\n",
      "[Epoch:  5/  5] [data: 8192/24058] Training Loss: 0.0728 Training Acc: 95.3247%\n",
      "[Epoch:  5/  5] [data: 8320/24058] Training Loss: 0.1740 Training Acc: 95.3245%\n",
      "[Epoch:  5/  5] [data: 8448/24058] Training Loss: 0.2341 Training Acc: 95.2770%\n",
      "[Epoch:  5/  5] [data: 8576/24058] Training Loss: 0.0790 Training Acc: 95.3242%\n",
      "[Epoch:  5/  5] [data: 8704/24058] Training Loss: 0.1844 Training Acc: 95.3010%\n",
      "[Epoch:  5/  5] [data: 8832/24058] Training Loss: 0.2657 Training Acc: 95.2559%\n",
      "[Epoch:  5/  5] [data: 8960/24058] Training Loss: 0.2667 Training Acc: 95.2121%\n",
      "[Epoch:  5/  5] [data: 9088/24058] Training Loss: 0.1713 Training Acc: 95.2025%\n",
      "[Epoch:  5/  5] [data: 9216/24058] Training Loss: 0.1256 Training Acc: 95.2148%\n",
      "[Epoch:  5/  5] [data: 9344/24058] Training Loss: 0.1803 Training Acc: 95.2055%\n",
      "[Epoch:  5/  5] [data: 9472/24058] Training Loss: 0.2136 Training Acc: 95.1964%\n",
      "[Epoch:  5/  5] [data: 9600/24058] Training Loss: 0.2609 Training Acc: 95.1667%\n",
      "[Epoch:  5/  5] [data: 9728/24058] Training Loss: 0.1766 Training Acc: 95.1789%\n",
      "[Epoch:  5/  5] [data: 9856/24058] Training Loss: 0.1922 Training Acc: 95.1705%\n",
      "[Epoch:  5/  5] [data: 9984/24058] Training Loss: 0.1553 Training Acc: 95.1623%\n",
      "[Epoch:  5/  5] [data: 10112/24058] Training Loss: 0.2274 Training Acc: 95.1444%\n",
      "[Epoch:  5/  5] [data: 10240/24058] Training Loss: 0.1363 Training Acc: 95.1465%\n",
      "[Epoch:  5/  5] [data: 10368/24058] Training Loss: 0.1501 Training Acc: 95.1582%\n",
      "[Epoch:  5/  5] [data: 10496/24058] Training Loss: 0.1230 Training Acc: 95.1791%\n",
      "[Epoch:  5/  5] [data: 10624/24058] Training Loss: 0.0811 Training Acc: 95.1995%\n",
      "[Epoch:  5/  5] [data: 10752/24058] Training Loss: 0.1205 Training Acc: 95.2195%\n",
      "[Epoch:  5/  5] [data: 10880/24058] Training Loss: 0.1666 Training Acc: 95.2298%\n",
      "[Epoch:  5/  5] [data: 11008/24058] Training Loss: 0.2323 Training Acc: 95.1944%\n",
      "[Epoch:  5/  5] [data: 11136/24058] Training Loss: 0.2074 Training Acc: 95.1868%\n",
      "[Epoch:  5/  5] [data: 11264/24058] Training Loss: 0.1938 Training Acc: 95.1705%\n",
      "[Epoch:  5/  5] [data: 11392/24058] Training Loss: 0.0737 Training Acc: 95.2072%\n",
      "[Epoch:  5/  5] [data: 11520/24058] Training Loss: 0.1521 Training Acc: 95.1997%\n",
      "[Epoch:  5/  5] [data: 11648/24058] Training Loss: 0.1814 Training Acc: 95.2009%\n",
      "[Epoch:  5/  5] [data: 11776/24058] Training Loss: 0.2810 Training Acc: 95.1596%\n",
      "[Epoch:  5/  5] [data: 11904/24058] Training Loss: 0.1504 Training Acc: 95.1697%\n",
      "[Epoch:  5/  5] [data: 12032/24058] Training Loss: 0.1342 Training Acc: 95.1712%\n",
      "[Epoch:  5/  5] [data: 12160/24058] Training Loss: 0.1468 Training Acc: 95.1727%\n",
      "[Epoch:  5/  5] [data: 12288/24058] Training Loss: 0.1536 Training Acc: 95.1823%\n",
      "[Epoch:  5/  5] [data: 12416/24058] Training Loss: 0.2574 Training Acc: 95.1434%\n",
      "[Epoch:  5/  5] [data: 12544/24058] Training Loss: 0.1448 Training Acc: 95.1531%\n",
      "[Epoch:  5/  5] [data: 12672/24058] Training Loss: 0.2146 Training Acc: 95.1310%\n",
      "[Epoch:  5/  5] [data: 12800/24058] Training Loss: 0.2255 Training Acc: 95.1172%\n",
      "[Epoch:  5/  5] [data: 12928/24058] Training Loss: 0.1269 Training Acc: 95.1346%\n",
      "[Epoch:  5/  5] [data: 13056/24058] Training Loss: 0.2010 Training Acc: 95.1287%\n",
      "[Epoch:  5/  5] [data: 13184/24058] Training Loss: 0.1769 Training Acc: 95.1305%\n",
      "[Epoch:  5/  5] [data: 13312/24058] Training Loss: 0.1025 Training Acc: 95.1547%\n",
      "[Epoch:  5/  5] [data: 13440/24058] Training Loss: 0.0780 Training Acc: 95.1860%\n",
      "[Epoch:  5/  5] [data: 13568/24058] Training Loss: 0.1768 Training Acc: 95.1872%\n",
      "[Epoch:  5/  5] [data: 13696/24058] Training Loss: 0.1828 Training Acc: 95.1811%\n",
      "[Epoch:  5/  5] [data: 13824/24058] Training Loss: 0.1315 Training Acc: 95.1895%\n",
      "[Epoch:  5/  5] [data: 13952/24058] Training Loss: 0.1605 Training Acc: 95.1907%\n",
      "[Epoch:  5/  5] [data: 14080/24058] Training Loss: 0.1522 Training Acc: 95.1989%\n",
      "[Epoch:  5/  5] [data: 14208/24058] Training Loss: 0.2588 Training Acc: 95.1788%\n",
      "[Epoch:  5/  5] [data: 14336/24058] Training Loss: 0.1258 Training Acc: 95.1939%\n",
      "[Epoch:  5/  5] [data: 14464/24058] Training Loss: 0.0652 Training Acc: 95.2226%\n",
      "[Epoch:  5/  5] [data: 14592/24058] Training Loss: 0.1705 Training Acc: 95.2234%\n",
      "[Epoch:  5/  5] [data: 14720/24058] Training Loss: 0.0678 Training Acc: 95.2514%\n",
      "[Epoch:  5/  5] [data: 14848/24058] Training Loss: 0.1656 Training Acc: 95.2452%\n",
      "[Epoch:  5/  5] [data: 14976/24058] Training Loss: 0.1464 Training Acc: 95.2457%\n",
      "[Epoch:  5/  5] [data: 15104/24058] Training Loss: 0.1161 Training Acc: 95.2529%\n",
      "[Epoch:  5/  5] [data: 15232/24058] Training Loss: 0.1460 Training Acc: 95.2600%\n",
      "[Epoch:  5/  5] [data: 15360/24058] Training Loss: 0.1240 Training Acc: 95.2734%\n",
      "[Epoch:  5/  5] [data: 15488/24058] Training Loss: 0.1309 Training Acc: 95.2738%\n",
      "[Epoch:  5/  5] [data: 15616/24058] Training Loss: 0.1721 Training Acc: 95.2741%\n",
      "[Epoch:  5/  5] [data: 15744/24058] Training Loss: 0.2333 Training Acc: 95.2617%\n",
      "[Epoch:  5/  5] [data: 15872/24058] Training Loss: 0.1333 Training Acc: 95.2747%\n",
      "[Epoch:  5/  5] [data: 16000/24058] Training Loss: 0.1916 Training Acc: 95.2750%\n",
      "[Epoch:  5/  5] [data: 16128/24058] Training Loss: 0.1315 Training Acc: 95.2815%\n",
      "[Epoch:  5/  5] [data: 16256/24058] Training Loss: 0.1960 Training Acc: 95.2756%\n",
      "[Epoch:  5/  5] [data: 16384/24058] Training Loss: 0.1822 Training Acc: 95.2759%\n",
      "[Epoch:  5/  5] [data: 16512/24058] Training Loss: 0.2515 Training Acc: 95.2519%\n",
      "[Epoch:  5/  5] [data: 16640/24058] Training Loss: 0.1755 Training Acc: 95.2464%\n",
      "[Epoch:  5/  5] [data: 16768/24058] Training Loss: 0.1275 Training Acc: 95.2588%\n",
      "[Epoch:  5/  5] [data: 16896/24058] Training Loss: 0.1829 Training Acc: 95.2592%\n",
      "[Epoch:  5/  5] [data: 17024/24058] Training Loss: 0.2840 Training Acc: 95.2361%\n",
      "[Epoch:  5/  5] [data: 17152/24058] Training Loss: 0.1760 Training Acc: 95.2309%\n",
      "[Epoch:  5/  5] [data: 17280/24058] Training Loss: 0.2517 Training Acc: 95.2141%\n",
      "[Epoch:  5/  5] [data: 17408/24058] Training Loss: 0.2270 Training Acc: 95.2034%\n",
      "[Epoch:  5/  5] [data: 17536/24058] Training Loss: 0.1077 Training Acc: 95.2213%\n",
      "[Epoch:  5/  5] [data: 17664/24058] Training Loss: 0.1524 Training Acc: 95.2276%\n",
      "[Epoch:  5/  5] [data: 17792/24058] Training Loss: 0.1367 Training Acc: 95.2394%\n",
      "[Epoch:  5/  5] [data: 17920/24058] Training Loss: 0.2048 Training Acc: 95.2288%\n",
      "[Epoch:  5/  5] [data: 18048/24058] Training Loss: 0.1262 Training Acc: 95.2405%\n",
      "[Epoch:  5/  5] [data: 18176/24058] Training Loss: 0.2825 Training Acc: 95.2135%\n",
      "[Epoch:  5/  5] [data: 18304/24058] Training Loss: 0.1744 Training Acc: 95.2142%\n",
      "[Epoch:  5/  5] [data: 18432/24058] Training Loss: 0.1334 Training Acc: 95.2203%\n",
      "[Epoch:  5/  5] [data: 18560/24058] Training Loss: 0.1870 Training Acc: 95.2101%\n",
      "[Epoch:  5/  5] [data: 18688/24058] Training Loss: 0.0967 Training Acc: 95.2269%\n",
      "[Epoch:  5/  5] [data: 18816/24058] Training Loss: 0.1488 Training Acc: 95.2328%\n",
      "[Epoch:  5/  5] [data: 18944/24058] Training Loss: 0.1734 Training Acc: 95.2333%\n",
      "[Epoch:  5/  5] [data: 19072/24058] Training Loss: 0.1290 Training Acc: 95.2443%\n",
      "[Epoch:  5/  5] [data: 19200/24058] Training Loss: 0.0922 Training Acc: 95.2604%\n",
      "[Epoch:  5/  5] [data: 19328/24058] Training Loss: 0.2057 Training Acc: 95.2504%\n",
      "[Epoch:  5/  5] [data: 19456/24058] Training Loss: 0.1684 Training Acc: 95.2508%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 19584/24058] Training Loss: 0.1014 Training Acc: 95.2614%\n",
      "[Epoch:  5/  5] [data: 19712/24058] Training Loss: 0.1513 Training Acc: 95.2668%\n",
      "[Epoch:  5/  5] [data: 19840/24058] Training Loss: 0.1272 Training Acc: 95.2722%\n",
      "[Epoch:  5/  5] [data: 19968/24058] Training Loss: 0.2007 Training Acc: 95.2624%\n",
      "[Epoch:  5/  5] [data: 20096/24058] Training Loss: 0.3121 Training Acc: 95.2329%\n",
      "[Epoch:  5/  5] [data: 20224/24058] Training Loss: 0.2314 Training Acc: 95.2136%\n",
      "[Epoch:  5/  5] [data: 20352/24058] Training Loss: 0.1273 Training Acc: 95.2191%\n",
      "[Epoch:  5/  5] [data: 20480/24058] Training Loss: 0.0812 Training Acc: 95.2393%\n",
      "[Epoch:  5/  5] [data: 20608/24058] Training Loss: 0.2590 Training Acc: 95.2106%\n",
      "[Epoch:  5/  5] [data: 20736/24058] Training Loss: 0.1208 Training Acc: 95.2160%\n",
      "[Epoch:  5/  5] [data: 20864/24058] Training Loss: 0.1032 Training Acc: 95.2262%\n",
      "[Epoch:  5/  5] [data: 20992/24058] Training Loss: 0.1824 Training Acc: 95.2172%\n",
      "[Epoch:  5/  5] [data: 21120/24058] Training Loss: 0.1877 Training Acc: 95.2131%\n",
      "[Epoch:  5/  5] [data: 21248/24058] Training Loss: 0.1207 Training Acc: 95.2231%\n",
      "[Epoch:  5/  5] [data: 21376/24058] Training Loss: 0.1472 Training Acc: 95.2236%\n",
      "[Epoch:  5/  5] [data: 21504/24058] Training Loss: 0.0702 Training Acc: 95.2427%\n",
      "[Epoch:  5/  5] [data: 21632/24058] Training Loss: 0.2690 Training Acc: 95.2247%\n",
      "[Epoch:  5/  5] [data: 21760/24058] Training Loss: 0.1706 Training Acc: 95.2252%\n",
      "[Epoch:  5/  5] [data: 21888/24058] Training Loss: 0.2264 Training Acc: 95.2166%\n",
      "[Epoch:  5/  5] [data: 22016/24058] Training Loss: 0.1856 Training Acc: 95.2126%\n",
      "[Epoch:  5/  5] [data: 22144/24058] Training Loss: 0.1950 Training Acc: 95.2086%\n",
      "[Epoch:  5/  5] [data: 22272/24058] Training Loss: 0.1425 Training Acc: 95.2182%\n",
      "[Epoch:  5/  5] [data: 22400/24058] Training Loss: 0.2026 Training Acc: 95.2143%\n",
      "[Epoch:  5/  5] [data: 22528/24058] Training Loss: 0.1794 Training Acc: 95.2104%\n",
      "[Epoch:  5/  5] [data: 22656/24058] Training Loss: 0.2270 Training Acc: 95.2022%\n",
      "[Epoch:  5/  5] [data: 22784/24058] Training Loss: 0.1545 Training Acc: 95.2028%\n",
      "[Epoch:  5/  5] [data: 22912/24058] Training Loss: 0.1931 Training Acc: 95.1990%\n",
      "[Epoch:  5/  5] [data: 23040/24058] Training Loss: 0.2158 Training Acc: 95.1910%\n",
      "[Epoch:  5/  5] [data: 23168/24058] Training Loss: 0.1318 Training Acc: 95.1960%\n",
      "[Epoch:  5/  5] [data: 23296/24058] Training Loss: 0.2038 Training Acc: 95.1923%\n",
      "[Epoch:  5/  5] [data: 23424/24058] Training Loss: 0.1040 Training Acc: 95.2015%\n",
      "[Epoch:  5/  5] [data: 23552/24058] Training Loss: 0.1430 Training Acc: 95.2021%\n",
      "[Epoch:  5/  5] [data: 23680/24058] Training Loss: 0.1911 Training Acc: 95.1943%\n",
      "[Epoch:  5/  5] [data: 23808/24058] Training Loss: 0.0520 Training Acc: 95.2159%\n",
      "[Epoch:  5/  5] [data: 23936/24058] Training Loss: 0.1460 Training Acc: 95.2206%\n",
      "[Epoch:  5/  5] [data: 24058/24058] Training Loss: 0.1627 Training Acc: 95.2240%\n",
      "Testing-4...\n",
      "[Epoch:  5/  5] Validation Loss: 0.1728 Validation Acc: 95.0480%\n",
      "Time used: 1154.2623541355133s\n",
      "Type-4: 1.0 0.90067214339059 0.9477406679764243\n",
      "[Epoch:  1/  5] [data: 128/24026] Training Loss: 0.3361 Training Acc: 89.8438%\n",
      "[Epoch:  1/  5] [data: 256/24026] Training Loss: 0.2518 Training Acc: 91.0156%\n",
      "[Epoch:  1/  5] [data: 384/24026] Training Loss: 0.2204 Training Acc: 91.9271%\n",
      "[Epoch:  1/  5] [data: 512/24026] Training Loss: 0.3869 Training Acc: 91.2109%\n",
      "[Epoch:  1/  5] [data: 640/24026] Training Loss: 0.1686 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 768/24026] Training Loss: 0.3553 Training Acc: 91.9271%\n",
      "[Epoch:  1/  5] [data: 896/24026] Training Loss: 0.2678 Training Acc: 91.9643%\n",
      "[Epoch:  1/  5] [data: 1024/24026] Training Loss: 0.2296 Training Acc: 92.1875%\n",
      "[Epoch:  1/  5] [data: 1152/24026] Training Loss: 0.1086 Training Acc: 92.7083%\n",
      "[Epoch:  1/  5] [data: 1280/24026] Training Loss: 0.1787 Training Acc: 92.8906%\n",
      "[Epoch:  1/  5] [data: 1408/24026] Training Loss: 0.1938 Training Acc: 93.0398%\n",
      "[Epoch:  1/  5] [data: 1536/24026] Training Loss: 0.3329 Training Acc: 92.8385%\n",
      "[Epoch:  1/  5] [data: 1664/24026] Training Loss: 0.2110 Training Acc: 92.9688%\n",
      "[Epoch:  1/  5] [data: 1792/24026] Training Loss: 0.2849 Training Acc: 92.9129%\n",
      "[Epoch:  1/  5] [data: 1920/24026] Training Loss: 0.2114 Training Acc: 93.0208%\n",
      "[Epoch:  1/  5] [data: 2048/24026] Training Loss: 0.2064 Training Acc: 93.0664%\n",
      "[Epoch:  1/  5] [data: 2176/24026] Training Loss: 0.1729 Training Acc: 93.1985%\n",
      "[Epoch:  1/  5] [data: 2304/24026] Training Loss: 0.2028 Training Acc: 93.2726%\n",
      "[Epoch:  1/  5] [data: 2432/24026] Training Loss: 0.2515 Training Acc: 93.2155%\n",
      "[Epoch:  1/  5] [data: 2560/24026] Training Loss: 0.1497 Training Acc: 93.3594%\n",
      "[Epoch:  1/  5] [data: 2688/24026] Training Loss: 0.2188 Training Acc: 93.3780%\n",
      "[Epoch:  1/  5] [data: 2816/24026] Training Loss: 0.1536 Training Acc: 93.5014%\n",
      "[Epoch:  1/  5] [data: 2944/24026] Training Loss: 0.2153 Training Acc: 93.5122%\n",
      "[Epoch:  1/  5] [data: 3072/24026] Training Loss: 0.4258 Training Acc: 93.2617%\n",
      "[Epoch:  1/  5] [data: 3200/24026] Training Loss: 0.2037 Training Acc: 93.3125%\n",
      "[Epoch:  1/  5] [data: 3328/24026] Training Loss: 0.2131 Training Acc: 93.2993%\n",
      "[Epoch:  1/  5] [data: 3456/24026] Training Loss: 0.2746 Training Acc: 93.2292%\n",
      "[Epoch:  1/  5] [data: 3584/24026] Training Loss: 0.2659 Training Acc: 93.1920%\n",
      "[Epoch:  1/  5] [data: 3712/24026] Training Loss: 0.2056 Training Acc: 93.2381%\n",
      "[Epoch:  1/  5] [data: 3840/24026] Training Loss: 0.2070 Training Acc: 93.2812%\n",
      "[Epoch:  1/  5] [data: 3968/24026] Training Loss: 0.1019 Training Acc: 93.4476%\n",
      "[Epoch:  1/  5] [data: 4096/24026] Training Loss: 0.2058 Training Acc: 93.4814%\n",
      "[Epoch:  1/  5] [data: 4224/24026] Training Loss: 0.2606 Training Acc: 93.4659%\n",
      "[Epoch:  1/  5] [data: 4352/24026] Training Loss: 0.2387 Training Acc: 93.4743%\n",
      "[Epoch:  1/  5] [data: 4480/24026] Training Loss: 0.3365 Training Acc: 93.4152%\n",
      "[Epoch:  1/  5] [data: 4608/24026] Training Loss: 0.2869 Training Acc: 93.3811%\n",
      "[Epoch:  1/  5] [data: 4736/24026] Training Loss: 0.1445 Training Acc: 93.4333%\n",
      "[Epoch:  1/  5] [data: 4864/24026] Training Loss: 0.1245 Training Acc: 93.5238%\n",
      "[Epoch:  1/  5] [data: 4992/24026] Training Loss: 0.3333 Training Acc: 93.4295%\n",
      "[Epoch:  1/  5] [data: 5120/24026] Training Loss: 0.2247 Training Acc: 93.4180%\n",
      "[Epoch:  1/  5] [data: 5248/24026] Training Loss: 0.1802 Training Acc: 93.4451%\n",
      "[Epoch:  1/  5] [data: 5376/24026] Training Loss: 0.2802 Training Acc: 93.4152%\n",
      "[Epoch:  1/  5] [data: 5504/24026] Training Loss: 0.1784 Training Acc: 93.4775%\n",
      "[Epoch:  1/  5] [data: 5632/24026] Training Loss: 0.2777 Training Acc: 93.4482%\n",
      "[Epoch:  1/  5] [data: 5760/24026] Training Loss: 0.3363 Training Acc: 93.3681%\n",
      "[Epoch:  1/  5] [data: 5888/24026] Training Loss: 0.2048 Training Acc: 93.3933%\n",
      "[Epoch:  1/  5] [data: 6016/24026] Training Loss: 0.1291 Training Acc: 93.4674%\n",
      "[Epoch:  1/  5] [data: 6144/24026] Training Loss: 0.1474 Training Acc: 93.5059%\n",
      "[Epoch:  1/  5] [data: 6272/24026] Training Loss: 0.1770 Training Acc: 93.5427%\n",
      "[Epoch:  1/  5] [data: 6400/24026] Training Loss: 0.2065 Training Acc: 93.5625%\n",
      "[Epoch:  1/  5] [data: 6528/24026] Training Loss: 0.2116 Training Acc: 93.5662%\n",
      "[Epoch:  1/  5] [data: 6656/24026] Training Loss: 0.1618 Training Acc: 93.5998%\n",
      "[Epoch:  1/  5] [data: 6784/24026] Training Loss: 0.2397 Training Acc: 93.6026%\n",
      "[Epoch:  1/  5] [data: 6912/24026] Training Loss: 0.3766 Training Acc: 93.5330%\n",
      "[Epoch:  1/  5] [data: 7040/24026] Training Loss: 0.1975 Training Acc: 93.5511%\n",
      "[Epoch:  1/  5] [data: 7168/24026] Training Loss: 0.2254 Training Acc: 93.5547%\n",
      "[Epoch:  1/  5] [data: 7296/24026] Training Loss: 0.2471 Training Acc: 93.5444%\n",
      "[Epoch:  1/  5] [data: 7424/24026] Training Loss: 0.2272 Training Acc: 93.5480%\n",
      "[Epoch:  1/  5] [data: 7552/24026] Training Loss: 0.1531 Training Acc: 93.6043%\n",
      "[Epoch:  1/  5] [data: 7680/24026] Training Loss: 0.1953 Training Acc: 93.6328%\n",
      "[Epoch:  1/  5] [data: 7808/24026] Training Loss: 0.2427 Training Acc: 93.6219%\n",
      "[Epoch:  1/  5] [data: 7936/24026] Training Loss: 0.2133 Training Acc: 93.6240%\n",
      "[Epoch:  1/  5] [data: 8064/24026] Training Loss: 0.2726 Training Acc: 93.6012%\n",
      "[Epoch:  1/  5] [data: 8192/24026] Training Loss: 0.1985 Training Acc: 93.6157%\n",
      "[Epoch:  1/  5] [data: 8320/24026] Training Loss: 0.1117 Training Acc: 93.6538%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  5] [data: 8448/24026] Training Loss: 0.2742 Training Acc: 93.6316%\n",
      "[Epoch:  1/  5] [data: 8576/24026] Training Loss: 0.1814 Training Acc: 93.6567%\n",
      "[Epoch:  1/  5] [data: 8704/24026] Training Loss: 0.2017 Training Acc: 93.6696%\n",
      "[Epoch:  1/  5] [data: 8832/24026] Training Loss: 0.1249 Training Acc: 93.7160%\n",
      "[Epoch:  1/  5] [data: 8960/24026] Training Loss: 0.1536 Training Acc: 93.7500%\n",
      "[Epoch:  1/  5] [data: 9088/24026] Training Loss: 0.3665 Training Acc: 93.6620%\n",
      "[Epoch:  1/  5] [data: 9216/24026] Training Loss: 0.1432 Training Acc: 93.6957%\n",
      "[Epoch:  1/  5] [data: 9344/24026] Training Loss: 0.0941 Training Acc: 93.7607%\n",
      "[Epoch:  1/  5] [data: 9472/24026] Training Loss: 0.2043 Training Acc: 93.7711%\n",
      "[Epoch:  1/  5] [data: 9600/24026] Training Loss: 0.2613 Training Acc: 93.7500%\n",
      "[Epoch:  1/  5] [data: 9728/24026] Training Loss: 0.2555 Training Acc: 93.7397%\n",
      "[Epoch:  1/  5] [data: 9856/24026] Training Loss: 0.2996 Training Acc: 93.6993%\n",
      "[Epoch:  1/  5] [data: 9984/24026] Training Loss: 0.3943 Training Acc: 93.6198%\n",
      "[Epoch:  1/  5] [data: 10112/24026] Training Loss: 0.3374 Training Acc: 93.5720%\n",
      "[Epoch:  1/  5] [data: 10240/24026] Training Loss: 0.2051 Training Acc: 93.5840%\n",
      "[Epoch:  1/  5] [data: 10368/24026] Training Loss: 0.1727 Training Acc: 93.6150%\n",
      "[Epoch:  1/  5] [data: 10496/24026] Training Loss: 0.2383 Training Acc: 93.6166%\n",
      "[Epoch:  1/  5] [data: 10624/24026] Training Loss: 0.1863 Training Acc: 93.6276%\n",
      "[Epoch:  1/  5] [data: 10752/24026] Training Loss: 0.2450 Training Acc: 93.6291%\n",
      "[Epoch:  1/  5] [data: 10880/24026] Training Loss: 0.2041 Training Acc: 93.6213%\n",
      "[Epoch:  1/  5] [data: 11008/24026] Training Loss: 0.3623 Training Acc: 93.5774%\n",
      "[Epoch:  1/  5] [data: 11136/24026] Training Loss: 0.3054 Training Acc: 93.5435%\n",
      "[Epoch:  1/  5] [data: 11264/24026] Training Loss: 0.2168 Training Acc: 93.5458%\n",
      "[Epoch:  1/  5] [data: 11392/24026] Training Loss: 0.1316 Training Acc: 93.5832%\n",
      "[Epoch:  1/  5] [data: 11520/24026] Training Loss: 0.1799 Training Acc: 93.6024%\n",
      "[Epoch:  1/  5] [data: 11648/24026] Training Loss: 0.1597 Training Acc: 93.6212%\n",
      "[Epoch:  1/  5] [data: 11776/24026] Training Loss: 0.2290 Training Acc: 93.6226%\n",
      "[Epoch:  1/  5] [data: 11904/24026] Training Loss: 0.2584 Training Acc: 93.6072%\n",
      "[Epoch:  1/  5] [data: 12032/24026] Training Loss: 0.1983 Training Acc: 93.6170%\n",
      "[Epoch:  1/  5] [data: 12160/24026] Training Loss: 0.2941 Training Acc: 93.6020%\n",
      "[Epoch:  1/  5] [data: 12288/24026] Training Loss: 0.1700 Training Acc: 93.6198%\n",
      "[Epoch:  1/  5] [data: 12416/24026] Training Loss: 0.3016 Training Acc: 93.5889%\n",
      "[Epoch:  1/  5] [data: 12544/24026] Training Loss: 0.2039 Training Acc: 93.5906%\n",
      "[Epoch:  1/  5] [data: 12672/24026] Training Loss: 0.3042 Training Acc: 93.5685%\n",
      "[Epoch:  1/  5] [data: 12800/24026] Training Loss: 0.1979 Training Acc: 93.5781%\n",
      "[Epoch:  1/  5] [data: 12928/24026] Training Loss: 0.2080 Training Acc: 93.5876%\n",
      "[Epoch:  1/  5] [data: 13056/24026] Training Loss: 0.2024 Training Acc: 93.5968%\n",
      "[Epoch:  1/  5] [data: 13184/24026] Training Loss: 0.2191 Training Acc: 93.5983%\n",
      "[Epoch:  1/  5] [data: 13312/24026] Training Loss: 0.2194 Training Acc: 93.5998%\n",
      "[Epoch:  1/  5] [data: 13440/24026] Training Loss: 0.1934 Training Acc: 93.6012%\n",
      "[Epoch:  1/  5] [data: 13568/24026] Training Loss: 0.2369 Training Acc: 93.5952%\n",
      "[Epoch:  1/  5] [data: 13696/24026] Training Loss: 0.2492 Training Acc: 93.5894%\n",
      "[Epoch:  1/  5] [data: 13824/24026] Training Loss: 0.1918 Training Acc: 93.6053%\n",
      "[Epoch:  1/  5] [data: 13952/24026] Training Loss: 0.2410 Training Acc: 93.6067%\n",
      "[Epoch:  1/  5] [data: 14080/24026] Training Loss: 0.2513 Training Acc: 93.6009%\n",
      "[Epoch:  1/  5] [data: 14208/24026] Training Loss: 0.1374 Training Acc: 93.6303%\n",
      "[Epoch:  1/  5] [data: 14336/24026] Training Loss: 0.2279 Training Acc: 93.6314%\n",
      "[Epoch:  1/  5] [data: 14464/24026] Training Loss: 0.2336 Training Acc: 93.6325%\n",
      "[Epoch:  1/  5] [data: 14592/24026] Training Loss: 0.1569 Training Acc: 93.6541%\n",
      "[Epoch:  1/  5] [data: 14720/24026] Training Loss: 0.2033 Training Acc: 93.6617%\n",
      "[Epoch:  1/  5] [data: 14848/24026] Training Loss: 0.3383 Training Acc: 93.6220%\n",
      "[Epoch:  1/  5] [data: 14976/24026] Training Loss: 0.2865 Training Acc: 93.6031%\n",
      "[Epoch:  1/  5] [data: 15104/24026] Training Loss: 0.2750 Training Acc: 93.5911%\n",
      "[Epoch:  1/  5] [data: 15232/24026] Training Loss: 0.2555 Training Acc: 93.5859%\n",
      "[Epoch:  1/  5] [data: 15360/24026] Training Loss: 0.2753 Training Acc: 93.5742%\n",
      "[Epoch:  1/  5] [data: 15488/24026] Training Loss: 0.1613 Training Acc: 93.5950%\n",
      "[Epoch:  1/  5] [data: 15616/24026] Training Loss: 0.2522 Training Acc: 93.5899%\n",
      "[Epoch:  1/  5] [data: 15744/24026] Training Loss: 0.2045 Training Acc: 93.5976%\n",
      "[Epoch:  1/  5] [data: 15872/24026] Training Loss: 0.2910 Training Acc: 93.5799%\n",
      "[Epoch:  1/  5] [data: 16000/24026] Training Loss: 0.3623 Training Acc: 93.5500%\n",
      "[Epoch:  1/  5] [data: 16128/24026] Training Loss: 0.1767 Training Acc: 93.5640%\n",
      "[Epoch:  1/  5] [data: 16256/24026] Training Loss: 0.2491 Training Acc: 93.5593%\n",
      "[Epoch:  1/  5] [data: 16384/24026] Training Loss: 0.1985 Training Acc: 93.5669%\n",
      "[Epoch:  1/  5] [data: 16512/24026] Training Loss: 0.1776 Training Acc: 93.5804%\n",
      "[Epoch:  1/  5] [data: 16640/24026] Training Loss: 0.2036 Training Acc: 93.5817%\n",
      "[Epoch:  1/  5] [data: 16768/24026] Training Loss: 0.2730 Training Acc: 93.5651%\n",
      "[Epoch:  1/  5] [data: 16896/24026] Training Loss: 0.2013 Training Acc: 93.5724%\n",
      "[Epoch:  1/  5] [data: 17024/24026] Training Loss: 0.3690 Training Acc: 93.5385%\n",
      "[Epoch:  1/  5] [data: 17152/24026] Training Loss: 0.1600 Training Acc: 93.5576%\n",
      "[Epoch:  1/  5] [data: 17280/24026] Training Loss: 0.2243 Training Acc: 93.5590%\n",
      "[Epoch:  1/  5] [data: 17408/24026] Training Loss: 0.3864 Training Acc: 93.5202%\n",
      "[Epoch:  1/  5] [data: 17536/24026] Training Loss: 0.1797 Training Acc: 93.5276%\n",
      "[Epoch:  1/  5] [data: 17664/24026] Training Loss: 0.2287 Training Acc: 93.5292%\n",
      "[Epoch:  1/  5] [data: 17792/24026] Training Loss: 0.2144 Training Acc: 93.5308%\n",
      "[Epoch:  1/  5] [data: 17920/24026] Training Loss: 0.1907 Training Acc: 93.5435%\n",
      "[Epoch:  1/  5] [data: 18048/24026] Training Loss: 0.2543 Training Acc: 93.5339%\n",
      "[Epoch:  1/  5] [data: 18176/24026] Training Loss: 0.2772 Training Acc: 93.5189%\n",
      "[Epoch:  1/  5] [data: 18304/24026] Training Loss: 0.1508 Training Acc: 93.5369%\n",
      "[Epoch:  1/  5] [data: 18432/24026] Training Loss: 0.1896 Training Acc: 93.5438%\n",
      "[Epoch:  1/  5] [data: 18560/24026] Training Loss: 0.2198 Training Acc: 93.5453%\n",
      "[Epoch:  1/  5] [data: 18688/24026] Training Loss: 0.1977 Training Acc: 93.5520%\n",
      "[Epoch:  1/  5] [data: 18816/24026] Training Loss: 0.3141 Training Acc: 93.5321%\n",
      "[Epoch:  1/  5] [data: 18944/24026] Training Loss: 0.2922 Training Acc: 93.5177%\n",
      "[Epoch:  1/  5] [data: 19072/24026] Training Loss: 0.3708 Training Acc: 93.4878%\n",
      "[Epoch:  1/  5] [data: 19200/24026] Training Loss: 0.3352 Training Acc: 93.4688%\n",
      "[Epoch:  1/  5] [data: 19328/24026] Training Loss: 0.2417 Training Acc: 93.4706%\n",
      "[Epoch:  1/  5] [data: 19456/24026] Training Loss: 0.3810 Training Acc: 93.4416%\n",
      "[Epoch:  1/  5] [data: 19584/24026] Training Loss: 0.1945 Training Acc: 93.4487%\n",
      "[Epoch:  1/  5] [data: 19712/24026] Training Loss: 0.2889 Training Acc: 93.4355%\n",
      "[Epoch:  1/  5] [data: 19840/24026] Training Loss: 0.3753 Training Acc: 93.4073%\n",
      "[Epoch:  1/  5] [data: 19968/24026] Training Loss: 0.2048 Training Acc: 93.4145%\n",
      "[Epoch:  1/  5] [data: 20096/24026] Training Loss: 0.1770 Training Acc: 93.4266%\n",
      "[Epoch:  1/  5] [data: 20224/24026] Training Loss: 0.2533 Training Acc: 93.4187%\n",
      "[Epoch:  1/  5] [data: 20352/24026] Training Loss: 0.2932 Training Acc: 93.4061%\n",
      "[Epoch:  1/  5] [data: 20480/24026] Training Loss: 0.2145 Training Acc: 93.4082%\n",
      "[Epoch:  1/  5] [data: 20608/24026] Training Loss: 0.2068 Training Acc: 93.4152%\n",
      "[Epoch:  1/  5] [data: 20736/24026] Training Loss: 0.3466 Training Acc: 93.3931%\n",
      "[Epoch:  1/  5] [data: 20864/24026] Training Loss: 0.2224 Training Acc: 93.3953%\n",
      "[Epoch:  1/  5] [data: 20992/24026] Training Loss: 0.1965 Training Acc: 93.4022%\n",
      "[Epoch:  1/  5] [data: 21120/24026] Training Loss: 0.1241 Training Acc: 93.4280%\n",
      "[Epoch:  1/  5] [data: 21248/24026] Training Loss: 0.2278 Training Acc: 93.4300%\n",
      "[Epoch:  1/  5] [data: 21376/24026] Training Loss: 0.1483 Training Acc: 93.4459%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/  5] [data: 21504/24026] Training Loss: 0.1725 Training Acc: 93.4570%\n",
      "[Epoch:  1/  5] [data: 21632/24026] Training Loss: 0.1546 Training Acc: 93.4726%\n",
      "[Epoch:  1/  5] [data: 21760/24026] Training Loss: 0.2521 Training Acc: 93.4651%\n",
      "[Epoch:  1/  5] [data: 21888/24026] Training Loss: 0.1489 Training Acc: 93.4759%\n",
      "[Epoch:  1/  5] [data: 22016/24026] Training Loss: 0.2049 Training Acc: 93.4775%\n",
      "[Epoch:  1/  5] [data: 22144/24026] Training Loss: 0.2109 Training Acc: 93.4836%\n",
      "[Epoch:  1/  5] [data: 22272/24026] Training Loss: 0.1862 Training Acc: 93.4896%\n",
      "[Epoch:  1/  5] [data: 22400/24026] Training Loss: 0.2480 Training Acc: 93.4866%\n",
      "[Epoch:  1/  5] [data: 22528/24026] Training Loss: 0.2373 Training Acc: 93.4881%\n",
      "[Epoch:  1/  5] [data: 22656/24026] Training Loss: 0.2034 Training Acc: 93.4940%\n",
      "[Epoch:  1/  5] [data: 22784/24026] Training Loss: 0.2283 Training Acc: 93.4954%\n",
      "[Epoch:  1/  5] [data: 22912/24026] Training Loss: 0.3191 Training Acc: 93.4794%\n",
      "[Epoch:  1/  5] [data: 23040/24026] Training Loss: 0.2747 Training Acc: 93.4722%\n",
      "[Epoch:  1/  5] [data: 23168/24026] Training Loss: 0.2513 Training Acc: 93.4694%\n",
      "[Epoch:  1/  5] [data: 23296/24026] Training Loss: 0.2664 Training Acc: 93.4624%\n",
      "[Epoch:  1/  5] [data: 23424/24026] Training Loss: 0.1844 Training Acc: 93.4725%\n",
      "[Epoch:  1/  5] [data: 23552/24026] Training Loss: 0.2557 Training Acc: 93.4698%\n",
      "[Epoch:  1/  5] [data: 23680/24026] Training Loss: 0.1699 Training Acc: 93.4840%\n",
      "[Epoch:  1/  5] [data: 23808/24026] Training Loss: 0.2819 Training Acc: 93.4770%\n",
      "[Epoch:  1/  5] [data: 23936/24026] Training Loss: 0.2738 Training Acc: 93.4701%\n",
      "[Epoch:  1/  5] [data: 24026/24026] Training Loss: 0.1721 Training Acc: 93.4779%\n",
      "Testing-5...\n",
      "[Epoch:  1/  5] Validation Loss: 0.2301 Validation Acc: 93.5161%\n",
      "Time used: 2607.167178630829s\n",
      "[Epoch:  2/  5] [data: 128/24026] Training Loss: 0.3376 Training Acc: 90.6250%\n",
      "[Epoch:  2/  5] [data: 256/24026] Training Loss: 0.2496 Training Acc: 91.7969%\n",
      "[Epoch:  2/  5] [data: 384/24026] Training Loss: 0.2292 Training Acc: 92.4479%\n",
      "[Epoch:  2/  5] [data: 512/24026] Training Loss: 0.3506 Training Acc: 91.7969%\n",
      "[Epoch:  2/  5] [data: 640/24026] Training Loss: 0.1521 Training Acc: 92.6562%\n",
      "[Epoch:  2/  5] [data: 768/24026] Training Loss: 0.3151 Training Acc: 92.3177%\n",
      "[Epoch:  2/  5] [data: 896/24026] Training Loss: 0.2660 Training Acc: 92.4107%\n",
      "[Epoch:  2/  5] [data: 1024/24026] Training Loss: 0.2279 Training Acc: 92.5781%\n",
      "[Epoch:  2/  5] [data: 1152/24026] Training Loss: 0.1194 Training Acc: 93.1424%\n",
      "[Epoch:  2/  5] [data: 1280/24026] Training Loss: 0.1774 Training Acc: 93.2812%\n",
      "[Epoch:  2/  5] [data: 1408/24026] Training Loss: 0.1869 Training Acc: 93.3949%\n",
      "[Epoch:  2/  5] [data: 1536/24026] Training Loss: 0.3135 Training Acc: 93.1641%\n",
      "[Epoch:  2/  5] [data: 1664/24026] Training Loss: 0.2183 Training Acc: 93.2692%\n",
      "[Epoch:  2/  5] [data: 1792/24026] Training Loss: 0.2799 Training Acc: 93.1920%\n",
      "[Epoch:  2/  5] [data: 1920/24026] Training Loss: 0.2094 Training Acc: 93.2812%\n",
      "[Epoch:  2/  5] [data: 2048/24026] Training Loss: 0.2082 Training Acc: 93.3105%\n",
      "[Epoch:  2/  5] [data: 2176/24026] Training Loss: 0.1688 Training Acc: 93.4283%\n",
      "[Epoch:  2/  5] [data: 2304/24026] Training Loss: 0.1974 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 2432/24026] Training Loss: 0.2536 Training Acc: 93.4211%\n",
      "[Epoch:  2/  5] [data: 2560/24026] Training Loss: 0.1503 Training Acc: 93.5156%\n",
      "[Epoch:  2/  5] [data: 2688/24026] Training Loss: 0.2134 Training Acc: 93.5268%\n",
      "[Epoch:  2/  5] [data: 2816/24026] Training Loss: 0.1465 Training Acc: 93.6435%\n",
      "[Epoch:  2/  5] [data: 2944/24026] Training Loss: 0.2181 Training Acc: 93.6481%\n",
      "[Epoch:  2/  5] [data: 3072/24026] Training Loss: 0.4245 Training Acc: 93.3919%\n",
      "[Epoch:  2/  5] [data: 3200/24026] Training Loss: 0.1978 Training Acc: 93.4375%\n",
      "[Epoch:  2/  5] [data: 3328/24026] Training Loss: 0.2013 Training Acc: 93.4195%\n",
      "[Epoch:  2/  5] [data: 3456/24026] Training Loss: 0.2733 Training Acc: 93.3449%\n",
      "[Epoch:  2/  5] [data: 3584/24026] Training Loss: 0.2693 Training Acc: 93.3036%\n",
      "[Epoch:  2/  5] [data: 3712/24026] Training Loss: 0.1801 Training Acc: 93.3728%\n",
      "[Epoch:  2/  5] [data: 3840/24026] Training Loss: 0.2007 Training Acc: 93.4115%\n",
      "[Epoch:  2/  5] [data: 3968/24026] Training Loss: 0.0884 Training Acc: 93.5736%\n",
      "[Epoch:  2/  5] [data: 4096/24026] Training Loss: 0.1972 Training Acc: 93.6035%\n",
      "[Epoch:  2/  5] [data: 4224/24026] Training Loss: 0.2552 Training Acc: 93.5843%\n",
      "[Epoch:  2/  5] [data: 4352/24026] Training Loss: 0.2367 Training Acc: 93.5892%\n",
      "[Epoch:  2/  5] [data: 4480/24026] Training Loss: 0.3223 Training Acc: 93.5268%\n",
      "[Epoch:  2/  5] [data: 4608/24026] Training Loss: 0.2875 Training Acc: 93.4896%\n",
      "[Epoch:  2/  5] [data: 4736/24026] Training Loss: 0.1375 Training Acc: 93.5389%\n",
      "[Epoch:  2/  5] [data: 4864/24026] Training Loss: 0.1278 Training Acc: 93.6266%\n",
      "[Epoch:  2/  5] [data: 4992/24026] Training Loss: 0.3345 Training Acc: 93.5296%\n",
      "[Epoch:  2/  5] [data: 5120/24026] Training Loss: 0.2205 Training Acc: 93.5352%\n",
      "[Epoch:  2/  5] [data: 5248/24026] Training Loss: 0.1744 Training Acc: 93.5785%\n",
      "[Epoch:  2/  5] [data: 5376/24026] Training Loss: 0.2665 Training Acc: 93.5454%\n",
      "[Epoch:  2/  5] [data: 5504/24026] Training Loss: 0.1664 Training Acc: 93.6047%\n",
      "[Epoch:  2/  5] [data: 5632/24026] Training Loss: 0.2582 Training Acc: 93.5902%\n",
      "[Epoch:  2/  5] [data: 5760/24026] Training Loss: 0.3267 Training Acc: 93.5243%\n",
      "[Epoch:  2/  5] [data: 5888/24026] Training Loss: 0.1987 Training Acc: 93.5462%\n",
      "[Epoch:  2/  5] [data: 6016/24026] Training Loss: 0.1363 Training Acc: 93.6170%\n",
      "[Epoch:  2/  5] [data: 6144/24026] Training Loss: 0.1469 Training Acc: 93.6686%\n",
      "[Epoch:  2/  5] [data: 6272/24026] Training Loss: 0.1763 Training Acc: 93.7022%\n",
      "[Epoch:  2/  5] [data: 6400/24026] Training Loss: 0.2045 Training Acc: 93.7188%\n",
      "[Epoch:  2/  5] [data: 6528/24026] Training Loss: 0.2101 Training Acc: 93.7194%\n",
      "[Epoch:  2/  5] [data: 6656/24026] Training Loss: 0.1558 Training Acc: 93.7500%\n",
      "[Epoch:  2/  5] [data: 6784/24026] Training Loss: 0.2356 Training Acc: 93.7500%\n",
      "[Epoch:  2/  5] [data: 6912/24026] Training Loss: 0.3758 Training Acc: 93.6777%\n",
      "[Epoch:  2/  5] [data: 7040/24026] Training Loss: 0.1988 Training Acc: 93.6932%\n",
      "[Epoch:  2/  5] [data: 7168/24026] Training Loss: 0.2320 Training Acc: 93.6942%\n",
      "[Epoch:  2/  5] [data: 7296/24026] Training Loss: 0.2431 Training Acc: 93.6815%\n",
      "[Epoch:  2/  5] [data: 7424/24026] Training Loss: 0.2185 Training Acc: 93.6692%\n",
      "[Epoch:  2/  5] [data: 7552/24026] Training Loss: 0.1334 Training Acc: 93.7235%\n",
      "[Epoch:  2/  5] [data: 7680/24026] Training Loss: 0.1856 Training Acc: 93.7500%\n",
      "[Epoch:  2/  5] [data: 7808/24026] Training Loss: 0.2501 Training Acc: 93.7372%\n",
      "[Epoch:  2/  5] [data: 7936/24026] Training Loss: 0.2137 Training Acc: 93.7374%\n",
      "[Epoch:  2/  5] [data: 8064/24026] Training Loss: 0.2684 Training Acc: 93.7128%\n",
      "[Epoch:  2/  5] [data: 8192/24026] Training Loss: 0.2001 Training Acc: 93.7256%\n",
      "[Epoch:  2/  5] [data: 8320/24026] Training Loss: 0.1064 Training Acc: 93.7861%\n",
      "[Epoch:  2/  5] [data: 8448/24026] Training Loss: 0.2681 Training Acc: 93.7618%\n",
      "[Epoch:  2/  5] [data: 8576/24026] Training Loss: 0.1761 Training Acc: 93.7850%\n",
      "[Epoch:  2/  5] [data: 8704/24026] Training Loss: 0.1931 Training Acc: 93.7960%\n",
      "[Epoch:  2/  5] [data: 8832/24026] Training Loss: 0.1271 Training Acc: 93.8406%\n",
      "[Epoch:  2/  5] [data: 8960/24026] Training Loss: 0.1496 Training Acc: 93.8728%\n",
      "[Epoch:  2/  5] [data: 9088/24026] Training Loss: 0.3860 Training Acc: 93.7830%\n",
      "[Epoch:  2/  5] [data: 9216/24026] Training Loss: 0.1259 Training Acc: 93.8151%\n",
      "[Epoch:  2/  5] [data: 9344/24026] Training Loss: 0.0765 Training Acc: 93.8784%\n",
      "[Epoch:  2/  5] [data: 9472/24026] Training Loss: 0.2062 Training Acc: 93.8872%\n",
      "[Epoch:  2/  5] [data: 9600/24026] Training Loss: 0.2623 Training Acc: 93.8646%\n",
      "[Epoch:  2/  5] [data: 9728/24026] Training Loss: 0.2529 Training Acc: 93.8528%\n",
      "[Epoch:  2/  5] [data: 9856/24026] Training Loss: 0.2974 Training Acc: 93.8210%\n",
      "[Epoch:  2/  5] [data: 9984/24026] Training Loss: 0.3900 Training Acc: 93.7400%\n",
      "[Epoch:  2/  5] [data: 10112/24026] Training Loss: 0.3325 Training Acc: 93.6907%\n",
      "[Epoch:  2/  5] [data: 10240/24026] Training Loss: 0.2103 Training Acc: 93.7012%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 10368/24026] Training Loss: 0.1793 Training Acc: 93.7307%\n",
      "[Epoch:  2/  5] [data: 10496/24026] Training Loss: 0.2385 Training Acc: 93.7309%\n",
      "[Epoch:  2/  5] [data: 10624/24026] Training Loss: 0.1830 Training Acc: 93.7406%\n",
      "[Epoch:  2/  5] [data: 10752/24026] Training Loss: 0.2377 Training Acc: 93.7407%\n",
      "[Epoch:  2/  5] [data: 10880/24026] Training Loss: 0.2127 Training Acc: 93.7316%\n",
      "[Epoch:  2/  5] [data: 11008/24026] Training Loss: 0.3669 Training Acc: 93.6864%\n",
      "[Epoch:  2/  5] [data: 11136/24026] Training Loss: 0.3059 Training Acc: 93.6512%\n",
      "[Epoch:  2/  5] [data: 11264/24026] Training Loss: 0.2274 Training Acc: 93.6435%\n",
      "[Epoch:  2/  5] [data: 11392/24026] Training Loss: 0.1292 Training Acc: 93.6798%\n",
      "[Epoch:  2/  5] [data: 11520/24026] Training Loss: 0.1792 Training Acc: 93.6979%\n",
      "[Epoch:  2/  5] [data: 11648/24026] Training Loss: 0.1633 Training Acc: 93.7157%\n",
      "[Epoch:  2/  5] [data: 11776/24026] Training Loss: 0.2247 Training Acc: 93.7160%\n",
      "[Epoch:  2/  5] [data: 11904/24026] Training Loss: 0.2517 Training Acc: 93.7080%\n",
      "[Epoch:  2/  5] [data: 12032/24026] Training Loss: 0.2052 Training Acc: 93.7168%\n",
      "[Epoch:  2/  5] [data: 12160/24026] Training Loss: 0.2844 Training Acc: 93.7007%\n",
      "[Epoch:  2/  5] [data: 12288/24026] Training Loss: 0.1604 Training Acc: 93.7256%\n",
      "[Epoch:  2/  5] [data: 12416/24026] Training Loss: 0.3128 Training Acc: 93.6936%\n",
      "[Epoch:  2/  5] [data: 12544/24026] Training Loss: 0.2028 Training Acc: 93.6942%\n",
      "[Epoch:  2/  5] [data: 12672/24026] Training Loss: 0.3017 Training Acc: 93.6711%\n",
      "[Epoch:  2/  5] [data: 12800/24026] Training Loss: 0.1984 Training Acc: 93.6797%\n",
      "[Epoch:  2/  5] [data: 12928/24026] Training Loss: 0.2069 Training Acc: 93.6881%\n",
      "[Epoch:  2/  5] [data: 13056/24026] Training Loss: 0.2001 Training Acc: 93.6964%\n",
      "[Epoch:  2/  5] [data: 13184/24026] Training Loss: 0.2162 Training Acc: 93.6969%\n",
      "[Epoch:  2/  5] [data: 13312/24026] Training Loss: 0.2221 Training Acc: 93.6974%\n",
      "[Epoch:  2/  5] [data: 13440/24026] Training Loss: 0.1895 Training Acc: 93.6979%\n",
      "[Epoch:  2/  5] [data: 13568/24026] Training Loss: 0.2341 Training Acc: 93.6837%\n",
      "[Epoch:  2/  5] [data: 13696/24026] Training Loss: 0.2484 Training Acc: 93.6770%\n",
      "[Epoch:  2/  5] [data: 13824/24026] Training Loss: 0.1837 Training Acc: 93.6921%\n",
      "[Epoch:  2/  5] [data: 13952/24026] Training Loss: 0.2379 Training Acc: 93.6855%\n",
      "[Epoch:  2/  5] [data: 14080/24026] Training Loss: 0.2512 Training Acc: 93.6790%\n",
      "[Epoch:  2/  5] [data: 14208/24026] Training Loss: 0.1274 Training Acc: 93.7078%\n",
      "[Epoch:  2/  5] [data: 14336/24026] Training Loss: 0.2331 Training Acc: 93.7081%\n",
      "[Epoch:  2/  5] [data: 14464/24026] Training Loss: 0.2298 Training Acc: 93.7085%\n",
      "[Epoch:  2/  5] [data: 14592/24026] Training Loss: 0.1535 Training Acc: 93.7294%\n",
      "[Epoch:  2/  5] [data: 14720/24026] Training Loss: 0.2020 Training Acc: 93.7364%\n",
      "[Epoch:  2/  5] [data: 14848/24026] Training Loss: 0.3281 Training Acc: 93.7096%\n",
      "[Epoch:  2/  5] [data: 14976/24026] Training Loss: 0.2715 Training Acc: 93.6966%\n",
      "[Epoch:  2/  5] [data: 15104/24026] Training Loss: 0.2752 Training Acc: 93.6838%\n",
      "[Epoch:  2/  5] [data: 15232/24026] Training Loss: 0.2507 Training Acc: 93.6778%\n",
      "[Epoch:  2/  5] [data: 15360/24026] Training Loss: 0.2753 Training Acc: 93.6654%\n",
      "[Epoch:  2/  5] [data: 15488/24026] Training Loss: 0.1561 Training Acc: 93.6854%\n",
      "[Epoch:  2/  5] [data: 15616/24026] Training Loss: 0.2529 Training Acc: 93.6796%\n",
      "[Epoch:  2/  5] [data: 15744/24026] Training Loss: 0.2054 Training Acc: 93.6865%\n",
      "[Epoch:  2/  5] [data: 15872/24026] Training Loss: 0.2964 Training Acc: 93.6681%\n",
      "[Epoch:  2/  5] [data: 16000/24026] Training Loss: 0.3573 Training Acc: 93.6375%\n",
      "[Epoch:  2/  5] [data: 16128/24026] Training Loss: 0.1774 Training Acc: 93.6508%\n",
      "[Epoch:  2/  5] [data: 16256/24026] Training Loss: 0.2475 Training Acc: 93.6454%\n",
      "[Epoch:  2/  5] [data: 16384/24026] Training Loss: 0.1932 Training Acc: 93.6523%\n",
      "[Epoch:  2/  5] [data: 16512/24026] Training Loss: 0.1738 Training Acc: 93.6652%\n",
      "[Epoch:  2/  5] [data: 16640/24026] Training Loss: 0.2016 Training Acc: 93.6719%\n",
      "[Epoch:  2/  5] [data: 16768/24026] Training Loss: 0.2683 Training Acc: 93.6605%\n",
      "[Epoch:  2/  5] [data: 16896/24026] Training Loss: 0.2018 Training Acc: 93.6671%\n",
      "[Epoch:  2/  5] [data: 17024/24026] Training Loss: 0.3682 Training Acc: 93.6325%\n",
      "[Epoch:  2/  5] [data: 17152/24026] Training Loss: 0.1517 Training Acc: 93.6509%\n",
      "[Epoch:  2/  5] [data: 17280/24026] Training Loss: 0.2275 Training Acc: 93.6516%\n",
      "[Epoch:  2/  5] [data: 17408/24026] Training Loss: 0.3955 Training Acc: 93.6121%\n",
      "[Epoch:  2/  5] [data: 17536/24026] Training Loss: 0.1714 Training Acc: 93.6245%\n",
      "[Epoch:  2/  5] [data: 17664/24026] Training Loss: 0.2252 Training Acc: 93.6255%\n",
      "[Epoch:  2/  5] [data: 17792/24026] Training Loss: 0.2065 Training Acc: 93.6263%\n",
      "[Epoch:  2/  5] [data: 17920/24026] Training Loss: 0.1830 Training Acc: 93.6384%\n",
      "[Epoch:  2/  5] [data: 18048/24026] Training Loss: 0.2462 Training Acc: 93.6336%\n",
      "[Epoch:  2/  5] [data: 18176/24026] Training Loss: 0.2738 Training Acc: 93.6180%\n",
      "[Epoch:  2/  5] [data: 18304/24026] Training Loss: 0.1478 Training Acc: 93.6353%\n",
      "[Epoch:  2/  5] [data: 18432/24026] Training Loss: 0.1831 Training Acc: 93.6469%\n",
      "[Epoch:  2/  5] [data: 18560/24026] Training Loss: 0.2172 Training Acc: 93.6476%\n",
      "[Epoch:  2/  5] [data: 18688/24026] Training Loss: 0.1976 Training Acc: 93.6537%\n",
      "[Epoch:  2/  5] [data: 18816/24026] Training Loss: 0.3157 Training Acc: 93.6331%\n",
      "[Epoch:  2/  5] [data: 18944/24026] Training Loss: 0.3075 Training Acc: 93.6180%\n",
      "[Epoch:  2/  5] [data: 19072/24026] Training Loss: 0.3696 Training Acc: 93.5875%\n",
      "[Epoch:  2/  5] [data: 19200/24026] Training Loss: 0.3233 Training Acc: 93.5677%\n",
      "[Epoch:  2/  5] [data: 19328/24026] Training Loss: 0.2285 Training Acc: 93.5689%\n",
      "[Epoch:  2/  5] [data: 19456/24026] Training Loss: 0.3731 Training Acc: 93.5393%\n",
      "[Epoch:  2/  5] [data: 19584/24026] Training Loss: 0.2013 Training Acc: 93.5458%\n",
      "[Epoch:  2/  5] [data: 19712/24026] Training Loss: 0.2932 Training Acc: 93.5319%\n",
      "[Epoch:  2/  5] [data: 19840/24026] Training Loss: 0.3565 Training Acc: 93.5030%\n",
      "[Epoch:  2/  5] [data: 19968/24026] Training Loss: 0.2057 Training Acc: 93.5096%\n",
      "[Epoch:  2/  5] [data: 20096/24026] Training Loss: 0.1735 Training Acc: 93.5211%\n",
      "[Epoch:  2/  5] [data: 20224/24026] Training Loss: 0.2500 Training Acc: 93.5176%\n",
      "[Epoch:  2/  5] [data: 20352/24026] Training Loss: 0.2887 Training Acc: 93.5043%\n",
      "[Epoch:  2/  5] [data: 20480/24026] Training Loss: 0.2020 Training Acc: 93.5107%\n",
      "[Epoch:  2/  5] [data: 20608/24026] Training Loss: 0.1992 Training Acc: 93.5171%\n",
      "[Epoch:  2/  5] [data: 20736/24026] Training Loss: 0.3603 Training Acc: 93.4944%\n",
      "[Epoch:  2/  5] [data: 20864/24026] Training Loss: 0.2216 Training Acc: 93.4960%\n",
      "[Epoch:  2/  5] [data: 20992/24026] Training Loss: 0.1928 Training Acc: 93.5023%\n",
      "[Epoch:  2/  5] [data: 21120/24026] Training Loss: 0.1089 Training Acc: 93.5275%\n",
      "[Epoch:  2/  5] [data: 21248/24026] Training Loss: 0.2273 Training Acc: 93.5288%\n",
      "[Epoch:  2/  5] [data: 21376/24026] Training Loss: 0.1474 Training Acc: 93.5442%\n",
      "[Epoch:  2/  5] [data: 21504/24026] Training Loss: 0.1729 Training Acc: 93.5547%\n",
      "[Epoch:  2/  5] [data: 21632/24026] Training Loss: 0.1524 Training Acc: 93.5697%\n",
      "[Epoch:  2/  5] [data: 21760/24026] Training Loss: 0.2423 Training Acc: 93.5616%\n",
      "[Epoch:  2/  5] [data: 21888/24026] Training Loss: 0.1480 Training Acc: 93.5764%\n",
      "[Epoch:  2/  5] [data: 22016/24026] Training Loss: 0.1992 Training Acc: 93.5774%\n",
      "[Epoch:  2/  5] [data: 22144/24026] Training Loss: 0.2083 Training Acc: 93.5829%\n",
      "[Epoch:  2/  5] [data: 22272/24026] Training Loss: 0.1864 Training Acc: 93.5884%\n",
      "[Epoch:  2/  5] [data: 22400/24026] Training Loss: 0.2472 Training Acc: 93.5848%\n",
      "[Epoch:  2/  5] [data: 22528/24026] Training Loss: 0.2390 Training Acc: 93.5858%\n",
      "[Epoch:  2/  5] [data: 22656/24026] Training Loss: 0.2026 Training Acc: 93.5911%\n",
      "[Epoch:  2/  5] [data: 22784/24026] Training Loss: 0.2286 Training Acc: 93.5920%\n",
      "[Epoch:  2/  5] [data: 22912/24026] Training Loss: 0.3234 Training Acc: 93.5711%\n",
      "[Epoch:  2/  5] [data: 23040/24026] Training Loss: 0.2750 Training Acc: 93.5634%\n",
      "[Epoch:  2/  5] [data: 23168/24026] Training Loss: 0.2492 Training Acc: 93.5601%\n",
      "[Epoch:  2/  5] [data: 23296/24026] Training Loss: 0.2658 Training Acc: 93.5525%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  2/  5] [data: 23424/24026] Training Loss: 0.1796 Training Acc: 93.5622%\n",
      "[Epoch:  2/  5] [data: 23552/24026] Training Loss: 0.2550 Training Acc: 93.5589%\n",
      "[Epoch:  2/  5] [data: 23680/24026] Training Loss: 0.1809 Training Acc: 93.5726%\n",
      "[Epoch:  2/  5] [data: 23808/24026] Training Loss: 0.2823 Training Acc: 93.5652%\n",
      "[Epoch:  2/  5] [data: 23936/24026] Training Loss: 0.2724 Training Acc: 93.5578%\n",
      "[Epoch:  2/  5] [data: 24026/24026] Training Loss: 0.1712 Training Acc: 93.5653%\n",
      "Testing-5...\n",
      "[Epoch:  2/  5] Validation Loss: 0.2232 Validation Acc: 93.7289%\n",
      "Time used: 2608.227438926697s\n",
      "[Epoch:  3/  5] [data: 128/24026] Training Loss: 0.3228 Training Acc: 90.6250%\n",
      "[Epoch:  3/  5] [data: 256/24026] Training Loss: 0.2439 Training Acc: 91.7969%\n",
      "[Epoch:  3/  5] [data: 384/24026] Training Loss: 0.2192 Training Acc: 92.4479%\n",
      "[Epoch:  3/  5] [data: 512/24026] Training Loss: 0.3549 Training Acc: 91.7969%\n",
      "[Epoch:  3/  5] [data: 640/24026] Training Loss: 0.1482 Training Acc: 92.6562%\n",
      "[Epoch:  3/  5] [data: 768/24026] Training Loss: 0.3163 Training Acc: 92.3177%\n",
      "[Epoch:  3/  5] [data: 896/24026] Training Loss: 0.2557 Training Acc: 92.4107%\n",
      "[Epoch:  3/  5] [data: 1024/24026] Training Loss: 0.2248 Training Acc: 92.5781%\n",
      "[Epoch:  3/  5] [data: 1152/24026] Training Loss: 0.1089 Training Acc: 93.1424%\n",
      "[Epoch:  3/  5] [data: 1280/24026] Training Loss: 0.1714 Training Acc: 93.3594%\n",
      "[Epoch:  3/  5] [data: 1408/24026] Training Loss: 0.1823 Training Acc: 93.4659%\n",
      "[Epoch:  3/  5] [data: 1536/24026] Training Loss: 0.3149 Training Acc: 93.2292%\n",
      "[Epoch:  3/  5] [data: 1664/24026] Training Loss: 0.2124 Training Acc: 93.3293%\n",
      "[Epoch:  3/  5] [data: 1792/24026] Training Loss: 0.2729 Training Acc: 93.2478%\n",
      "[Epoch:  3/  5] [data: 1920/24026] Training Loss: 0.2071 Training Acc: 93.3333%\n",
      "[Epoch:  3/  5] [data: 2048/24026] Training Loss: 0.2041 Training Acc: 93.4082%\n",
      "[Epoch:  3/  5] [data: 2176/24026] Training Loss: 0.1712 Training Acc: 93.5202%\n",
      "[Epoch:  3/  5] [data: 2304/24026] Training Loss: 0.1944 Training Acc: 93.5764%\n",
      "[Epoch:  3/  5] [data: 2432/24026] Training Loss: 0.2472 Training Acc: 93.5444%\n",
      "[Epoch:  3/  5] [data: 2560/24026] Training Loss: 0.1461 Training Acc: 93.6719%\n",
      "[Epoch:  3/  5] [data: 2688/24026] Training Loss: 0.2103 Training Acc: 93.6756%\n",
      "[Epoch:  3/  5] [data: 2816/24026] Training Loss: 0.1450 Training Acc: 93.7855%\n",
      "[Epoch:  3/  5] [data: 2944/24026] Training Loss: 0.2098 Training Acc: 93.7840%\n",
      "[Epoch:  3/  5] [data: 3072/24026] Training Loss: 0.4265 Training Acc: 93.5221%\n",
      "[Epoch:  3/  5] [data: 3200/24026] Training Loss: 0.2002 Training Acc: 93.5625%\n",
      "[Epoch:  3/  5] [data: 3328/24026] Training Loss: 0.1979 Training Acc: 93.5397%\n",
      "[Epoch:  3/  5] [data: 3456/24026] Training Loss: 0.2743 Training Acc: 93.4606%\n",
      "[Epoch:  3/  5] [data: 3584/24026] Training Loss: 0.2696 Training Acc: 93.4152%\n",
      "[Epoch:  3/  5] [data: 3712/24026] Training Loss: 0.1778 Training Acc: 93.4806%\n",
      "[Epoch:  3/  5] [data: 3840/24026] Training Loss: 0.2025 Training Acc: 93.5156%\n",
      "[Epoch:  3/  5] [data: 3968/24026] Training Loss: 0.0945 Training Acc: 93.6744%\n",
      "[Epoch:  3/  5] [data: 4096/24026] Training Loss: 0.1972 Training Acc: 93.7012%\n",
      "[Epoch:  3/  5] [data: 4224/24026] Training Loss: 0.2498 Training Acc: 93.6790%\n",
      "[Epoch:  3/  5] [data: 4352/24026] Training Loss: 0.2301 Training Acc: 93.6811%\n",
      "[Epoch:  3/  5] [data: 4480/24026] Training Loss: 0.3130 Training Acc: 93.6161%\n",
      "[Epoch:  3/  5] [data: 4608/24026] Training Loss: 0.2833 Training Acc: 93.5764%\n",
      "[Epoch:  3/  5] [data: 4736/24026] Training Loss: 0.1384 Training Acc: 93.6022%\n",
      "[Epoch:  3/  5] [data: 4864/24026] Training Loss: 0.1267 Training Acc: 93.6883%\n",
      "[Epoch:  3/  5] [data: 4992/24026] Training Loss: 0.3390 Training Acc: 93.5897%\n",
      "[Epoch:  3/  5] [data: 5120/24026] Training Loss: 0.2124 Training Acc: 93.5938%\n",
      "[Epoch:  3/  5] [data: 5248/24026] Training Loss: 0.1742 Training Acc: 93.6357%\n",
      "[Epoch:  3/  5] [data: 5376/24026] Training Loss: 0.2659 Training Acc: 93.6012%\n",
      "[Epoch:  3/  5] [data: 5504/24026] Training Loss: 0.1587 Training Acc: 93.6592%\n",
      "[Epoch:  3/  5] [data: 5632/24026] Training Loss: 0.2552 Training Acc: 93.6435%\n",
      "[Epoch:  3/  5] [data: 5760/24026] Training Loss: 0.3262 Training Acc: 93.5764%\n",
      "[Epoch:  3/  5] [data: 5888/24026] Training Loss: 0.1978 Training Acc: 93.5971%\n",
      "[Epoch:  3/  5] [data: 6016/24026] Training Loss: 0.1440 Training Acc: 93.6669%\n",
      "[Epoch:  3/  5] [data: 6144/24026] Training Loss: 0.1536 Training Acc: 93.7174%\n",
      "[Epoch:  3/  5] [data: 6272/24026] Training Loss: 0.1773 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 6400/24026] Training Loss: 0.2014 Training Acc: 93.7656%\n",
      "[Epoch:  3/  5] [data: 6528/24026] Training Loss: 0.2030 Training Acc: 93.7653%\n",
      "[Epoch:  3/  5] [data: 6656/24026] Training Loss: 0.1498 Training Acc: 93.7951%\n",
      "[Epoch:  3/  5] [data: 6784/24026] Training Loss: 0.2273 Training Acc: 93.7942%\n",
      "[Epoch:  3/  5] [data: 6912/24026] Training Loss: 0.3644 Training Acc: 93.7211%\n",
      "[Epoch:  3/  5] [data: 7040/24026] Training Loss: 0.1969 Training Acc: 93.7358%\n",
      "[Epoch:  3/  5] [data: 7168/24026] Training Loss: 0.2385 Training Acc: 93.7221%\n",
      "[Epoch:  3/  5] [data: 7296/24026] Training Loss: 0.2445 Training Acc: 93.7089%\n",
      "[Epoch:  3/  5] [data: 7424/24026] Training Loss: 0.2239 Training Acc: 93.6961%\n",
      "[Epoch:  3/  5] [data: 7552/24026] Training Loss: 0.1246 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 7680/24026] Training Loss: 0.1787 Training Acc: 93.7760%\n",
      "[Epoch:  3/  5] [data: 7808/24026] Training Loss: 0.2415 Training Acc: 93.7628%\n",
      "[Epoch:  3/  5] [data: 7936/24026] Training Loss: 0.2091 Training Acc: 93.7626%\n",
      "[Epoch:  3/  5] [data: 8064/24026] Training Loss: 0.2695 Training Acc: 93.7376%\n",
      "[Epoch:  3/  5] [data: 8192/24026] Training Loss: 0.1976 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 8320/24026] Training Loss: 0.1106 Training Acc: 93.8101%\n",
      "[Epoch:  3/  5] [data: 8448/24026] Training Loss: 0.2651 Training Acc: 93.7855%\n",
      "[Epoch:  3/  5] [data: 8576/24026] Training Loss: 0.1792 Training Acc: 93.8083%\n",
      "[Epoch:  3/  5] [data: 8704/24026] Training Loss: 0.1966 Training Acc: 93.8189%\n",
      "[Epoch:  3/  5] [data: 8832/24026] Training Loss: 0.1287 Training Acc: 93.8632%\n",
      "[Epoch:  3/  5] [data: 8960/24026] Training Loss: 0.1512 Training Acc: 93.8951%\n",
      "[Epoch:  3/  5] [data: 9088/24026] Training Loss: 0.3810 Training Acc: 93.8050%\n",
      "[Epoch:  3/  5] [data: 9216/24026] Training Loss: 0.1431 Training Acc: 93.8368%\n",
      "[Epoch:  3/  5] [data: 9344/24026] Training Loss: 0.0777 Training Acc: 93.8998%\n",
      "[Epoch:  3/  5] [data: 9472/24026] Training Loss: 0.2077 Training Acc: 93.9084%\n",
      "[Epoch:  3/  5] [data: 9600/24026] Training Loss: 0.2653 Training Acc: 93.8854%\n",
      "[Epoch:  3/  5] [data: 9728/24026] Training Loss: 0.2576 Training Acc: 93.8734%\n",
      "[Epoch:  3/  5] [data: 9856/24026] Training Loss: 0.2994 Training Acc: 93.8312%\n",
      "[Epoch:  3/  5] [data: 9984/24026] Training Loss: 0.4010 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 10112/24026] Training Loss: 0.3411 Training Acc: 93.7006%\n",
      "[Epoch:  3/  5] [data: 10240/24026] Training Loss: 0.1965 Training Acc: 93.7109%\n",
      "[Epoch:  3/  5] [data: 10368/24026] Training Loss: 0.1657 Training Acc: 93.7404%\n",
      "[Epoch:  3/  5] [data: 10496/24026] Training Loss: 0.2390 Training Acc: 93.7405%\n",
      "[Epoch:  3/  5] [data: 10624/24026] Training Loss: 0.1888 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 10752/24026] Training Loss: 0.2434 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 10880/24026] Training Loss: 0.2107 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 11008/24026] Training Loss: 0.3469 Training Acc: 93.7046%\n",
      "[Epoch:  3/  5] [data: 11136/24026] Training Loss: 0.2986 Training Acc: 93.6782%\n",
      "[Epoch:  3/  5] [data: 11264/24026] Training Loss: 0.2114 Training Acc: 93.6790%\n",
      "[Epoch:  3/  5] [data: 11392/24026] Training Loss: 0.1296 Training Acc: 93.7149%\n",
      "[Epoch:  3/  5] [data: 11520/24026] Training Loss: 0.1787 Training Acc: 93.7326%\n",
      "[Epoch:  3/  5] [data: 11648/24026] Training Loss: 0.1567 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 11776/24026] Training Loss: 0.2226 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 11904/24026] Training Loss: 0.2530 Training Acc: 93.7416%\n",
      "[Epoch:  3/  5] [data: 12032/24026] Training Loss: 0.1988 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 12160/24026] Training Loss: 0.2855 Training Acc: 93.7336%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  3/  5] [data: 12288/24026] Training Loss: 0.1582 Training Acc: 93.7500%\n",
      "[Epoch:  3/  5] [data: 12416/24026] Training Loss: 0.3099 Training Acc: 93.7178%\n",
      "[Epoch:  3/  5] [data: 12544/24026] Training Loss: 0.2025 Training Acc: 93.7181%\n",
      "[Epoch:  3/  5] [data: 12672/24026] Training Loss: 0.3056 Training Acc: 93.6948%\n",
      "[Epoch:  3/  5] [data: 12800/24026] Training Loss: 0.1974 Training Acc: 93.7031%\n",
      "[Epoch:  3/  5] [data: 12928/24026] Training Loss: 0.1946 Training Acc: 93.7113%\n",
      "[Epoch:  3/  5] [data: 13056/24026] Training Loss: 0.2046 Training Acc: 93.7194%\n",
      "[Epoch:  3/  5] [data: 13184/24026] Training Loss: 0.2154 Training Acc: 93.7197%\n",
      "[Epoch:  3/  5] [data: 13312/24026] Training Loss: 0.2235 Training Acc: 93.7200%\n",
      "[Epoch:  3/  5] [data: 13440/24026] Training Loss: 0.1848 Training Acc: 93.7351%\n",
      "[Epoch:  3/  5] [data: 13568/24026] Training Loss: 0.2354 Training Acc: 93.7205%\n",
      "[Epoch:  3/  5] [data: 13696/24026] Training Loss: 0.2434 Training Acc: 93.7135%\n",
      "[Epoch:  3/  5] [data: 13824/24026] Training Loss: 0.1810 Training Acc: 93.7283%\n",
      "[Epoch:  3/  5] [data: 13952/24026] Training Loss: 0.2339 Training Acc: 93.7285%\n",
      "[Epoch:  3/  5] [data: 14080/24026] Training Loss: 0.2469 Training Acc: 93.7216%\n",
      "[Epoch:  3/  5] [data: 14208/24026] Training Loss: 0.1440 Training Acc: 93.7430%\n",
      "[Epoch:  3/  5] [data: 14336/24026] Training Loss: 0.2289 Training Acc: 93.7430%\n",
      "[Epoch:  3/  5] [data: 14464/24026] Training Loss: 0.2310 Training Acc: 93.7431%\n",
      "[Epoch:  3/  5] [data: 14592/24026] Training Loss: 0.1520 Training Acc: 93.7637%\n",
      "[Epoch:  3/  5] [data: 14720/24026] Training Loss: 0.2006 Training Acc: 93.7704%\n",
      "[Epoch:  3/  5] [data: 14848/24026] Training Loss: 0.3295 Training Acc: 93.7365%\n",
      "[Epoch:  3/  5] [data: 14976/24026] Training Loss: 0.2714 Training Acc: 93.7233%\n",
      "[Epoch:  3/  5] [data: 15104/24026] Training Loss: 0.2772 Training Acc: 93.7103%\n",
      "[Epoch:  3/  5] [data: 15232/24026] Training Loss: 0.2537 Training Acc: 93.7040%\n",
      "[Epoch:  3/  5] [data: 15360/24026] Training Loss: 0.2750 Training Acc: 93.6914%\n",
      "[Epoch:  3/  5] [data: 15488/24026] Training Loss: 0.1551 Training Acc: 93.7113%\n",
      "[Epoch:  3/  5] [data: 15616/24026] Training Loss: 0.2561 Training Acc: 93.7052%\n",
      "[Epoch:  3/  5] [data: 15744/24026] Training Loss: 0.2062 Training Acc: 93.7119%\n",
      "[Epoch:  3/  5] [data: 15872/24026] Training Loss: 0.3000 Training Acc: 93.6933%\n",
      "[Epoch:  3/  5] [data: 16000/24026] Training Loss: 0.3551 Training Acc: 93.6625%\n",
      "[Epoch:  3/  5] [data: 16128/24026] Training Loss: 0.1819 Training Acc: 93.6756%\n",
      "[Epoch:  3/  5] [data: 16256/24026] Training Loss: 0.2450 Training Acc: 93.6700%\n",
      "[Epoch:  3/  5] [data: 16384/24026] Training Loss: 0.1941 Training Acc: 93.6768%\n",
      "[Epoch:  3/  5] [data: 16512/24026] Training Loss: 0.1765 Training Acc: 93.6894%\n",
      "[Epoch:  3/  5] [data: 16640/24026] Training Loss: 0.2029 Training Acc: 93.6959%\n",
      "[Epoch:  3/  5] [data: 16768/24026] Training Loss: 0.2681 Training Acc: 93.6844%\n",
      "[Epoch:  3/  5] [data: 16896/24026] Training Loss: 0.2026 Training Acc: 93.6908%\n",
      "[Epoch:  3/  5] [data: 17024/24026] Training Loss: 0.3712 Training Acc: 93.6560%\n",
      "[Epoch:  3/  5] [data: 17152/24026] Training Loss: 0.1508 Training Acc: 93.6742%\n",
      "[Epoch:  3/  5] [data: 17280/24026] Training Loss: 0.2226 Training Acc: 93.6748%\n",
      "[Epoch:  3/  5] [data: 17408/24026] Training Loss: 0.3908 Training Acc: 93.6351%\n",
      "[Epoch:  3/  5] [data: 17536/24026] Training Loss: 0.1710 Training Acc: 93.6474%\n",
      "[Epoch:  3/  5] [data: 17664/24026] Training Loss: 0.2252 Training Acc: 93.6481%\n",
      "[Epoch:  3/  5] [data: 17792/24026] Training Loss: 0.2060 Training Acc: 93.6488%\n",
      "[Epoch:  3/  5] [data: 17920/24026] Training Loss: 0.1806 Training Acc: 93.6607%\n",
      "[Epoch:  3/  5] [data: 18048/24026] Training Loss: 0.2461 Training Acc: 93.6558%\n",
      "[Epoch:  3/  5] [data: 18176/24026] Training Loss: 0.2705 Training Acc: 93.6455%\n",
      "[Epoch:  3/  5] [data: 18304/24026] Training Loss: 0.1457 Training Acc: 93.6626%\n",
      "[Epoch:  3/  5] [data: 18432/24026] Training Loss: 0.1849 Training Acc: 93.6740%\n",
      "[Epoch:  3/  5] [data: 18560/24026] Training Loss: 0.2194 Training Acc: 93.6746%\n",
      "[Epoch:  3/  5] [data: 18688/24026] Training Loss: 0.1976 Training Acc: 93.6804%\n",
      "[Epoch:  3/  5] [data: 18816/24026] Training Loss: 0.3125 Training Acc: 93.6597%\n",
      "[Epoch:  3/  5] [data: 18944/24026] Training Loss: 0.3084 Training Acc: 93.6444%\n",
      "[Epoch:  3/  5] [data: 19072/24026] Training Loss: 0.3615 Training Acc: 93.6137%\n",
      "[Epoch:  3/  5] [data: 19200/24026] Training Loss: 0.3267 Training Acc: 93.5938%\n",
      "[Epoch:  3/  5] [data: 19328/24026] Training Loss: 0.2300 Training Acc: 93.5948%\n",
      "[Epoch:  3/  5] [data: 19456/24026] Training Loss: 0.3736 Training Acc: 93.5650%\n",
      "[Epoch:  3/  5] [data: 19584/24026] Training Loss: 0.1905 Training Acc: 93.5713%\n",
      "[Epoch:  3/  5] [data: 19712/24026] Training Loss: 0.2898 Training Acc: 93.5572%\n",
      "[Epoch:  3/  5] [data: 19840/24026] Training Loss: 0.3620 Training Acc: 93.5282%\n",
      "[Epoch:  3/  5] [data: 19968/24026] Training Loss: 0.2057 Training Acc: 93.5347%\n",
      "[Epoch:  3/  5] [data: 20096/24026] Training Loss: 0.1746 Training Acc: 93.5460%\n",
      "[Epoch:  3/  5] [data: 20224/24026] Training Loss: 0.2492 Training Acc: 93.5423%\n",
      "[Epoch:  3/  5] [data: 20352/24026] Training Loss: 0.2895 Training Acc: 93.5289%\n",
      "[Epoch:  3/  5] [data: 20480/24026] Training Loss: 0.2020 Training Acc: 93.5352%\n",
      "[Epoch:  3/  5] [data: 20608/24026] Training Loss: 0.1961 Training Acc: 93.5413%\n",
      "[Epoch:  3/  5] [data: 20736/24026] Training Loss: 0.3541 Training Acc: 93.5185%\n",
      "[Epoch:  3/  5] [data: 20864/24026] Training Loss: 0.2196 Training Acc: 93.5199%\n",
      "[Epoch:  3/  5] [data: 20992/24026] Training Loss: 0.1960 Training Acc: 93.5261%\n",
      "[Epoch:  3/  5] [data: 21120/24026] Training Loss: 0.1069 Training Acc: 93.5511%\n",
      "[Epoch:  3/  5] [data: 21248/24026] Training Loss: 0.2265 Training Acc: 93.5523%\n",
      "[Epoch:  3/  5] [data: 21376/24026] Training Loss: 0.1453 Training Acc: 93.5676%\n",
      "[Epoch:  3/  5] [data: 21504/24026] Training Loss: 0.1731 Training Acc: 93.5779%\n",
      "[Epoch:  3/  5] [data: 21632/24026] Training Loss: 0.1538 Training Acc: 93.5928%\n",
      "[Epoch:  3/  5] [data: 21760/24026] Training Loss: 0.2419 Training Acc: 93.5846%\n",
      "[Epoch:  3/  5] [data: 21888/24026] Training Loss: 0.1476 Training Acc: 93.5992%\n",
      "[Epoch:  3/  5] [data: 22016/24026] Training Loss: 0.1974 Training Acc: 93.6047%\n",
      "[Epoch:  3/  5] [data: 22144/24026] Training Loss: 0.2076 Training Acc: 93.6100%\n",
      "[Epoch:  3/  5] [data: 22272/24026] Training Loss: 0.1849 Training Acc: 93.6153%\n",
      "[Epoch:  3/  5] [data: 22400/24026] Training Loss: 0.2457 Training Acc: 93.6116%\n",
      "[Epoch:  3/  5] [data: 22528/24026] Training Loss: 0.2391 Training Acc: 93.6124%\n",
      "[Epoch:  3/  5] [data: 22656/24026] Training Loss: 0.2037 Training Acc: 93.6176%\n",
      "[Epoch:  3/  5] [data: 22784/24026] Training Loss: 0.2275 Training Acc: 93.6183%\n",
      "[Epoch:  3/  5] [data: 22912/24026] Training Loss: 0.3251 Training Acc: 93.5972%\n",
      "[Epoch:  3/  5] [data: 23040/24026] Training Loss: 0.2716 Training Acc: 93.5894%\n",
      "[Epoch:  3/  5] [data: 23168/24026] Training Loss: 0.2512 Training Acc: 93.5860%\n",
      "[Epoch:  3/  5] [data: 23296/24026] Training Loss: 0.2670 Training Acc: 93.5783%\n",
      "[Epoch:  3/  5] [data: 23424/24026] Training Loss: 0.1777 Training Acc: 93.5878%\n",
      "[Epoch:  3/  5] [data: 23552/24026] Training Loss: 0.2489 Training Acc: 93.5844%\n",
      "[Epoch:  3/  5] [data: 23680/24026] Training Loss: 0.1870 Training Acc: 93.5938%\n",
      "[Epoch:  3/  5] [data: 23808/24026] Training Loss: 0.2834 Training Acc: 93.5862%\n",
      "[Epoch:  3/  5] [data: 23936/24026] Training Loss: 0.2710 Training Acc: 93.5787%\n",
      "[Epoch:  3/  5] [data: 24026/24026] Training Loss: 0.1725 Training Acc: 93.5861%\n",
      "Testing-5...\n",
      "[Epoch:  3/  5] Validation Loss: 0.2216 Validation Acc: 93.7915%\n",
      "Time used: 2652.402359485626s\n",
      "[Epoch:  4/  5] [data: 128/24026] Training Loss: 0.3162 Training Acc: 90.6250%\n",
      "[Epoch:  4/  5] [data: 256/24026] Training Loss: 0.2409 Training Acc: 91.7969%\n",
      "[Epoch:  4/  5] [data: 384/24026] Training Loss: 0.2216 Training Acc: 92.4479%\n",
      "[Epoch:  4/  5] [data: 512/24026] Training Loss: 0.3552 Training Acc: 91.7969%\n",
      "[Epoch:  4/  5] [data: 640/24026] Training Loss: 0.1479 Training Acc: 92.6562%\n",
      "[Epoch:  4/  5] [data: 768/24026] Training Loss: 0.3163 Training Acc: 92.4479%\n",
      "[Epoch:  4/  5] [data: 896/24026] Training Loss: 0.2499 Training Acc: 92.5223%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 1024/24026] Training Loss: 0.2191 Training Acc: 92.6758%\n",
      "[Epoch:  4/  5] [data: 1152/24026] Training Loss: 0.1050 Training Acc: 93.2292%\n",
      "[Epoch:  4/  5] [data: 1280/24026] Training Loss: 0.1707 Training Acc: 93.4375%\n",
      "[Epoch:  4/  5] [data: 1408/24026] Training Loss: 0.1838 Training Acc: 93.5369%\n",
      "[Epoch:  4/  5] [data: 1536/24026] Training Loss: 0.3155 Training Acc: 93.2943%\n",
      "[Epoch:  4/  5] [data: 1664/24026] Training Loss: 0.2077 Training Acc: 93.3894%\n",
      "[Epoch:  4/  5] [data: 1792/24026] Training Loss: 0.2705 Training Acc: 93.3036%\n",
      "[Epoch:  4/  5] [data: 1920/24026] Training Loss: 0.2058 Training Acc: 93.3854%\n",
      "[Epoch:  4/  5] [data: 2048/24026] Training Loss: 0.2003 Training Acc: 93.4570%\n",
      "[Epoch:  4/  5] [data: 2176/24026] Training Loss: 0.1684 Training Acc: 93.5662%\n",
      "[Epoch:  4/  5] [data: 2304/24026] Training Loss: 0.1964 Training Acc: 93.6198%\n",
      "[Epoch:  4/  5] [data: 2432/24026] Training Loss: 0.2466 Training Acc: 93.5855%\n",
      "[Epoch:  4/  5] [data: 2560/24026] Training Loss: 0.1463 Training Acc: 93.7109%\n",
      "[Epoch:  4/  5] [data: 2688/24026] Training Loss: 0.2127 Training Acc: 93.7128%\n",
      "[Epoch:  4/  5] [data: 2816/24026] Training Loss: 0.1469 Training Acc: 93.8210%\n",
      "[Epoch:  4/  5] [data: 2944/24026] Training Loss: 0.2072 Training Acc: 93.8179%\n",
      "[Epoch:  4/  5] [data: 3072/24026] Training Loss: 0.4290 Training Acc: 93.5547%\n",
      "[Epoch:  4/  5] [data: 3200/24026] Training Loss: 0.2001 Training Acc: 93.5938%\n",
      "[Epoch:  4/  5] [data: 3328/24026] Training Loss: 0.2016 Training Acc: 93.5697%\n",
      "[Epoch:  4/  5] [data: 3456/24026] Training Loss: 0.2714 Training Acc: 93.4896%\n",
      "[Epoch:  4/  5] [data: 3584/24026] Training Loss: 0.2714 Training Acc: 93.4431%\n",
      "[Epoch:  4/  5] [data: 3712/24026] Training Loss: 0.1750 Training Acc: 93.5075%\n",
      "[Epoch:  4/  5] [data: 3840/24026] Training Loss: 0.1997 Training Acc: 93.5417%\n",
      "[Epoch:  4/  5] [data: 3968/24026] Training Loss: 0.0928 Training Acc: 93.6996%\n",
      "[Epoch:  4/  5] [data: 4096/24026] Training Loss: 0.1966 Training Acc: 93.7256%\n",
      "[Epoch:  4/  5] [data: 4224/24026] Training Loss: 0.2488 Training Acc: 93.7027%\n",
      "[Epoch:  4/  5] [data: 4352/24026] Training Loss: 0.2257 Training Acc: 93.7040%\n",
      "[Epoch:  4/  5] [data: 4480/24026] Training Loss: 0.3077 Training Acc: 93.6384%\n",
      "[Epoch:  4/  5] [data: 4608/24026] Training Loss: 0.2763 Training Acc: 93.5981%\n",
      "[Epoch:  4/  5] [data: 4736/24026] Training Loss: 0.1346 Training Acc: 93.6444%\n",
      "[Epoch:  4/  5] [data: 4864/24026] Training Loss: 0.1270 Training Acc: 93.7294%\n",
      "[Epoch:  4/  5] [data: 4992/24026] Training Loss: 0.3397 Training Acc: 93.6298%\n",
      "[Epoch:  4/  5] [data: 5120/24026] Training Loss: 0.2094 Training Acc: 93.6328%\n",
      "[Epoch:  4/  5] [data: 5248/24026] Training Loss: 0.1749 Training Acc: 93.6738%\n",
      "[Epoch:  4/  5] [data: 5376/24026] Training Loss: 0.2678 Training Acc: 93.6384%\n",
      "[Epoch:  4/  5] [data: 5504/24026] Training Loss: 0.1561 Training Acc: 93.6955%\n",
      "[Epoch:  4/  5] [data: 5632/24026] Training Loss: 0.2542 Training Acc: 93.6790%\n",
      "[Epoch:  4/  5] [data: 5760/24026] Training Loss: 0.3255 Training Acc: 93.6111%\n",
      "[Epoch:  4/  5] [data: 5888/24026] Training Loss: 0.1966 Training Acc: 93.6311%\n",
      "[Epoch:  4/  5] [data: 6016/24026] Training Loss: 0.1383 Training Acc: 93.7001%\n",
      "[Epoch:  4/  5] [data: 6144/24026] Training Loss: 0.1535 Training Acc: 93.7500%\n",
      "[Epoch:  4/  5] [data: 6272/24026] Training Loss: 0.1765 Training Acc: 93.7819%\n",
      "[Epoch:  4/  5] [data: 6400/24026] Training Loss: 0.1997 Training Acc: 93.7969%\n",
      "[Epoch:  4/  5] [data: 6528/24026] Training Loss: 0.2018 Training Acc: 93.7960%\n",
      "[Epoch:  4/  5] [data: 6656/24026] Training Loss: 0.1489 Training Acc: 93.8401%\n",
      "[Epoch:  4/  5] [data: 6784/24026] Training Loss: 0.2246 Training Acc: 93.8384%\n",
      "[Epoch:  4/  5] [data: 6912/24026] Training Loss: 0.3559 Training Acc: 93.7645%\n",
      "[Epoch:  4/  5] [data: 7040/24026] Training Loss: 0.1963 Training Acc: 93.7784%\n",
      "[Epoch:  4/  5] [data: 7168/24026] Training Loss: 0.2299 Training Acc: 93.7779%\n",
      "[Epoch:  4/  5] [data: 7296/24026] Training Loss: 0.2437 Training Acc: 93.7637%\n",
      "[Epoch:  4/  5] [data: 7424/24026] Training Loss: 0.2239 Training Acc: 93.7635%\n",
      "[Epoch:  4/  5] [data: 7552/24026] Training Loss: 0.1211 Training Acc: 93.8162%\n",
      "[Epoch:  4/  5] [data: 7680/24026] Training Loss: 0.1755 Training Acc: 93.8411%\n",
      "[Epoch:  4/  5] [data: 7808/24026] Training Loss: 0.2445 Training Acc: 93.8268%\n",
      "[Epoch:  4/  5] [data: 7936/24026] Training Loss: 0.2053 Training Acc: 93.8382%\n",
      "[Epoch:  4/  5] [data: 8064/24026] Training Loss: 0.2672 Training Acc: 93.8120%\n",
      "[Epoch:  4/  5] [data: 8192/24026] Training Loss: 0.1998 Training Acc: 93.8232%\n",
      "[Epoch:  4/  5] [data: 8320/24026] Training Loss: 0.1050 Training Acc: 93.8822%\n",
      "[Epoch:  4/  5] [data: 8448/24026] Training Loss: 0.2661 Training Acc: 93.8565%\n",
      "[Epoch:  4/  5] [data: 8576/24026] Training Loss: 0.1789 Training Acc: 93.8783%\n",
      "[Epoch:  4/  5] [data: 8704/24026] Training Loss: 0.1996 Training Acc: 93.8879%\n",
      "[Epoch:  4/  5] [data: 8832/24026] Training Loss: 0.1254 Training Acc: 93.9312%\n",
      "[Epoch:  4/  5] [data: 8960/24026] Training Loss: 0.1521 Training Acc: 93.9621%\n",
      "[Epoch:  4/  5] [data: 9088/24026] Training Loss: 0.3669 Training Acc: 93.8930%\n",
      "[Epoch:  4/  5] [data: 9216/24026] Training Loss: 0.1199 Training Acc: 93.9345%\n",
      "[Epoch:  4/  5] [data: 9344/24026] Training Loss: 0.0771 Training Acc: 93.9961%\n",
      "[Epoch:  4/  5] [data: 9472/24026] Training Loss: 0.2044 Training Acc: 94.0034%\n",
      "[Epoch:  4/  5] [data: 9600/24026] Training Loss: 0.2623 Training Acc: 93.9792%\n",
      "[Epoch:  4/  5] [data: 9728/24026] Training Loss: 0.2572 Training Acc: 93.9659%\n",
      "[Epoch:  4/  5] [data: 9856/24026] Training Loss: 0.3102 Training Acc: 93.9326%\n",
      "[Epoch:  4/  5] [data: 9984/24026] Training Loss: 0.4024 Training Acc: 93.8502%\n",
      "[Epoch:  4/  5] [data: 10112/24026] Training Loss: 0.3464 Training Acc: 93.7994%\n",
      "[Epoch:  4/  5] [data: 10240/24026] Training Loss: 0.1958 Training Acc: 93.8086%\n",
      "[Epoch:  4/  5] [data: 10368/24026] Training Loss: 0.1524 Training Acc: 93.8368%\n",
      "[Epoch:  4/  5] [data: 10496/24026] Training Loss: 0.2288 Training Acc: 93.8357%\n",
      "[Epoch:  4/  5] [data: 10624/24026] Training Loss: 0.1841 Training Acc: 93.8535%\n",
      "[Epoch:  4/  5] [data: 10752/24026] Training Loss: 0.2418 Training Acc: 93.8523%\n",
      "[Epoch:  4/  5] [data: 10880/24026] Training Loss: 0.2061 Training Acc: 93.8603%\n",
      "[Epoch:  4/  5] [data: 11008/24026] Training Loss: 0.3320 Training Acc: 93.8136%\n",
      "[Epoch:  4/  5] [data: 11136/24026] Training Loss: 0.3004 Training Acc: 93.7859%\n",
      "[Epoch:  4/  5] [data: 11264/24026] Training Loss: 0.2077 Training Acc: 93.7855%\n",
      "[Epoch:  4/  5] [data: 11392/24026] Training Loss: 0.1359 Training Acc: 93.8202%\n",
      "[Epoch:  4/  5] [data: 11520/24026] Training Loss: 0.1809 Training Acc: 93.8368%\n",
      "[Epoch:  4/  5] [data: 11648/24026] Training Loss: 0.1571 Training Acc: 93.8616%\n",
      "[Epoch:  4/  5] [data: 11776/24026] Training Loss: 0.2221 Training Acc: 93.8604%\n",
      "[Epoch:  4/  5] [data: 11904/24026] Training Loss: 0.2508 Training Acc: 93.8508%\n",
      "[Epoch:  4/  5] [data: 12032/24026] Training Loss: 0.1975 Training Acc: 93.8580%\n",
      "[Epoch:  4/  5] [data: 12160/24026] Training Loss: 0.2805 Training Acc: 93.8405%\n",
      "[Epoch:  4/  5] [data: 12288/24026] Training Loss: 0.1518 Training Acc: 93.8639%\n",
      "[Epoch:  4/  5] [data: 12416/24026] Training Loss: 0.3275 Training Acc: 93.8305%\n",
      "[Epoch:  4/  5] [data: 12544/24026] Training Loss: 0.2012 Training Acc: 93.8297%\n",
      "[Epoch:  4/  5] [data: 12672/24026] Training Loss: 0.3085 Training Acc: 93.8052%\n",
      "[Epoch:  4/  5] [data: 12800/24026] Training Loss: 0.1967 Training Acc: 93.8125%\n",
      "[Epoch:  4/  5] [data: 12928/24026] Training Loss: 0.1896 Training Acc: 93.8119%\n",
      "[Epoch:  4/  5] [data: 13056/24026] Training Loss: 0.2018 Training Acc: 93.8189%\n",
      "[Epoch:  4/  5] [data: 13184/24026] Training Loss: 0.2166 Training Acc: 93.8183%\n",
      "[Epoch:  4/  5] [data: 13312/24026] Training Loss: 0.2212 Training Acc: 93.8176%\n",
      "[Epoch:  4/  5] [data: 13440/24026] Training Loss: 0.1817 Training Acc: 93.8318%\n",
      "[Epoch:  4/  5] [data: 13568/24026] Training Loss: 0.2303 Training Acc: 93.8237%\n",
      "[Epoch:  4/  5] [data: 13696/24026] Training Loss: 0.2446 Training Acc: 93.8157%\n",
      "[Epoch:  4/  5] [data: 13824/24026] Training Loss: 0.1799 Training Acc: 93.8296%\n",
      "[Epoch:  4/  5] [data: 13952/24026] Training Loss: 0.2325 Training Acc: 93.8288%\n",
      "[Epoch:  4/  5] [data: 14080/24026] Training Loss: 0.2445 Training Acc: 93.8210%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/  5] [data: 14208/24026] Training Loss: 0.1408 Training Acc: 93.8415%\n",
      "[Epoch:  4/  5] [data: 14336/24026] Training Loss: 0.2272 Training Acc: 93.8407%\n",
      "[Epoch:  4/  5] [data: 14464/24026] Training Loss: 0.2285 Training Acc: 93.8399%\n",
      "[Epoch:  4/  5] [data: 14592/24026] Training Loss: 0.1512 Training Acc: 93.8596%\n",
      "[Epoch:  4/  5] [data: 14720/24026] Training Loss: 0.1976 Training Acc: 93.8655%\n",
      "[Epoch:  4/  5] [data: 14848/24026] Training Loss: 0.3245 Training Acc: 93.8308%\n",
      "[Epoch:  4/  5] [data: 14976/24026] Training Loss: 0.2701 Training Acc: 93.8168%\n",
      "[Epoch:  4/  5] [data: 15104/24026] Training Loss: 0.2773 Training Acc: 93.8030%\n",
      "[Epoch:  4/  5] [data: 15232/24026] Training Loss: 0.2533 Training Acc: 93.7960%\n",
      "[Epoch:  4/  5] [data: 15360/24026] Training Loss: 0.2749 Training Acc: 93.7826%\n",
      "[Epoch:  4/  5] [data: 15488/24026] Training Loss: 0.1518 Training Acc: 93.8017%\n",
      "[Epoch:  4/  5] [data: 15616/24026] Training Loss: 0.2498 Training Acc: 93.7948%\n",
      "[Epoch:  4/  5] [data: 15744/24026] Training Loss: 0.2021 Training Acc: 93.8008%\n",
      "[Epoch:  4/  5] [data: 15872/24026] Training Loss: 0.2979 Training Acc: 93.7815%\n",
      "[Epoch:  4/  5] [data: 16000/24026] Training Loss: 0.3506 Training Acc: 93.7500%\n",
      "[Epoch:  4/  5] [data: 16128/24026] Training Loss: 0.1797 Training Acc: 93.7624%\n",
      "[Epoch:  4/  5] [data: 16256/24026] Training Loss: 0.2440 Training Acc: 93.7562%\n",
      "[Epoch:  4/  5] [data: 16384/24026] Training Loss: 0.1924 Training Acc: 93.7622%\n",
      "[Epoch:  4/  5] [data: 16512/24026] Training Loss: 0.1744 Training Acc: 93.7742%\n",
      "[Epoch:  4/  5] [data: 16640/24026] Training Loss: 0.2031 Training Acc: 93.7800%\n",
      "[Epoch:  4/  5] [data: 16768/24026] Training Loss: 0.2654 Training Acc: 93.7679%\n",
      "[Epoch:  4/  5] [data: 16896/24026] Training Loss: 0.2006 Training Acc: 93.7737%\n",
      "[Epoch:  4/  5] [data: 17024/24026] Training Loss: 0.3669 Training Acc: 93.7383%\n",
      "[Epoch:  4/  5] [data: 17152/24026] Training Loss: 0.1516 Training Acc: 93.7558%\n",
      "[Epoch:  4/  5] [data: 17280/24026] Training Loss: 0.2213 Training Acc: 93.7558%\n",
      "[Epoch:  4/  5] [data: 17408/24026] Training Loss: 0.3910 Training Acc: 93.7155%\n",
      "[Epoch:  4/  5] [data: 17536/24026] Training Loss: 0.1706 Training Acc: 93.7272%\n",
      "[Epoch:  4/  5] [data: 17664/24026] Training Loss: 0.2250 Training Acc: 93.7274%\n",
      "[Epoch:  4/  5] [data: 17792/24026] Training Loss: 0.2042 Training Acc: 93.7275%\n",
      "[Epoch:  4/  5] [data: 17920/24026] Training Loss: 0.1787 Training Acc: 93.7388%\n",
      "[Epoch:  4/  5] [data: 18048/24026] Training Loss: 0.2455 Training Acc: 93.7334%\n",
      "[Epoch:  4/  5] [data: 18176/24026] Training Loss: 0.2669 Training Acc: 93.7225%\n",
      "[Epoch:  4/  5] [data: 18304/24026] Training Loss: 0.1456 Training Acc: 93.7391%\n",
      "[Epoch:  4/  5] [data: 18432/24026] Training Loss: 0.1798 Training Acc: 93.7500%\n",
      "[Epoch:  4/  5] [data: 18560/24026] Training Loss: 0.2168 Training Acc: 93.7500%\n",
      "[Epoch:  4/  5] [data: 18688/24026] Training Loss: 0.1974 Training Acc: 93.7554%\n",
      "[Epoch:  4/  5] [data: 18816/24026] Training Loss: 0.3122 Training Acc: 93.7341%\n",
      "[Epoch:  4/  5] [data: 18944/24026] Training Loss: 0.3084 Training Acc: 93.7183%\n",
      "[Epoch:  4/  5] [data: 19072/24026] Training Loss: 0.3715 Training Acc: 93.6871%\n",
      "[Epoch:  4/  5] [data: 19200/24026] Training Loss: 0.3224 Training Acc: 93.6667%\n",
      "[Epoch:  4/  5] [data: 19328/24026] Training Loss: 0.2253 Training Acc: 93.6672%\n",
      "[Epoch:  4/  5] [data: 19456/24026] Training Loss: 0.3702 Training Acc: 93.6369%\n",
      "[Epoch:  4/  5] [data: 19584/24026] Training Loss: 0.2032 Training Acc: 93.6428%\n",
      "[Epoch:  4/  5] [data: 19712/24026] Training Loss: 0.2924 Training Acc: 93.6282%\n",
      "[Epoch:  4/  5] [data: 19840/24026] Training Loss: 0.3503 Training Acc: 93.5988%\n",
      "[Epoch:  4/  5] [data: 19968/24026] Training Loss: 0.2132 Training Acc: 93.6048%\n",
      "[Epoch:  4/  5] [data: 20096/24026] Training Loss: 0.1789 Training Acc: 93.6156%\n",
      "[Epoch:  4/  5] [data: 20224/24026] Training Loss: 0.2502 Training Acc: 93.6116%\n",
      "[Epoch:  4/  5] [data: 20352/24026] Training Loss: 0.2856 Training Acc: 93.5977%\n",
      "[Epoch:  4/  5] [data: 20480/24026] Training Loss: 0.2022 Training Acc: 93.6035%\n",
      "[Epoch:  4/  5] [data: 20608/24026] Training Loss: 0.1949 Training Acc: 93.6093%\n",
      "[Epoch:  4/  5] [data: 20736/24026] Training Loss: 0.3556 Training Acc: 93.5860%\n",
      "[Epoch:  4/  5] [data: 20864/24026] Training Loss: 0.2235 Training Acc: 93.5870%\n",
      "[Epoch:  4/  5] [data: 20992/24026] Training Loss: 0.1938 Training Acc: 93.5928%\n",
      "[Epoch:  4/  5] [data: 21120/24026] Training Loss: 0.1053 Training Acc: 93.6174%\n",
      "[Epoch:  4/  5] [data: 21248/24026] Training Loss: 0.2261 Training Acc: 93.6182%\n",
      "[Epoch:  4/  5] [data: 21376/24026] Training Loss: 0.1479 Training Acc: 93.6330%\n",
      "[Epoch:  4/  5] [data: 21504/24026] Training Loss: 0.1735 Training Acc: 93.6430%\n",
      "[Epoch:  4/  5] [data: 21632/24026] Training Loss: 0.1499 Training Acc: 93.6575%\n",
      "[Epoch:  4/  5] [data: 21760/24026] Training Loss: 0.2359 Training Acc: 93.6535%\n",
      "[Epoch:  4/  5] [data: 21888/24026] Training Loss: 0.1495 Training Acc: 93.6678%\n",
      "[Epoch:  4/  5] [data: 22016/24026] Training Loss: 0.1994 Training Acc: 93.6682%\n",
      "[Epoch:  4/  5] [data: 22144/24026] Training Loss: 0.2048 Training Acc: 93.6732%\n",
      "[Epoch:  4/  5] [data: 22272/24026] Training Loss: 0.1826 Training Acc: 93.6827%\n",
      "[Epoch:  4/  5] [data: 22400/24026] Training Loss: 0.2489 Training Acc: 93.6786%\n",
      "[Epoch:  4/  5] [data: 22528/24026] Training Loss: 0.2389 Training Acc: 93.6790%\n",
      "[Epoch:  4/  5] [data: 22656/24026] Training Loss: 0.2038 Training Acc: 93.6838%\n",
      "[Epoch:  4/  5] [data: 22784/24026] Training Loss: 0.2278 Training Acc: 93.6842%\n",
      "[Epoch:  4/  5] [data: 22912/24026] Training Loss: 0.3124 Training Acc: 93.6627%\n",
      "[Epoch:  4/  5] [data: 23040/24026] Training Loss: 0.2737 Training Acc: 93.6545%\n",
      "[Epoch:  4/  5] [data: 23168/24026] Training Loss: 0.2475 Training Acc: 93.6507%\n",
      "[Epoch:  4/  5] [data: 23296/24026] Training Loss: 0.2691 Training Acc: 93.6427%\n",
      "[Epoch:  4/  5] [data: 23424/24026] Training Loss: 0.1767 Training Acc: 93.6518%\n",
      "[Epoch:  4/  5] [data: 23552/24026] Training Loss: 0.2561 Training Acc: 93.6481%\n",
      "[Epoch:  4/  5] [data: 23680/24026] Training Loss: 0.1744 Training Acc: 93.6613%\n",
      "[Epoch:  4/  5] [data: 23808/24026] Training Loss: 0.2785 Training Acc: 93.6534%\n",
      "[Epoch:  4/  5] [data: 23936/24026] Training Loss: 0.2732 Training Acc: 93.6456%\n",
      "[Epoch:  4/  5] [data: 24026/24026] Training Loss: 0.1720 Training Acc: 93.6527%\n",
      "Testing-5...\n",
      "[Epoch:  4/  5] Validation Loss: 0.2213 Validation Acc: 93.7915%\n",
      "Time used: 2646.988073825836s\n",
      "[Epoch:  5/  5] [data: 128/24026] Training Loss: 0.3188 Training Acc: 90.6250%\n",
      "[Epoch:  5/  5] [data: 256/24026] Training Loss: 0.2409 Training Acc: 91.7969%\n",
      "[Epoch:  5/  5] [data: 384/24026] Training Loss: 0.2175 Training Acc: 92.4479%\n",
      "[Epoch:  5/  5] [data: 512/24026] Training Loss: 0.3531 Training Acc: 91.7969%\n",
      "[Epoch:  5/  5] [data: 640/24026] Training Loss: 0.1487 Training Acc: 92.6562%\n",
      "[Epoch:  5/  5] [data: 768/24026] Training Loss: 0.3090 Training Acc: 92.4479%\n",
      "[Epoch:  5/  5] [data: 896/24026] Training Loss: 0.2518 Training Acc: 92.5223%\n",
      "[Epoch:  5/  5] [data: 1024/24026] Training Loss: 0.2241 Training Acc: 92.6758%\n",
      "[Epoch:  5/  5] [data: 1152/24026] Training Loss: 0.1037 Training Acc: 93.2292%\n",
      "[Epoch:  5/  5] [data: 1280/24026] Training Loss: 0.1709 Training Acc: 93.4375%\n",
      "[Epoch:  5/  5] [data: 1408/24026] Training Loss: 0.1801 Training Acc: 93.5369%\n",
      "[Epoch:  5/  5] [data: 1536/24026] Training Loss: 0.3145 Training Acc: 93.2943%\n",
      "[Epoch:  5/  5] [data: 1664/24026] Training Loss: 0.2079 Training Acc: 93.3894%\n",
      "[Epoch:  5/  5] [data: 1792/24026] Training Loss: 0.2759 Training Acc: 93.3036%\n",
      "[Epoch:  5/  5] [data: 1920/24026] Training Loss: 0.2042 Training Acc: 93.3854%\n",
      "[Epoch:  5/  5] [data: 2048/24026] Training Loss: 0.1974 Training Acc: 93.4570%\n",
      "[Epoch:  5/  5] [data: 2176/24026] Training Loss: 0.1721 Training Acc: 93.5662%\n",
      "[Epoch:  5/  5] [data: 2304/24026] Training Loss: 0.1942 Training Acc: 93.6198%\n",
      "[Epoch:  5/  5] [data: 2432/24026] Training Loss: 0.2467 Training Acc: 93.5855%\n",
      "[Epoch:  5/  5] [data: 2560/24026] Training Loss: 0.1484 Training Acc: 93.7109%\n",
      "[Epoch:  5/  5] [data: 2688/24026] Training Loss: 0.2115 Training Acc: 93.7128%\n",
      "[Epoch:  5/  5] [data: 2816/24026] Training Loss: 0.1450 Training Acc: 93.8210%\n",
      "[Epoch:  5/  5] [data: 2944/24026] Training Loss: 0.2098 Training Acc: 93.8179%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 3072/24026] Training Loss: 0.4249 Training Acc: 93.5547%\n",
      "[Epoch:  5/  5] [data: 3200/24026] Training Loss: 0.2015 Training Acc: 93.5938%\n",
      "[Epoch:  5/  5] [data: 3328/24026] Training Loss: 0.1983 Training Acc: 93.5697%\n",
      "[Epoch:  5/  5] [data: 3456/24026] Training Loss: 0.2740 Training Acc: 93.4896%\n",
      "[Epoch:  5/  5] [data: 3584/24026] Training Loss: 0.2726 Training Acc: 93.4431%\n",
      "[Epoch:  5/  5] [data: 3712/24026] Training Loss: 0.1753 Training Acc: 93.5075%\n",
      "[Epoch:  5/  5] [data: 3840/24026] Training Loss: 0.2006 Training Acc: 93.5417%\n",
      "[Epoch:  5/  5] [data: 3968/24026] Training Loss: 0.0917 Training Acc: 93.6996%\n",
      "[Epoch:  5/  5] [data: 4096/24026] Training Loss: 0.1962 Training Acc: 93.7256%\n",
      "[Epoch:  5/  5] [data: 4224/24026] Training Loss: 0.2493 Training Acc: 93.7027%\n",
      "[Epoch:  5/  5] [data: 4352/24026] Training Loss: 0.2259 Training Acc: 93.7040%\n",
      "[Epoch:  5/  5] [data: 4480/24026] Training Loss: 0.3028 Training Acc: 93.6384%\n",
      "[Epoch:  5/  5] [data: 4608/24026] Training Loss: 0.2741 Training Acc: 93.5981%\n",
      "[Epoch:  5/  5] [data: 4736/24026] Training Loss: 0.1325 Training Acc: 93.6655%\n",
      "[Epoch:  5/  5] [data: 4864/24026] Training Loss: 0.1222 Training Acc: 93.7500%\n",
      "[Epoch:  5/  5] [data: 4992/24026] Training Loss: 0.3386 Training Acc: 93.6498%\n",
      "[Epoch:  5/  5] [data: 5120/24026] Training Loss: 0.2101 Training Acc: 93.6523%\n",
      "[Epoch:  5/  5] [data: 5248/24026] Training Loss: 0.1740 Training Acc: 93.6928%\n",
      "[Epoch:  5/  5] [data: 5376/24026] Training Loss: 0.2693 Training Acc: 93.6570%\n",
      "[Epoch:  5/  5] [data: 5504/24026] Training Loss: 0.1547 Training Acc: 93.7137%\n",
      "[Epoch:  5/  5] [data: 5632/24026] Training Loss: 0.2525 Training Acc: 93.6967%\n",
      "[Epoch:  5/  5] [data: 5760/24026] Training Loss: 0.3249 Training Acc: 93.6285%\n",
      "[Epoch:  5/  5] [data: 5888/24026] Training Loss: 0.1964 Training Acc: 93.6481%\n",
      "[Epoch:  5/  5] [data: 6016/24026] Training Loss: 0.1356 Training Acc: 93.7168%\n",
      "[Epoch:  5/  5] [data: 6144/24026] Training Loss: 0.1500 Training Acc: 93.7663%\n",
      "[Epoch:  5/  5] [data: 6272/24026] Training Loss: 0.1761 Training Acc: 93.7978%\n",
      "[Epoch:  5/  5] [data: 6400/24026] Training Loss: 0.1972 Training Acc: 93.8125%\n",
      "[Epoch:  5/  5] [data: 6528/24026] Training Loss: 0.1996 Training Acc: 93.8113%\n",
      "[Epoch:  5/  5] [data: 6656/24026] Training Loss: 0.1492 Training Acc: 93.8552%\n",
      "[Epoch:  5/  5] [data: 6784/24026] Training Loss: 0.2246 Training Acc: 93.8532%\n",
      "[Epoch:  5/  5] [data: 6912/24026] Training Loss: 0.3561 Training Acc: 93.7789%\n",
      "[Epoch:  5/  5] [data: 7040/24026] Training Loss: 0.1964 Training Acc: 93.7926%\n",
      "[Epoch:  5/  5] [data: 7168/24026] Training Loss: 0.2297 Training Acc: 93.7919%\n",
      "[Epoch:  5/  5] [data: 7296/24026] Training Loss: 0.2445 Training Acc: 93.7774%\n",
      "[Epoch:  5/  5] [data: 7424/24026] Training Loss: 0.2224 Training Acc: 93.7769%\n",
      "[Epoch:  5/  5] [data: 7552/24026] Training Loss: 0.1220 Training Acc: 93.8294%\n",
      "[Epoch:  5/  5] [data: 7680/24026] Training Loss: 0.1754 Training Acc: 93.8542%\n",
      "[Epoch:  5/  5] [data: 7808/24026] Training Loss: 0.2460 Training Acc: 93.8397%\n",
      "[Epoch:  5/  5] [data: 7936/24026] Training Loss: 0.2044 Training Acc: 93.8508%\n",
      "[Epoch:  5/  5] [data: 8064/24026] Training Loss: 0.2682 Training Acc: 93.8244%\n",
      "[Epoch:  5/  5] [data: 8192/24026] Training Loss: 0.1991 Training Acc: 93.8354%\n",
      "[Epoch:  5/  5] [data: 8320/24026] Training Loss: 0.1043 Training Acc: 93.8942%\n",
      "[Epoch:  5/  5] [data: 8448/24026] Training Loss: 0.2669 Training Acc: 93.8684%\n",
      "[Epoch:  5/  5] [data: 8576/24026] Training Loss: 0.1782 Training Acc: 93.8899%\n",
      "[Epoch:  5/  5] [data: 8704/24026] Training Loss: 0.1989 Training Acc: 93.8994%\n",
      "[Epoch:  5/  5] [data: 8832/24026] Training Loss: 0.1252 Training Acc: 93.9425%\n",
      "[Epoch:  5/  5] [data: 8960/24026] Training Loss: 0.1518 Training Acc: 93.9732%\n",
      "[Epoch:  5/  5] [data: 9088/24026] Training Loss: 0.3652 Training Acc: 93.9040%\n",
      "[Epoch:  5/  5] [data: 9216/24026] Training Loss: 0.1174 Training Acc: 93.9453%\n",
      "[Epoch:  5/  5] [data: 9344/24026] Training Loss: 0.0767 Training Acc: 94.0068%\n",
      "[Epoch:  5/  5] [data: 9472/24026] Training Loss: 0.2038 Training Acc: 94.0139%\n",
      "[Epoch:  5/  5] [data: 9600/24026] Training Loss: 0.2605 Training Acc: 93.9896%\n",
      "[Epoch:  5/  5] [data: 9728/24026] Training Loss: 0.2563 Training Acc: 93.9762%\n",
      "[Epoch:  5/  5] [data: 9856/24026] Training Loss: 0.3094 Training Acc: 93.9428%\n",
      "[Epoch:  5/  5] [data: 9984/24026] Training Loss: 0.4033 Training Acc: 93.8702%\n",
      "[Epoch:  5/  5] [data: 10112/24026] Training Loss: 0.3483 Training Acc: 93.8192%\n",
      "[Epoch:  5/  5] [data: 10240/24026] Training Loss: 0.1969 Training Acc: 93.8281%\n",
      "[Epoch:  5/  5] [data: 10368/24026] Training Loss: 0.1507 Training Acc: 93.8561%\n",
      "[Epoch:  5/  5] [data: 10496/24026] Training Loss: 0.2280 Training Acc: 93.8548%\n",
      "[Epoch:  5/  5] [data: 10624/24026] Training Loss: 0.1814 Training Acc: 93.8724%\n",
      "[Epoch:  5/  5] [data: 10752/24026] Training Loss: 0.2384 Training Acc: 93.8709%\n",
      "[Epoch:  5/  5] [data: 10880/24026] Training Loss: 0.2051 Training Acc: 93.8787%\n",
      "[Epoch:  5/  5] [data: 11008/24026] Training Loss: 0.3305 Training Acc: 93.8318%\n",
      "[Epoch:  5/  5] [data: 11136/24026] Training Loss: 0.3016 Training Acc: 93.8039%\n",
      "[Epoch:  5/  5] [data: 11264/24026] Training Loss: 0.2081 Training Acc: 93.8033%\n",
      "[Epoch:  5/  5] [data: 11392/24026] Training Loss: 0.1382 Training Acc: 93.8378%\n",
      "[Epoch:  5/  5] [data: 11520/24026] Training Loss: 0.1821 Training Acc: 93.8542%\n",
      "[Epoch:  5/  5] [data: 11648/24026] Training Loss: 0.1567 Training Acc: 93.8788%\n",
      "[Epoch:  5/  5] [data: 11776/24026] Training Loss: 0.2228 Training Acc: 93.8774%\n",
      "[Epoch:  5/  5] [data: 11904/24026] Training Loss: 0.2499 Training Acc: 93.8676%\n",
      "[Epoch:  5/  5] [data: 12032/24026] Training Loss: 0.1907 Training Acc: 93.8747%\n",
      "[Epoch:  5/  5] [data: 12160/24026] Training Loss: 0.2821 Training Acc: 93.8569%\n",
      "[Epoch:  5/  5] [data: 12288/24026] Training Loss: 0.1528 Training Acc: 93.8721%\n",
      "[Epoch:  5/  5] [data: 12416/24026] Training Loss: 0.3243 Training Acc: 93.8386%\n",
      "[Epoch:  5/  5] [data: 12544/24026] Training Loss: 0.1996 Training Acc: 93.8377%\n",
      "[Epoch:  5/  5] [data: 12672/24026] Training Loss: 0.3086 Training Acc: 93.8131%\n",
      "[Epoch:  5/  5] [data: 12800/24026] Training Loss: 0.1973 Training Acc: 93.8203%\n",
      "[Epoch:  5/  5] [data: 12928/24026] Training Loss: 0.1879 Training Acc: 93.8274%\n",
      "[Epoch:  5/  5] [data: 13056/24026] Training Loss: 0.2008 Training Acc: 93.8343%\n",
      "[Epoch:  5/  5] [data: 13184/24026] Training Loss: 0.2175 Training Acc: 93.8334%\n",
      "[Epoch:  5/  5] [data: 13312/24026] Training Loss: 0.2195 Training Acc: 93.8326%\n",
      "[Epoch:  5/  5] [data: 13440/24026] Training Loss: 0.1810 Training Acc: 93.8467%\n",
      "[Epoch:  5/  5] [data: 13568/24026] Training Loss: 0.2271 Training Acc: 93.8458%\n",
      "[Epoch:  5/  5] [data: 13696/24026] Training Loss: 0.2439 Training Acc: 93.8376%\n",
      "[Epoch:  5/  5] [data: 13824/24026] Training Loss: 0.1795 Training Acc: 93.8513%\n",
      "[Epoch:  5/  5] [data: 13952/24026] Training Loss: 0.2312 Training Acc: 93.8503%\n",
      "[Epoch:  5/  5] [data: 14080/24026] Training Loss: 0.2436 Training Acc: 93.8423%\n",
      "[Epoch:  5/  5] [data: 14208/24026] Training Loss: 0.1387 Training Acc: 93.8626%\n",
      "[Epoch:  5/  5] [data: 14336/24026] Training Loss: 0.2273 Training Acc: 93.8616%\n",
      "[Epoch:  5/  5] [data: 14464/24026] Training Loss: 0.2266 Training Acc: 93.8606%\n",
      "[Epoch:  5/  5] [data: 14592/24026] Training Loss: 0.1509 Training Acc: 93.8802%\n",
      "[Epoch:  5/  5] [data: 14720/24026] Training Loss: 0.1969 Training Acc: 93.8859%\n",
      "[Epoch:  5/  5] [data: 14848/24026] Training Loss: 0.3209 Training Acc: 93.8510%\n",
      "[Epoch:  5/  5] [data: 14976/24026] Training Loss: 0.2688 Training Acc: 93.8368%\n",
      "[Epoch:  5/  5] [data: 15104/24026] Training Loss: 0.2784 Training Acc: 93.8228%\n",
      "[Epoch:  5/  5] [data: 15232/24026] Training Loss: 0.2525 Training Acc: 93.8157%\n",
      "[Epoch:  5/  5] [data: 15360/24026] Training Loss: 0.2745 Training Acc: 93.8021%\n",
      "[Epoch:  5/  5] [data: 15488/24026] Training Loss: 0.1506 Training Acc: 93.8210%\n",
      "[Epoch:  5/  5] [data: 15616/24026] Training Loss: 0.2509 Training Acc: 93.8140%\n",
      "[Epoch:  5/  5] [data: 15744/24026] Training Loss: 0.2026 Training Acc: 93.8199%\n",
      "[Epoch:  5/  5] [data: 15872/24026] Training Loss: 0.2977 Training Acc: 93.8004%\n",
      "[Epoch:  5/  5] [data: 16000/24026] Training Loss: 0.3501 Training Acc: 93.7687%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/  5] [data: 16128/24026] Training Loss: 0.1793 Training Acc: 93.7810%\n",
      "[Epoch:  5/  5] [data: 16256/24026] Training Loss: 0.2437 Training Acc: 93.7746%\n",
      "[Epoch:  5/  5] [data: 16384/24026] Training Loss: 0.1916 Training Acc: 93.7805%\n",
      "[Epoch:  5/  5] [data: 16512/24026] Training Loss: 0.1738 Training Acc: 93.7924%\n",
      "[Epoch:  5/  5] [data: 16640/24026] Training Loss: 0.2028 Training Acc: 93.7981%\n",
      "[Epoch:  5/  5] [data: 16768/24026] Training Loss: 0.2651 Training Acc: 93.7858%\n",
      "[Epoch:  5/  5] [data: 16896/24026] Training Loss: 0.2020 Training Acc: 93.7914%\n",
      "[Epoch:  5/  5] [data: 17024/24026] Training Loss: 0.3628 Training Acc: 93.7617%\n",
      "[Epoch:  5/  5] [data: 17152/24026] Training Loss: 0.1532 Training Acc: 93.7792%\n",
      "[Epoch:  5/  5] [data: 17280/24026] Training Loss: 0.2175 Training Acc: 93.7789%\n",
      "[Epoch:  5/  5] [data: 17408/24026] Training Loss: 0.3881 Training Acc: 93.7385%\n",
      "[Epoch:  5/  5] [data: 17536/24026] Training Loss: 0.1709 Training Acc: 93.7500%\n",
      "[Epoch:  5/  5] [data: 17664/24026] Training Loss: 0.2252 Training Acc: 93.7500%\n",
      "[Epoch:  5/  5] [data: 17792/24026] Training Loss: 0.2041 Training Acc: 93.7500%\n",
      "[Epoch:  5/  5] [data: 17920/24026] Training Loss: 0.1799 Training Acc: 93.7612%\n",
      "[Epoch:  5/  5] [data: 18048/24026] Training Loss: 0.2454 Training Acc: 93.7555%\n",
      "[Epoch:  5/  5] [data: 18176/24026] Training Loss: 0.2656 Training Acc: 93.7445%\n",
      "[Epoch:  5/  5] [data: 18304/24026] Training Loss: 0.1453 Training Acc: 93.7609%\n",
      "[Epoch:  5/  5] [data: 18432/24026] Training Loss: 0.1801 Training Acc: 93.7717%\n",
      "[Epoch:  5/  5] [data: 18560/24026] Training Loss: 0.2176 Training Acc: 93.7716%\n",
      "[Epoch:  5/  5] [data: 18688/24026] Training Loss: 0.1968 Training Acc: 93.7768%\n",
      "[Epoch:  5/  5] [data: 18816/24026] Training Loss: 0.3127 Training Acc: 93.7553%\n",
      "[Epoch:  5/  5] [data: 18944/24026] Training Loss: 0.3080 Training Acc: 93.7394%\n",
      "[Epoch:  5/  5] [data: 19072/24026] Training Loss: 0.3727 Training Acc: 93.7081%\n",
      "[Epoch:  5/  5] [data: 19200/24026] Training Loss: 0.3222 Training Acc: 93.6875%\n",
      "[Epoch:  5/  5] [data: 19328/24026] Training Loss: 0.2236 Training Acc: 93.6879%\n",
      "[Epoch:  5/  5] [data: 19456/24026] Training Loss: 0.3715 Training Acc: 93.6575%\n",
      "[Epoch:  5/  5] [data: 19584/24026] Training Loss: 0.1991 Training Acc: 93.6632%\n",
      "[Epoch:  5/  5] [data: 19712/24026] Training Loss: 0.2927 Training Acc: 93.6485%\n",
      "[Epoch:  5/  5] [data: 19840/24026] Training Loss: 0.3503 Training Acc: 93.6190%\n",
      "[Epoch:  5/  5] [data: 19968/24026] Training Loss: 0.2144 Training Acc: 93.6248%\n",
      "[Epoch:  5/  5] [data: 20096/24026] Training Loss: 0.1814 Training Acc: 93.6355%\n",
      "[Epoch:  5/  5] [data: 20224/24026] Training Loss: 0.2517 Training Acc: 93.6313%\n",
      "[Epoch:  5/  5] [data: 20352/24026] Training Loss: 0.2842 Training Acc: 93.6173%\n",
      "[Epoch:  5/  5] [data: 20480/24026] Training Loss: 0.2040 Training Acc: 93.6230%\n",
      "[Epoch:  5/  5] [data: 20608/24026] Training Loss: 0.1966 Training Acc: 93.6287%\n",
      "[Epoch:  5/  5] [data: 20736/24026] Training Loss: 0.3491 Training Acc: 93.6053%\n",
      "[Epoch:  5/  5] [data: 20864/24026] Training Loss: 0.2224 Training Acc: 93.6062%\n",
      "[Epoch:  5/  5] [data: 20992/24026] Training Loss: 0.1932 Training Acc: 93.6119%\n",
      "[Epoch:  5/  5] [data: 21120/24026] Training Loss: 0.1069 Training Acc: 93.6364%\n",
      "[Epoch:  5/  5] [data: 21248/24026] Training Loss: 0.2257 Training Acc: 93.6370%\n",
      "[Epoch:  5/  5] [data: 21376/24026] Training Loss: 0.1479 Training Acc: 93.6518%\n",
      "[Epoch:  5/  5] [data: 21504/24026] Training Loss: 0.1752 Training Acc: 93.6616%\n",
      "[Epoch:  5/  5] [data: 21632/24026] Training Loss: 0.1492 Training Acc: 93.6760%\n",
      "[Epoch:  5/  5] [data: 21760/24026] Training Loss: 0.2343 Training Acc: 93.6719%\n",
      "[Epoch:  5/  5] [data: 21888/24026] Training Loss: 0.1476 Training Acc: 93.6860%\n",
      "[Epoch:  5/  5] [data: 22016/24026] Training Loss: 0.1998 Training Acc: 93.6864%\n",
      "[Epoch:  5/  5] [data: 22144/24026] Training Loss: 0.2056 Training Acc: 93.6913%\n",
      "[Epoch:  5/  5] [data: 22272/24026] Training Loss: 0.1829 Training Acc: 93.7006%\n",
      "[Epoch:  5/  5] [data: 22400/24026] Training Loss: 0.2477 Training Acc: 93.6964%\n",
      "[Epoch:  5/  5] [data: 22528/24026] Training Loss: 0.2395 Training Acc: 93.6967%\n",
      "[Epoch:  5/  5] [data: 22656/24026] Training Loss: 0.2047 Training Acc: 93.7014%\n",
      "[Epoch:  5/  5] [data: 22784/24026] Training Loss: 0.2283 Training Acc: 93.7017%\n",
      "[Epoch:  5/  5] [data: 22912/24026] Training Loss: 0.3090 Training Acc: 93.6845%\n",
      "[Epoch:  5/  5] [data: 23040/24026] Training Loss: 0.2742 Training Acc: 93.6762%\n",
      "[Epoch:  5/  5] [data: 23168/24026] Training Loss: 0.2493 Training Acc: 93.6723%\n",
      "[Epoch:  5/  5] [data: 23296/24026] Training Loss: 0.2707 Training Acc: 93.6641%\n",
      "[Epoch:  5/  5] [data: 23424/24026] Training Loss: 0.1705 Training Acc: 93.6732%\n",
      "[Epoch:  5/  5] [data: 23552/24026] Training Loss: 0.2480 Training Acc: 93.6693%\n",
      "[Epoch:  5/  5] [data: 23680/24026] Training Loss: 0.1663 Training Acc: 93.6824%\n",
      "[Epoch:  5/  5] [data: 23808/24026] Training Loss: 0.2784 Training Acc: 93.6744%\n",
      "[Epoch:  5/  5] [data: 23936/24026] Training Loss: 0.2735 Training Acc: 93.6664%\n",
      "[Epoch:  5/  5] [data: 24026/24026] Training Loss: 0.1753 Training Acc: 93.6735%\n",
      "Testing-5...\n",
      "[Epoch:  5/  5] Validation Loss: 0.2215 Validation Acc: 93.8165%\n",
      "Time used: 2651.28058218956s\n",
      "Type-5: 1.0 0.8812279818822345 0.9368646334938469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 1\n",
    "EPOCHS = [2,2,5,5,10]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = BatchProgramCC(EMBEDDING_DIM,\n",
    "                       HIDDEN_DIM,\n",
    "                       MAX_TOKENS+1,\n",
    "                       ENCODE_DIM,\n",
    "                       LABELS,\n",
    "                       BATCH_SIZE,\n",
    "                       device,\n",
    "                       embeddings)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adamax(parameters)\n",
    "#loss_function = torch.nn.BCELoss()\n",
    "loss_function = ContrastiveLoss(2.0)\n",
    "\n",
    "best_model_state_dict = model.state_dict()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_p = []\n",
    "test_r = []\n",
    "test_f = []\n",
    "best_acc = 0.0\n",
    "precision, recall, f1 = 0, 0, 0\n",
    "print('Start training...')\n",
    "\n",
    "for t in range(1, categories+1):\n",
    "    if lang == 'java':\n",
    "        train_data_t = train_data[train_data['label'].isin([t, 0])]\n",
    "        train_data_t.loc[train_data_t['label'] > 0, 'label'] = 1\n",
    "        \n",
    "        validation_data_t = validation_data[validation_data['label'].isin([t, 0])]\n",
    "        validation_data_t.loc[validation_data_t['label'] > 0, 'label'] = 1\n",
    "        \n",
    "        test_data_t = test_data[test_data['label'].isin([t, 0])]\n",
    "        test_data_t.loc[test_data_t['label'] > 0, 'label'] = 1\n",
    "    else:\n",
    "        train_data_t, validation_data_t, test_data_t = train_data, validation_data, test_data\n",
    "    \n",
    "    for epoch in range(0, EPOCHS[t-1]):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_loss, total_acc = train(model, train_data_t, BATCH_SIZE, device, EPOCHS[t-1], epoch, optimizer)\n",
    "        train_loss.append(total_loss)\n",
    "        train_acc.append(total_acc)\n",
    "        \n",
    "        print(\"Testing-%d...\"%t)\n",
    "        model.eval()\n",
    "        total_loss, total_acc = validation(model, validation_data_t, BATCH_SIZE, device, EPOCHS[t-1], epoch)\n",
    "        val_loss.append(total_loss)\n",
    "        val_acc.append(total_acc)\n",
    "        \n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Time used: {}s\".format(end_time-start_time))\n",
    "    \n",
    "    torch.save(model.state_dict(), 'code_clone_detection_java_model_train_with_ContrastiveLoss_on_java_label_' + str(t) + '_epoch_' + str(EPOCHS[t-1]) + '.pt')\n",
    "    total_loss, total_acc, precision, recall, f1 = test(model, test_data_t, BATCH_SIZE, device)\n",
    "    test_loss.append(total_loss)\n",
    "    test_acc.append(total_acc)\n",
    "    test_p.append(precision)\n",
    "    test_r.append(recall)\n",
    "    test_f.append(f1)\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), 'code_clone_detection_java_model_train_on_30_percent.pt')\n",
    "#model.load_state_dict(best_model_state_dict)\n",
    "#model.eval()\n",
    "#test(model, test_data, BATCH_SIZE*2, device)\n",
    "#print(\"Total testing results(P,R,F1):%.3f, %.3f, %.3f\" % (precision, recall, f1))\n",
    "\n",
    "#torch.save(model.state_dict(), 'code_clone_detection_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:59:37.676907Z",
     "start_time": "2020-09-21T13:59:35.725749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#epochs = len(hist.history['loss'][9:])\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(train_loss)), train_loss, label='loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='val_loss')\n",
    "plt.plot(range(len(test_loss)), test_loss, label='test_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(train_acc)), train_acc, label='accuracy')\n",
    "plt.plot(range(len(val_acc)), val_acc, label='val_accuracy')\n",
    "plt.plot(range(len(test_acc)), test_acc, label='test_accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(range(len(test_p)), test_p, label='precision')\n",
    "plt.plot(range(len(test_r)), test_r, label='recall')\n",
    "plt.plot(range(len(test_f)), test_f, label='f1')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T15:23:05.171694Z",
     "start_time": "2020-08-12T15:20:30.405009Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse source code...\n",
      "read id pairs...\n",
      "split data...\n",
      "train word embedding...\n",
      "generate block sequences...\n",
      "merge pairs and blocks...\n",
      "generate query source...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "class Pipeline:\n",
    "    def __init__(self,  ratio, root, language):\n",
    "        self.ratio = ratio\n",
    "        self.root = root\n",
    "        self.language = language\n",
    "        self.sources = None\n",
    "        self.blocks = None\n",
    "        self.pairs = None\n",
    "        self.train_file_path = None\n",
    "        self.dev_file_path = None\n",
    "        self.test_file_path = None\n",
    "        self.size = None\n",
    "        \n",
    "        self.seen_ids = None\n",
    "        self.unseen_ids = None\n",
    "\n",
    "    # parse source code\n",
    "    def parse_source(self, output_file, option):\n",
    "        path = self.root+self.language+'/'+output_file\n",
    "        if os.path.exists(path) and option == 'existing':\n",
    "            source = pd.read_pickle(path)\n",
    "        else:\n",
    "            if self.language == 'c':\n",
    "                from pycparser import c_parser\n",
    "                parser = c_parser.CParser()\n",
    "                source = pd.read_pickle(self.root+self.language+'/programs.pkl')\n",
    "                source.columns = ['id', 'code', 'label']\n",
    "                source['code'] = source['code'].apply(parser.parse)\n",
    "                source.to_pickle(path)\n",
    "            else:\n",
    "                import javalang\n",
    "                def parse_program(func):\n",
    "                    tokens = javalang.tokenizer.tokenize(func)\n",
    "                    parser = javalang.parser.Parser(tokens)\n",
    "                    tree = parser.parse_member_declaration()\n",
    "                    return tree\n",
    "                source = pd.read_csv(self.root+self.language+'/bcb_funcs_all.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "                source.columns = ['id', 'code']\n",
    "                source.loc[15167, 'code'] = source['code'][15167]+'\\r     */'\n",
    "                source['code'] = source['code'].apply(parse_program)\n",
    "                source.to_pickle(path)\n",
    "        self.sources = source\n",
    "        return source\n",
    "\n",
    "    # create clone pairs\n",
    "    def read_pairs(self, filename):\n",
    "        pairs = pd.read_pickle(self.root+self.language+'/'+filename)\n",
    "        self.pairs = pairs\n",
    "\n",
    "    # split data for training, developing and testing\n",
    "    def split_data(self):\n",
    "        data_path = self.root+self.language+'/'\n",
    "        data = self.pairs\n",
    "        data_num = len(data)\n",
    "        ratios = [int(r) for r in self.ratio.split(':')]\n",
    "        train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "        val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "\n",
    "        data = data.sample(frac=1, random_state=666)\n",
    "        train = data.iloc[:train_split]\n",
    "        dev = data.iloc[train_split:val_split]\n",
    "        test = data.iloc[val_split:]\n",
    "        # 好像这里并没能保证 train 和 test 不相交\n",
    "\n",
    "        def check_or_create(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "        train_path = data_path+'train/'\n",
    "        check_or_create(train_path)\n",
    "        self.train_file_path = train_path+'train_.pkl'\n",
    "        train.to_pickle(self.train_file_path)\n",
    "\n",
    "        dev_path = data_path+'dev/'\n",
    "        check_or_create(dev_path)\n",
    "        self.dev_file_path = dev_path+'dev_.pkl'\n",
    "        dev.to_pickle(self.dev_file_path)\n",
    "\n",
    "        test_path = data_path+'test/'\n",
    "        check_or_create(test_path)\n",
    "        self.test_file_path = test_path+'test_.pkl'\n",
    "        test.to_pickle(self.test_file_path)\n",
    "        \n",
    "        train_ids = train['id1'].append(train['id2']).unique()\n",
    "        dev_ids = dev['id1'].append(dev['id2']).unique()\n",
    "        test_ids = test['id1'].append(test['id2']).unique()\n",
    "        \n",
    "        import numpy as np\n",
    "        self.seen_ids = np.unique(np.hstack([train_ids, dev_ids]))\n",
    "        self.unseen_ids = np.setdiff1d(self.sources['id'].unique(), self.seen_ids)\n",
    "\n",
    "    # construct dictionary and train word embedding\n",
    "    def dictionary_and_embedding(self, input_file, size):\n",
    "        self.size = size\n",
    "        data_path = self.root+self.language+'/'\n",
    "        if not input_file:\n",
    "            input_file = self.train_file_path\n",
    "        pairs = pd.read_pickle(input_file)\n",
    "        train_ids = pairs['id1'].append(pairs['id2']).unique()\n",
    "\n",
    "        temp = self.sources.set_index('id',drop=False)\n",
    "        trees = temp.loc[temp.index.intersection(train_ids)]\n",
    "        if not os.path.exists(data_path+'train/embedding'):\n",
    "            os.mkdir(data_path+'train/embedding')\n",
    "        if self.language == 'c':\n",
    "            sys.path.append('../')\n",
    "            from prepare_data import get_sequences as func\n",
    "        else:\n",
    "            from utils import get_sequence as func\n",
    "\n",
    "        def trans_to_sequences(ast):\n",
    "            sequence = []\n",
    "            func(ast, sequence)\n",
    "            return sequence\n",
    "        corpus = trees['code'].apply(trans_to_sequences)\n",
    "        str_corpus = [' '.join(c) for c in corpus]\n",
    "        trees['code'] = pd.Series(str_corpus)\n",
    "        # trees.to_csv(data_path+'train/programs_ns.tsv')\n",
    "\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, max_final_vocab=3000)\n",
    "        w2v.save(data_path+'train/embedding/node_w2v_' + str(size))\n",
    "\n",
    "    # generate block sequences with index representations\n",
    "    def generate_block_seqs(self):\n",
    "        if self.language == 'c':\n",
    "            from prepare_data import get_blocks as func\n",
    "        else:\n",
    "            from utils import get_blocks_v1 as func\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "        word2vec = Word2Vec.load(self.root+self.language+'/train/embedding/node_w2v_' + str(self.size)).wv\n",
    "        vocab = word2vec.vocab\n",
    "        max_token = word2vec.vectors.shape[0]\n",
    "\n",
    "        def tree_to_index(node):\n",
    "            token = node.token\n",
    "            result = [vocab[token].index if token in vocab else max_token]\n",
    "            children = node.children\n",
    "            for child in children:\n",
    "                result.append(tree_to_index(child))\n",
    "            return result\n",
    "\n",
    "        def trans2seq(r):\n",
    "            blocks = []\n",
    "            func(r, blocks)\n",
    "            tree = []\n",
    "            for b in blocks:\n",
    "                btree = tree_to_index(b)\n",
    "                tree.append(btree)\n",
    "            return tree\n",
    "        trees = pd.DataFrame(self.sources, copy=True)\n",
    "        trees['code'] = trees['code'].apply(trans2seq)\n",
    "        if 'label' in trees.columns:\n",
    "            trees.drop('label', axis=1, inplace=True)\n",
    "        self.blocks = trees\n",
    "\n",
    "    # merge pairs\n",
    "    def merge(self,data_path,part):\n",
    "        pairs = pd.read_pickle(data_path)\n",
    "        pairs['id1'] = pairs['id1'].astype(int)\n",
    "        pairs['id2'] = pairs['id2'].astype(int)\n",
    "        df = pd.merge(pairs, self.blocks, how='left', left_on='id1', right_on='id')\n",
    "        df = pd.merge(df, self.blocks, how='left', left_on='id2', right_on='id')\n",
    "        df.drop(['id_x', 'id_y'], axis=1,inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        df.to_pickle(self.root+self.language+'/'+part+'/blocks.pkl')\n",
    "\n",
    "        \n",
    "    def generate_query_source(self):\n",
    "        self.sources['block'] = self.blocks['code']\n",
    "        self.sources.drop(columns=['code'], axis=1, inplace=True)\n",
    "        self.sources = self.sources.set_index('id')\n",
    "        self.query_source = self.sources.loc[self.sources.index.intersection(self.seen_ids)]\n",
    "        self.unseen_source = self.sources.loc[self.sources.index.intersection(self.unseen_ids)]\n",
    "        self.query_source.to_pickle(self.root+self.language+'/query_source.pkl')\n",
    "        self.unseen_source.to_pickle(self.root+self.language+'/unseen_source.pkl')\n",
    "        \n",
    "    # run for processing data to train\n",
    "    def run(self):\n",
    "        print('parse source code...')\n",
    "        self.parse_source(output_file='ast.pkl',option='existing')\n",
    "        print('read id pairs...')\n",
    "        if self.language == 'c':\n",
    "            self.read_pairs('oj_clone_ids.pkl')\n",
    "        else:\n",
    "            self.read_pairs('bcb_pair_ids.pkl')\n",
    "        print('split data...')\n",
    "        self.split_data()\n",
    "        print('train word embedding...')\n",
    "        self.dictionary_and_embedding(None,128)\n",
    "        print('generate block sequences...')\n",
    "        self.generate_block_seqs()\n",
    "        print('merge pairs and blocks...')\n",
    "        self.merge(self.train_file_path, 'train')\n",
    "        self.merge(self.dev_file_path, 'dev')\n",
    "        self.merge(self.test_file_path, 'test')\n",
    "        print('generate query source...')\n",
    "        self.generate_query_source()\n",
    "\n",
    "\n",
    "ppl = Pipeline('3:1:1', 'data/', 'java')\n",
    "ppl.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
