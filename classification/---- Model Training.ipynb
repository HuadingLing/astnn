{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:22:43.055045Z",
     "start_time": "2020-10-07T13:22:40.270466Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"D:\\\\GitHub\\\\DLpipeline\\\\dev\" not in sys.path:\n",
    "    sys.path.append(\"D:\\\\GitHub\\\\DLpipeline\\\\dev\")\n",
    "from dlpipeline import DLpipeline, Saver, FileNameManager, BasicExecutor, BasicReporter, Progbar, FormatDisplay2, reconstruct_from_cm, autolabel, plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "    \n",
    "\n",
    "class Center_loss_opt():\n",
    "    def __init__(self, optimizer_model, optimizer_centloss, criterion_cent, weight_cent):\n",
    "        self.optimizer_model = optimizer_model\n",
    "        self.optimizer_centloss = optimizer_centloss\n",
    "        self.criterion_cent = criterion_cent\n",
    "        self.weight_cent = weight_cent\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return {'optimizer_model': self.optimizer_model.state_dict(),\n",
    "                'optimizer_centloss': self.optimizer_centloss.state_dict()}\n",
    "        \n",
    "    def load_state_dict(self, d):\n",
    "        self.optimizer_model.load_state_dict(d['optimizer_model'])\n",
    "        self.optimizer_centloss.load_state_dict(d['optimizer_centloss'])\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.optimizer_model.zero_grad()\n",
    "        self.optimizer_centloss.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        self.optimizer_model.step()\n",
    "        # by doing so, weight_cent would not impact on the learning of centers\n",
    "        for param in self.criterion_cent.parameters():\n",
    "            param.grad.data *= (1. / self.weight_cent)\n",
    "        self.optimizer_centloss.step()\n",
    "        \n",
    "\n",
    "\n",
    "class ASTNN_Executor(BasicExecutor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ASTNN_Executor, self).__init__(**kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        pipeline = self.pipeline\n",
    "        device = pipeline.device\n",
    "        model = pipeline.model\n",
    "        criterion_xent = pipeline.criterion[0]\n",
    "        criterion_cent = pipeline.criterion[1]\n",
    "        \n",
    "        optimizer = pipeline.optimizer\n",
    "        weight_cent = optimizer.weight_cent\n",
    "        progressbar = pipeline.progressbar\n",
    "        \n",
    "        model.train()\n",
    "        tot_loss = tot_correct = total = 0\n",
    "        hist = []\n",
    "        batch_size = pipeline.trainloader.batch_size\n",
    "        format_display2 = FormatDisplay2(len(pipeline.trainloader.dataset))\n",
    "\n",
    "        progressbar.bar_prepare('train', _format=format_display2._format)\n",
    "        for batch_idx, (x, targets) in enumerate(pipeline.trainloader, 1):\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model.batch_size = num = targets.size(0)\n",
    "            model.hidden = model.init_hidden()\n",
    "            features, outputs = model(x)\n",
    "            \n",
    "            loss_xent = criterion_xent(outputs, targets)\n",
    "            loss_cent = criterion_cent(features, targets) * weight_cent\n",
    "            loss = loss_xent + loss_cent\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            tot_loss += batch_loss\n",
    "            total += num\n",
    "            with torch.no_grad():\n",
    "                predicted = outputs.data.max(1)[1].cpu().numpy()\n",
    "                correct = (predicted==targets.cpu().numpy()).sum()\n",
    "            tot_correct += correct\n",
    "\n",
    "            progressbar(batch_idx, (tot_loss / batch_idx if num == batch_size  # 'else' is for last batch correction， calculation is kind of weird, to prevent overflow\n",
    "                                    else (tot_loss - batch_loss * (1 - num / batch_size)) * (batch_size / len(pipeline.trainloader.dataset)),\n",
    "                                    tot_correct / total,\n",
    "                                    format_display2(tot_correct, total),\n",
    "                                    batch_loss,\n",
    "                                    correct / num,\n",
    "                                    correct,\n",
    "                                    num))\n",
    "            hist.append((batch_loss, correct))\n",
    "\n",
    "        return hist, tot_correct / total\n",
    "\n",
    "    def validation(self):\n",
    "        pipeline = self.pipeline\n",
    "        device = pipeline.device\n",
    "        model = pipeline.model\n",
    "        criterion_xent = pipeline.criterion[0]\n",
    "        criterion_cent = pipeline.criterion[1]\n",
    "        \n",
    "        weight_cent = pipeline.optimizer.weight_cent\n",
    "        progressbar = pipeline.progressbar\n",
    "        \n",
    "        model.eval()\n",
    "        tot_loss = tot_correct = total = 0\n",
    "        batch_size = pipeline.valloader.batch_size\n",
    "        format_display2 = FormatDisplay2(len(pipeline.valloader.dataset))\n",
    "\n",
    "        progressbar.bar_prepare('val', _format=format_display2._format)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, targets) in enumerate(pipeline.valloader, 1):\n",
    "                targets = targets.to(device)\n",
    "                model.batch_size = num = targets.size(0)\n",
    "                model.hidden = model.init_hidden()\n",
    "                features, outputs = model(x)\n",
    "            \n",
    "                loss_xent = criterion_xent(outputs, targets)\n",
    "                loss_cent = criterion_cent(features, targets) * weight_cent\n",
    "                loss = loss_xent + loss_cent\n",
    "\n",
    "                tot_loss += loss.item()\n",
    "                total += num\n",
    "                predicted = outputs.data.max(1)[1].cpu().numpy()\n",
    "                tot_correct += (predicted==targets.cpu().numpy()).sum()\n",
    "                val_loss = tot_loss / batch_idx if num == batch_size \\\n",
    "                            else (tot_loss - loss.item() * (1 - num / batch_size)) * (batch_size / len(pipeline.valloader.dataset))\n",
    "\n",
    "                progressbar(batch_idx, (val_loss,\n",
    "                                        tot_correct / total,\n",
    "                                        format_display2(tot_correct, total)))\n",
    "\n",
    "        return (val_loss, tot_correct), tot_correct / total\n",
    "\n",
    "    def test(self):\n",
    "        pipeline = self.pipeline\n",
    "        device = pipeline.device\n",
    "        model = pipeline.model\n",
    "        criterion_xent = pipeline.criterion[0]\n",
    "        criterion_cent = pipeline.criterion[1]\n",
    "        \n",
    "        weight_cent = pipeline.optimizer.weight_cent\n",
    "        progressbar = pipeline.progressbar\n",
    "        need_cm = pipeline.reporter.need_confusion_matrix\n",
    "        need_output = pipeline.reporter.need_output\n",
    "        need_store = need_cm or need_output\n",
    "        \n",
    "        \n",
    "        if need_store:\n",
    "            y_true = []\n",
    "            if need_cm:\n",
    "                y_pred = []\n",
    "            if need_output:\n",
    "                y_output = []\n",
    "        #labels = self.pipeline.reporter.labels\n",
    "        #cm = np.zeros((len(labels),len(labels)))\n",
    "        \n",
    "        model.eval()\n",
    "        tot_loss = tot_correct = total = 0\n",
    "        batch_size = pipeline.testloader.batch_size\n",
    "\n",
    "        progressbar.bar_prepare('test')\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, targets) in enumerate(pipeline.testloader, 1):\n",
    "                targets = targets.to(device)\n",
    "                model.batch_size = num = targets.size(0)\n",
    "                model.hidden = model.init_hidden()\n",
    "                features, outputs = model(x)\n",
    "            \n",
    "                loss_xent = criterion_xent(outputs, targets)\n",
    "                loss_cent = criterion_cent(features, targets) * weight_cent\n",
    "                loss = loss_xent + loss_cent\n",
    "\n",
    "                tot_loss += loss.item()\n",
    "                total += num\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                predicted = np.argmax(outputs, axis=1)\n",
    "                targets = targets.cpu().numpy()\n",
    "                tot_correct += (predicted==targets).sum()\n",
    "                test_loss = tot_loss / batch_idx if num == batch_size \\\n",
    "                            else (tot_loss - loss.item() * (1 - num / batch_size)) * (batch_size / len(pipeline.testloader.dataset))\n",
    "\n",
    "                progressbar(batch_idx, (test_loss,\n",
    "                                        tot_correct / total,\n",
    "                                        tot_correct,\n",
    "                                        total))\n",
    "                # precision，recall，F1 can be considered\n",
    "                \n",
    "                if need_store:\n",
    "                    y_true.extend(list(targets))\n",
    "                    if need_cm:\n",
    "                        y_pred.extend(list(predicted))\n",
    "                    if need_output:\n",
    "                        y_output.extend(list(outputs))\n",
    "                \n",
    "        test_hist = {'test loss': test_loss}\n",
    "        if need_cm:\n",
    "            test_hist['confusion matrix'] = confusion_matrix(y_true, \n",
    "                                                             y_pred,\n",
    "                                                             labels = self.pipeline.reporter.labels)\n",
    "        if need_output:\n",
    "            test_hist['y_true'] = y_true\n",
    "            test_hist['output'] = y_output\n",
    "\n",
    "        return test_hist, tot_correct / total\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:22:43.115857Z",
     "start_time": "2020-10-07T13:22:43.057039Z"
    }
   },
   "outputs": [],
   "source": [
    "class Reporter_c2(BasicReporter):\n",
    "    def __init__(self, \n",
    "                 labels = None,  # numerical value label of the classes\n",
    "                 class_names = None,  # string, use to display the classes\n",
    "                 need_output = True,  # whether to store the output of model during testing. useful for ROC and AUC\n",
    "                 need_confusion_matrix = False, # whether to store confusion matrix during testing\n",
    "                 output_to_score_fun = None,  # function to transfer the output of model to score-like value (i.e., sofmax, normalized, sum is 1)\n",
    "                 batch_figsize = (14, 6),  # figsize of batch-loss-acc figure in the report\n",
    "                 epoch_figsize = (10, 6), # figsize of epoch-loss-acc figure in the report\n",
    "                 cm_figsize = (10, 8),  # figsize of confusion matrix figure in the report\n",
    "                 cr_figsize = (18, 8),  # figsize of (sklearn's) classification_report figure in the report\n",
    "                 roc_figsize = (10, 8),  # figsize of ROC curve figure in the report\n",
    "                 **kwargs):\n",
    "        super(Reporter_c2, self).__init__(**kwargs)\n",
    "        \n",
    "        self.labels = labels\n",
    "        if class_names is None:\n",
    "            if labels is not None:\n",
    "                self.class_names = [str(label) for label in labels]\n",
    "            else:\n",
    "                print('Error for the labels.')\n",
    "        else:\n",
    "            self.class_names = class_names\n",
    "        assert len(self.labels) == len(self.class_names)\n",
    "        self.need_output = need_output\n",
    "        self.need_confusion_matrix = need_confusion_matrix\n",
    "        self.output_to_score_fun = output_to_score_fun\n",
    "        self.batch_figsize = batch_figsize\n",
    "        self.epoch_figsize = epoch_figsize\n",
    "        self.cm_figsize = cm_figsize\n",
    "        self.cr_figsize = cr_figsize\n",
    "        self.roc_figsize = roc_figsize\n",
    "        \n",
    "        \n",
    "    def check_and_report(self):\n",
    "        if self.report_interval > 0 and (self.pipeline.epoch - self.pipeline.start_epoch + 1) % self.report_interval == 0:\n",
    "            self.history['epoch'] = self.pipeline.epoch\n",
    "            if self.show_train_report or self.pipeline.save_train_report:\n",
    "                self.plot_train(hist = self.history, in_train = True)\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        self.plot_hist(self.history, **kwargs)\n",
    "        pass\n",
    "    \n",
    "    def plot_hist(self, hist, modes = 'all', **kwargs):\n",
    "        if isinstance(modes, str):\n",
    "            modes = [modes]\n",
    "        assert isinstance(modes, list)\n",
    "        if ('train' in modes or 'all' in modes) and 'train' in hist.keys() and hist['train']:\n",
    "            self.plot_train(hist, 'val' in modes or 'all' in modes, **kwargs)\n",
    "        if ('test' in modes or 'all' in modes) and 'test' in hist.keys() and hist['test']:\n",
    "            self.plot_test(hist)\n",
    "\n",
    "    def plot_train(self, hist, plot_val = True, in_train = False, drop_epochs = 0):\n",
    "        start_epoch = hist['start epoch']\n",
    "        temp_epoch = hist['epoch']\n",
    "        e = range(start_epoch, temp_epoch + 1)\n",
    "        batch_size = hist['batch size']\n",
    "        train_size = hist['trainset size']\n",
    "        last_size = train_size % batch_size  # size of the last batch\n",
    "        iters = train_size // batch_size\n",
    "        train_batch_loss = [l2[0] for l1 in hist['train'] for l2 in l1]\n",
    "        train_batch_correct = [l2[1] for l1 in hist['train'] for l2 in l1]\n",
    "        # train_loss = [l2[0]*batch_size for l1 in hist['train'] for l2 in l1]\n",
    "\n",
    "        if iters == len(hist['train'][0]):  # len(hist['train'][0]) indicates how many train_batch in one epoch\n",
    "            # no remnant batch\n",
    "            train_batch_acc = [l2[1]/batch_size for l1 in hist['train'] for l2 in l1]\n",
    "\n",
    "            train_loss = np.array(train_batch_loss).reshape(-1, iters)\n",
    "            train_acc = np.array(train_batch_acc).reshape(-1, iters)\n",
    "            train_loss = np.mean(train_loss, axis=1)\n",
    "            train_acc = np.mean(train_acc, axis=1)\n",
    "\n",
    "        else:  # the last train_batch is remnant\n",
    "            train_batch_acc = [(l1[i][1]/batch_size if i < iters else l1[i][1]/last_size)\n",
    "                               for l1 in hist['train'] for i in range(iters+1)]\n",
    "\n",
    "            train_loss = np.array(train_batch_loss).reshape(-1, iters+1)\n",
    "            train_acc = np.array(train_batch_correct).reshape(-1, iters+1)\n",
    "\n",
    "            train_loss = (np.sum(train_loss[:,:-1], axis=1) * batch_size + train_loss[:,-1] * last_size) / train_size\n",
    "            train_acc = np.sum(train_acc, axis=1) / train_size\n",
    "        \n",
    "        iters = range(1, len(train_batch_loss)+1)\n",
    "        if drop_epochs > 0:\n",
    "            if drop_epochs > len(hist['train']):\n",
    "                drop_epochs = len(hist['train'])\n",
    "            drop_iters = drop_epochs*len(hist['train'][0])\n",
    "            iters = iters[drop_iters:]\n",
    "            train_batch_loss = train_batch_loss[drop_iters:]\n",
    "            train_batch_acc = train_batch_acc[drop_iters:]\n",
    "            \n",
    "        fig, ax1 = plt.subplots(figsize=self.batch_figsize, facecolor='white')\n",
    "        color = 'C0'\n",
    "        ax1.set_xlabel('Iters')\n",
    "        ax1.set_ylabel('Batch Loss', color=color)\n",
    "        ax1.scatter(iters, train_batch_loss, s=8, color=color, marker='2', label='batch loss')\n",
    "        # marker: ascent uses '1', descent used '2', and more like '.', '+', 'x' and '|' ,just personal preference\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'C2'\n",
    "        ax2.set_ylabel('Batch Acc', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.scatter(iters, train_batch_acc, s=8, color=color, marker='1', label='batch acc')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='lower left')\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        if self.pipeline.saver.save_train_report or (not in_train and self.pipeline.saver.save_test_report):\n",
    "            # not in train equal to in test, depend on saver.save_test_report at this time\n",
    "            save_dir = os.path.join(self.pipeline.saver.execute_save_dir, self.pipeline.file_name_manager('report train'))\n",
    "            fig.savefig(save_dir, transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "        if not in_train or self.show_train_report:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=self.epoch_figsize, facecolor='white')\n",
    "        plt.grid(True, which='major', axis='y')  # place grid first, to prevent it cover the line follow-up (put grid in the bottom layer)\n",
    "        color = 'C0'\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss', color=color)\n",
    "        ax1.plot(e[drop_epochs:], train_loss[drop_epochs:], linestyle='-.', color=color, label='train loss')\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'C2'\n",
    "        ax2.set_ylabel('Acc', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(e[drop_epochs:], train_acc[drop_epochs:], linestyle='-.', color=color, label='train acc')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "        if plot_val and 'val' in hist.keys() and hist['val']:\n",
    "            val_loss = [l[0] for l in hist['val']]\n",
    "            val_acc = [l[1]/hist['valset size'] for l in hist['val']]\n",
    "            if len(val_loss) == len(e):\n",
    "                ev = e\n",
    "            else:  # val incomplete, maybe some experiments haven't setup valloader\n",
    "                ev = range(temp_epoch - len(val_loss) + 1, temp_epoch + 1)\n",
    "\n",
    "            ax1.plot(e[drop_epochs:], val_loss[drop_epochs:], color='C4', label='val loss')\n",
    "            ax2.plot(e[drop_epochs:], val_acc[drop_epochs:], color='C1', label='val acc')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='lower left')\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        if self.pipeline.saver.save_train_report or (not in_train and self.pipeline.saver.save_test_report):\n",
    "            save_dir = os.path.join(self.pipeline.saver.execute_save_dir, self.pipeline.file_name_manager('report val'))\n",
    "            fig.savefig(save_dir, transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "        if not in_train or self.show_train_report:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_test(self, hist):\n",
    "        return\n",
    "        test_dict = hist['test']\n",
    "        y_true = None\n",
    "        y_score = None\n",
    "        y_pred = None\n",
    "        cm = None\n",
    "        \n",
    "        # first see which we can get: y_true, y_score, y_pred, cm\n",
    "        # use y_score can get y_pred\n",
    "        # use y_true and y_pred can get cm\n",
    "        # use cm can get y_true and y_pred\n",
    "        if 'confusion matrix' in test_dict.keys():\n",
    "            cm = test_dict['confusion matrix']\n",
    "            # tn, fp, fn, tp = cm.ravel()\n",
    "        if 'y_true' in test_dict.keys() and 'output' in test_dict.keys():\n",
    "            y_true = test_dict['y_true']\n",
    "            if self.output_to_score_fun:\n",
    "                y_score = self.output_to_score_fun(test_dict['output'])\n",
    "            else:\n",
    "                y_score = np.array(test_dict['output'])  # make sure it's ndarray\n",
    "            y_pred = np.where(y_score < 0.5, 1, 0)\n",
    "            if cm is None:\n",
    "                cm = confusion_matrix(y_true, y_pred, labels = self.labels)\n",
    "\n",
    "        if cm is not None:\n",
    "            assert len(cm) == len(self.labels)\n",
    "            if self.pipeline.saver.save_test_report:\n",
    "                save_dir = os.path.join(self.pipeline.saver.execute_save_dir, self.pipeline.file_name_manager('report cm'))\n",
    "            else:\n",
    "                save_dir = None\n",
    "            plot_confusion_matrix(cm, labels = self.labels, figsize=self.cm_figsize, \n",
    "                                  title = 'Confusion matrix',\n",
    "                                  cmap='BuGn',  # can also use cmap='GnBu', just personal preference\n",
    "                                  #xticks_rotation = 30,\n",
    "                                  axes_style='dark', context_mode='notebook',\n",
    "                                  save_dir = save_dir, transparent=False, dpi=80, bbox_inches=\"tight\") \n",
    "            print('Confusion matrix:')\n",
    "            print(cm)  # display numerical value (text)\n",
    "\n",
    "            if y_true is None or y_pred is None:\n",
    "                y_true, y_pred = reconstruct_from_cm(cm)\n",
    "            cr_dict = classification_report(y_true, y_pred, labels = self.labels, target_names = self.class_names, digits = 4, output_dict = True)\n",
    "            self.plot_classification_report(cr_dict, self.cr_figsize)\n",
    "            cr = classification_report(y_true, y_pred, labels = self.labels, target_names = self.class_names, digits = 4, output_dict = False)\n",
    "            print('\\nClassification report:\\n')\n",
    "            print(cr)  # display classification report (text)\n",
    "            print()\n",
    "        if y_true is not None and y_score is not None:\n",
    "            self.plot_roc(y_true, 1-y_score)\n",
    "            \n",
    "            \n",
    "    def plot_classification_report(self, cr, figsize=(16, 8), width=0.24):\n",
    "        return\n",
    "        classes_labels = list(self.class_names)\n",
    "        classes_labels.append('macro avg')\n",
    "        classes_labels.append('weighted avg')\n",
    "        precision = [cr[cl]['precision'] for cl in classes_labels]\n",
    "        recall = [cr[cl]['recall'] for cl in classes_labels]\n",
    "        f1 = [cr[cl]['f1-score'] for cl in classes_labels]\n",
    "\n",
    "        classes_x = np.arange(len(classes_labels))  # the label locations\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize, facecolor='w')\n",
    "        ax.grid(True, which='major', axis='y')\n",
    "\n",
    "        rects_precision = ax.bar(classes_x - width, precision, width, label='precision')\n",
    "        rects_recall = ax.bar(classes_x, recall, width, label='recall')\n",
    "        rects_f1 = ax.bar(classes_x + width, f1, width, label='f1-score')\n",
    "        rects_acc = ax.bar(len(classes_x) - width, cr['accuracy'], width*1.2, color='C7', label='Acc')\n",
    "\n",
    "        xticks = [i for i in classes_x]\n",
    "        xticks.append(len(classes_x)-width)\n",
    "        xticklabels = [cl+'\\n('+str(cr[cl]['support']) + ')' for cl in classes_labels]\n",
    "        xticklabels.append('Acc\\n({:})'.format(cr['macro avg']['support']))\n",
    "\n",
    "        ax.set_ylabel('Scores')\n",
    "        ax.set_ylim([0.0, 1.02])\n",
    "        ax.set_title('Scores by Multi-class')\n",
    "        ax.set_xlabel('Items and support')\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels)\n",
    "        ax.set_xlim([-3*width, len(classes_x)+width])\n",
    "        ax.legend()\n",
    "\n",
    "        autolabel(ax, rects_precision)\n",
    "        autolabel(ax, rects_recall)\n",
    "        autolabel(ax, rects_f1)\n",
    "        autolabel(ax, rects_acc)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        if self.pipeline.saver.save_test_report:\n",
    "            save_dir = os.path.join(self.pipeline.saver.execute_save_dir, self.pipeline.file_name_manager('report cr'))\n",
    "            fig.savefig(save_dir, transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "            \n",
    "    def plot_roc(self, y_true, y_score):\n",
    "        return\n",
    "        assert len(y_true) == len(y_score)\n",
    "        #assert len(self.class_names) == y_score.shape[1]\n",
    "        macro_roc_auc = roc_auc_score(y_true, y_score, average=\"macro\")\n",
    "        weighted_roc_auc = roc_auc_score(y_true, y_score, average=\"weighted\")\n",
    "        print(\"ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} (weighted by prevalence)\"\n",
    "              .format(macro_roc_auc, weighted_roc_auc))\n",
    "        \n",
    "        p = self.class_names[1]\n",
    "        n = self.class_names[0]\n",
    "        \n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        \n",
    "        \n",
    "        fpr[p], tpr[p], _ = roc_curve(y_true, y_score, pos_label=self.labels[1])\n",
    "        roc_auc[p] = auc(fpr[p], tpr[p])\n",
    "        fpr[n], tpr[n], _ = roc_curve(y_true, 1-y_score, pos_label=self.labels[0])\n",
    "        roc_auc[n] = auc(fpr[n], tpr[n])\n",
    "        \n",
    "        \n",
    "        xlim1, ylim1 = [-0.02, 1.0], [0.0, 1.02]\n",
    "        \n",
    "        plt.figure(figsize=(10,8), facecolor='white')\n",
    "        plt.grid(True, which='major', axis='both')\n",
    "        \n",
    "        plt.plot(fpr[p], tpr[p], lw=1.0, linestyle='--', label='class {0} (area = {1:0.4f})'.format(p, roc_auc[p]))\n",
    "        plt.plot(fpr[n], tpr[n], lw=1.0, linestyle='--', label='class {0} (area = {1:0.4f})'.format(n, roc_auc[n]))\n",
    "        \n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='darkslategray', lw=1.0, linestyle='--')\n",
    "        plt.xlim(xlim1)\n",
    "        plt.ylim(ylim1)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves of multi-class')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        if save_dir is not None:\n",
    "            plt.savefig(save_dir, transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fpr = {}\n",
    "        tpr = {}\n",
    "        roc_auc = {}\n",
    "        y_true_one_hot = np.zeros((y_score.shape[0], len(self.labels)))\n",
    "        for i, j in enumerate(y_true):\n",
    "            y_true_one_hot[i][int(j)] = 1\n",
    "        for i, j in enumerate(self.class_names):\n",
    "            fpr[j], tpr[j], _ = roc_curve(y_true_one_hot[:,i], y_score[:,i])\n",
    "            roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_one_hot.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in self.class_names]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in self.class_names:\n",
    "            mean_tpr += scipy.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= len(self.class_names)\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        xlim1, ylim1 = [-0.02, 1.0], [0.0, 1.02]\n",
    "        xlim2, ylim2 = [-0.01, 0.5], [0.5, 1.01]\n",
    "        if self.pipeline.saver.save_test_report:\n",
    "            save_dir = os.path.join(self.pipeline.saver.execute_save_dir, self.pipeline.file_name_manager('report roc'))\n",
    "            self.plot_roc_curves(fpr, tpr, roc_auc, figsize=self.roc_figsize, xlim=xlim1, ylim=ylim1, save_dir = save_dir)\n",
    "            self.plot_roc_curves(fpr, tpr, roc_auc, figsize=self.roc_figsize, xlim=xlim2, ylim=ylim2, save_dir = save_dir[:-4]+' enlarge' + save_dir[-4:])  # enlarge version\n",
    "        else:\n",
    "            self.plot_roc_curves(fpr, tpr, roc_auc, figsize=self.roc_figsize, xlim=xlim1, ylim=ylim1)\n",
    "            self.plot_roc_curves(fpr, tpr, roc_auc, figsize=self.roc_figsize, xlim=xlim2, ylim=ylim2)  # enlarge version\n",
    "        \n",
    "    def plot_roc_curves(self, fpr, tpr, roc_auc, plot_all_classes=True, figsize=(12, 10), xlim=[-0.02, 1.0], ylim=[0.0, 1.02], save_dir=None):\n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=figsize, facecolor='white')\n",
    "        plt.grid(True, which='major', axis='both')\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average (area = {0:0.4f})'.format(roc_auc[\"micro\"]),\n",
    "                 linestyle='-', linewidth=1.5)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average (area = {0:0.4f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 linestyle='-', linewidth=1.5)\n",
    "        if plot_all_classes:\n",
    "            for i in self.class_names:\n",
    "                plt.plot(fpr[i], tpr[i], lw=1.0, linestyle='--',\n",
    "                         label='class {0} (area = {1:0.4f})'.format(i, roc_auc[i]))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='darkslategray', lw=1.0, linestyle='--')\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves of multi-class')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        if save_dir is not None:\n",
    "            plt.savefig(save_dir, transparent=False, dpi=80, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:22:44.101222Z",
     "start_time": "2020-10-07T13:22:43.117854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:102: UserWarning: \n",
      "    Found GPU0 GeForce GT 730 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n",
      "C:\\Users\\A1\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:125: UserWarning: \n",
      "GeForce GT 730 with CUDA capability sm_30 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the GeForce GT 730 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramCC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.get_device_name(0) == 'GeForce GT 730':\n",
    "            device = 'cpu'\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return torch.device(device)\n",
    "\n",
    "device = get_device()\n",
    "use_gpu = False if device.type == 'cpu' else True\n",
    "\n",
    "save_dir = '---- classification model/'\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=2, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss\n",
    "\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 104\n",
    "\n",
    "criterion_cent = CenterLoss(LABELS, 2*HIDDEN_DIM, use_gpu)\n",
    "\n",
    "\n",
    "basic_config = {'executor': ASTNN_Executor(),\n",
    "                'progressbar': Progbar(dynamic = False),\n",
    "                'reporter': Reporter_c2(labels = [i for i in range(LABELS)], \n",
    "                                        need_output = False,\n",
    "                                        need_confusion_matrix = False,\n",
    "                                        output_to_score_fun = None, \n",
    "                                        report_interval = 2,\n",
    "                                        show_train_report = True,\n",
    "                                        summary_fun = None),\n",
    "                'saver': Saver(save_dir = save_dir,\n",
    "                               save_meta_file = True,\n",
    "                               save_ckpt_model = True,\n",
    "                               save_val_model = True, \n",
    "                               save_final_model = True,\n",
    "                               save_final_optim = True,\n",
    "                               save_interval = 1, \n",
    "                               test_model_use = 'final', \n",
    "                               save_history = True,\n",
    "                               save_train_report = True,\n",
    "                               save_test_report = True),\n",
    "                'file_name_manager': FileNameManager(),\n",
    "                'device': device,\n",
    "                'criterion': [nn.CrossEntropyLoss(), criterion_cent],\n",
    "               }\n",
    "\n",
    "pipeline = DLpipeline(**basic_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:22:50.929147Z",
     "start_time": "2020-10-07T13:22:44.102244Z"
    }
   },
   "outputs": [],
   "source": [
    "root = 'data/'\n",
    "train_data = pd.read_pickle(root+'train/blocks.pkl')#.sample(frac=0.1)\n",
    "validation_data = pd.read_pickle(root + 'dev/blocks.pkl')#.sample(frac=0.1)\n",
    "test_data = pd.read_pickle(root+'test/blocks.pkl')#.sample(frac=0.1)\n",
    "train_data['label'] -= 1  # from [1, 104] to [0, 103]\n",
    "validation_data['label'] -= 1\n",
    "test_data['label'] -= 1\n",
    "\n",
    "\n",
    "word2vec = Word2Vec.load(root+\"train/embedding/node_w2v_128\").wv\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "embeddings[:MAX_TOKENS] = word2vec.vectors\n",
    "\n",
    "\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(device, embeddings):\n",
    "    model = BatchProgramCC(EMBEDDING_DIM,\n",
    "                           HIDDEN_DIM,\n",
    "                           MAX_TOKENS+1,\n",
    "                           ENCODE_DIM,\n",
    "                           LABELS,\n",
    "                           BATCH_SIZE,\n",
    "                           device,\n",
    "                           embeddings)\n",
    "\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:22:50.941966Z",
     "start_time": "2020-10-07T13:22:50.930989Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, dataset, batch_size, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.suffle_id = []\n",
    "        self.idx = 0\n",
    "        self.max_idx = len(dataset)\n",
    "        self.len = math.ceil(len(self.dataset) / self.batch_size)\n",
    "        \n",
    "    def __call__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __iter__(self):\n",
    "        #self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        if self.shuffle:\n",
    "            self.dataset = sklearn.utils.shuffle(self.dataset)\n",
    "        self.idx = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx >= self.max_idx:\n",
    "            raise StopIteration\n",
    "        tmp = self.dataset.iloc[self.idx: self.idx+self.batch_size]\n",
    "        x, labels = [], []\n",
    "        for _, item in tmp.iterrows():\n",
    "            x.append(item[1])\n",
    "            labels.append(item[2])\n",
    "        self.idx += self.batch_size\n",
    "        return x, torch.LongTensor(labels)  #torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "def get_config(train_data, validation_data, test_data, batch_size, device, embeddings, criterion_cent, weight_cent=1.0):\n",
    "    model = get_model(device, embeddings)\n",
    "    optimizer = Center_loss_opt(torch.optim.Adamax(model.parameters()),\n",
    "                                torch.optim.Adamax(criterion_cent.parameters(), lr=0.5),\n",
    "                                criterion_cent,\n",
    "                                weight_cent)\n",
    "    \n",
    "    config = {'trainloader': dataloader(train_data, batch_size, True),\n",
    "              'valloader': dataloader(validation_data, batch_size, False),\n",
    "              'testloader': dataloader(test_data, batch_size, False),\n",
    "              'model': model,\n",
    "              'model_name': 'model for classification task',\n",
    "              'optimizer': optimizer\n",
    "             }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:24:44.755589Z",
     "start_time": "2020-10-07T13:22:50.943925Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ...\n",
      "Load history successfully.\n",
      "Load model successfully.\n",
      "Load optimizer successfully.\n",
      "From epoch 64 by ckpt file {---- classification model\\20201006_151536 model for classification task\\20201007_205913 model for classification task epoch_64 final.pt}\n",
      "Start testing, test on 10401 samples.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                       Time used                 ETA         Loss     Acc                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[>...........................................................]  1/82   1s451ms (1s451ms/step)    1m57s       135.2104   0.9688 (124/128)\n",
      "[>...........................................................]  2/82   2s865ms (1s414ms/step)    1m55s       134.4055   0.9688 (248/256)\n",
      "[=>..........................................................]  3/82   4s302ms (1s437ms/step)    1m51s       134.8063   0.9635 (370/384)\n",
      "[=>..........................................................]  4/82   5s416ms (1s114ms/step)    1m49s       135.1967   0.9688 (496/512)\n",
      "[==>.........................................................]  5/82   6s524ms (1s108ms/step)    1m25s       135.3352   0.9734 (623/640)\n",
      "[===>........................................................]  6/82   7s688ms (1s163ms/step)    1m24s       135.1105   0.9727 (747/768)\n",
      "[====>.......................................................]  7/82   8s719ms (1s31ms/step)     1m26s       135.1157   0.9743 (873/896)\n",
      "[====>.......................................................]  8/82   10s128ms (1s409ms/step)   1m19s       135.0087   0.9756 (999/1024)\n",
      "[=====>......................................................]  9/82   12s35ms (1s906ms/step)    1m46s       134.9414   0.9757 (1124/1152)\n",
      "[======>.....................................................] 10/82   13s387ms (1s351ms/step)   2m13s       134.9800   0.9766 (1250/1280)\n",
      "[=======>....................................................] 11/82   14s825ms (1s438ms/step)   1m36s       135.3221   0.9780 (1377/1408)\n",
      "[=======>....................................................] 12/82   16s249ms (1s424ms/step)   1m40s       135.3391   0.9779 (1502/1536)\n",
      "[========>...................................................] 13/82   18s200ms (1s950ms/step)   1m41s       135.3104   0.9766 (1625/1664)\n",
      "[=========>..................................................] 14/82   19s832ms (1s632ms/step)   2m10s       135.3823   0.9777 (1752/1792)\n",
      "[=========>..................................................] 15/82   21s592ms (1s759ms/step)   1m50s       135.2732   0.9766 (1875/1920)\n",
      "[==========>.................................................] 16/82   22s873ms (1s281ms/step)   1m52s       135.2462   0.9756 (1998/2048)\n",
      "[===========>................................................] 17/82   24s292ms (1s418ms/step)   1m24s       135.2439   0.9756 (2123/2176)\n",
      "[============>...............................................] 18/82   25s433ms (1s141ms/step)   1m28s       135.1444   0.9753 (2247/2304)\n",
      "[============>...............................................] 19/82   27s14ms (1s580ms/step)    1m14s       135.2275   0.9749 (2371/2432)\n",
      "[=============>..............................................] 20/82   28s657ms (1s642ms/step)   1m38s       135.1919   0.9746 (2495/2560)\n",
      "[==============>.............................................] 21/82   29s970ms (1s313ms/step)   1m38s       135.1610   0.9747 (2620/2688)\n",
      "[===============>............................................] 22/82   31s186ms (1s215ms/step)   1m18s       135.1731   0.9744 (2744/2816)\n",
      "[===============>............................................] 23/82   33s156ms (1s969ms/step)   1m16s       135.2747   0.9745 (2869/2944)\n",
      "[================>...........................................] 24/82   34s715ms (1s558ms/step)   1m51s       135.2935   0.9746 (2994/3072)\n",
      "[=================>..........................................] 25/82   36s219ms (1s503ms/step)   1m28s       135.2276   0.9744 (3118/3200)\n",
      "[==================>.........................................] 26/82   37s499ms (1s280ms/step)   1m22s       135.2115   0.9745 (3243/3328)\n",
      "[==================>.........................................] 27/82   38s666ms (1s166ms/step)   1m9s        135.2166   0.9748 (3369/3456)\n",
      "[===================>........................................] 28/82   40s28ms (1s362ms/step)    1m4s        135.1998   0.9743 (3492/3584)\n",
      "[====================>.......................................] 29/82   41s538ms (1s509ms/step)   1m12s       135.1850   0.9744 (3617/3712)\n",
      "[====================>.......................................] 30/82   42s947ms (1s408ms/step)   1m17s       135.1732   0.9745 (3742/3840)\n",
      "[=====================>......................................] 31/82   44s62ms (1s115ms/step)    1m10s       135.1026   0.9751 (3869/3968)\n",
      "[======================>.....................................] 32/82   45s657ms (1s595ms/step)   58s154ms    135.1187   0.9749 (3993/4096)\n",
      "[=======================>....................................] 33/82   47s81ms (1s423ms/step)    1m17s       135.1090   0.9751 (4119/4224)\n",
      "[=======================>....................................] 34/82   48s179ms (1s98ms/step)    1m6s        135.0984   0.9747 (4242/4352)\n",
      "[========================>...................................] 35/82   49s172ms (993ms/step)     51s116ms    135.0991   0.9743 (4365/4480)\n",
      "[=========================>..................................] 36/82   50s681ms (1s508ms/step)   48s65ms     135.1094   0.9742 (4489/4608)\n",
      "[==========================>.................................] 37/82   52s894ms (2s213ms/step)   1m11s       135.1023   0.9740 (4613/4736)\n",
      "[==========================>.................................] 38/82   54s311ms (1s417ms/step)   1m33s       135.0378   0.9739 (4737/4864)\n",
      "[===========================>................................] 39/82   55s687ms (1s375ms/step)   1m759ms     135.0540   0.9740 (4862/4992)\n",
      "[============================>...............................] 40/82   56s905ms (1s218ms/step)   57s105ms    135.0678   0.9738 (4986/5120)\n",
      "[=============================>..............................] 41/82   58s189ms (1s283ms/step)   50s234ms    135.1032   0.9743 (5113/5248)\n",
      "[=============================>..............................] 42/82   59s434ms (1s244ms/step)   51s187ms    135.1260   0.9743 (5238/5376)\n",
      "[==============================>.............................] 43/82   1m1s (1s732ms/step)       50s444ms    135.1446   0.9747 (5365/5504)\n",
      "[===============================>............................] 44/82   1m2s (1s766ms/step)       1m5s        135.1750   0.9744 (5488/5632)\n",
      "[===============================>............................] 45/82   1m4s (1s665ms/step)       1m4s        135.2629   0.9747 (5614/5760)\n",
      "[================================>...........................] 46/82   1m6s (1s878ms/step)       1m728ms     135.2350   0.9749 (5740/5888)\n",
      "[=================================>..........................] 47/82   1m7s (1s453ms/step)       1m4s        135.2126   0.9749 (5865/6016)\n",
      "[==================================>.........................] 48/82   1m9s (1s407ms/step)       49s249ms    135.1914   0.9746 (5988/6144)\n",
      "[==================================>.........................] 49/82   1m10s (1s478ms/step)      46s672ms    135.1601   0.9746 (6113/6272)\n",
      "[===================================>........................] 50/82   1m12s (1s334ms/step)      46s837ms    135.1773   0.9744 (6236/6400)\n",
      "[====================================>.......................] 51/82   1m13s (1s340ms/step)      41s385ms    135.1888   0.9743 (6360/6528)\n",
      "[=====================================>......................] 52/82   1m14s (992ms/step)        39s168ms    135.1764   0.9742 (6484/6656)\n",
      "[=====================================>......................] 53/82   1m15s (1s222ms/step)      29s446ms    135.1702   0.9741 (6608/6784)\n",
      "[======================================>.....................] 54/82   1m16s (1s175ms/step)      34s105ms    135.1829   0.9740 (6732/6912)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================>....................] 55/82   1m18s (1s642ms/step)      33s8ms      135.1999   0.9740 (6857/7040)\n",
      "[=======================================>....................] 56/82   1m20s (1s546ms/step)      42s458ms    135.1828   0.9743 (6984/7168)\n",
      "[========================================>...................] 57/82   1m21s (1s353ms/step)      38s187ms    135.1970   0.9737 (7104/7296)\n",
      "[=========================================>..................] 58/82   1m22s (1s368ms/step)      32s517ms    135.2425   0.9735 (7227/7424)\n",
      "[==========================================>.................] 59/82   1m24s (1s252ms/step)      31s205ms    135.2331   0.9733 (7350/7552)\n",
      "[==========================================>.................] 60/82   1m25s (1s11ms/step)       27s27ms     135.2458   0.9734 (7476/7680)\n",
      "[===========================================>................] 61/82   1m26s (1s384ms/step)      22s20ms     135.2652   0.9735 (7601/7808)\n",
      "[============================================>...............] 62/82   1m27s (1s143ms/step)      27s205ms    135.2747   0.9738 (7728/7936)\n",
      "[=============================================>..............] 63/82   1m28s (1s212ms/step)      21s865ms    135.2360   0.9736 (7851/8064)\n",
      "[=============================================>..............] 64/82   1m30s (1s329ms/step)      22s39ms     135.2532   0.9736 (7976/8192)\n",
      "[==============================================>.............] 65/82   1m31s (1s416ms/step)      22s748ms    135.2296   0.9736 (8100/8320)\n",
      "[===============================================>............] 66/82   1m32s (1s354ms/step)      22s560ms    135.2269   0.9730 (8220/8448)\n",
      "[================================================>...........] 67/82   1m34s (1s196ms/step)      20s79ms     135.2795   0.9731 (8345/8576)\n",
      "[================================================>...........] 68/82   1m35s (1s148ms/step)      16s688ms    135.3037   0.9731 (8470/8704)\n",
      "[=================================================>..........] 69/82   1m36s (1s645ms/step)      15s581ms    135.3167   0.9731 (8594/8832)\n",
      "[==================================================>.........] 70/82   1m38s (1s398ms/step)      19s450ms    135.3172   0.9733 (8721/8960)\n",
      "[==================================================>.........] 71/82   1m39s (1s240ms/step)      15s207ms    135.3328   0.9735 (8847/9088)\n",
      "[===================================================>........] 72/82   1m40s (1s184ms/step)      12s350ms    135.3474   0.9737 (8974/9216)\n",
      "[====================================================>.......] 73/82   1m42s (1s477ms/step)      10s926ms    135.3312   0.9737 (9098/9344)\n",
      "[=====================================================>......] 74/82   1m43s (1s424ms/step)      11s774ms    135.3100   0.9739 (9225/9472)\n",
      "[=====================================================>......] 75/82   1m44s (1s184ms/step)      9s801ms     135.3331   0.9740 (9350/9600)\n",
      "[======================================================>.....] 76/82   1m45s (902ms/step)        6s939ms     135.3229   0.9742 (9477/9728)\n",
      "[=======================================================>....] 77/82   1m46s (1s236ms/step)      4s679ms     135.3227   0.9744 (9604/9856)\n",
      "[========================================================>...] 78/82   1m48s (1s536ms/step)      5s66ms      135.3182   0.9745 (9729/9984)\n",
      "[========================================================>...] 79/82   1m49s (1s495ms/step)      4s598ms     135.3390   0.9747 (9856/10112)\n",
      "[=========================================================>..] 80/82   1m51s (1s301ms/step)      2s951ms     135.3414   0.9746 (9980/10240)\n",
      "[==========================================================>.] 81/82   1m53s (1s995ms/step)      1s370ms     135.3599   0.9748 (10107/10368)\n",
      "[============================================================] 82/82   1m53s (457ms/step)        0ms         135.3534   0.9748 (10139/10401)\n"
     ]
    }
   ],
   "source": [
    "config = get_config(train_data, validation_data, test_data, BATCH_SIZE, device, embeddings, criterion_cent, 0.5)\n",
    "pipeline.setup(**config)\n",
    "\n",
    "pipeline.load('pipeline', save_dir+'20201006_151536 model for classification task/20201007_205913 model for classification task epoch_64 final.pt')\n",
    "pipeline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T13:26:39.099832Z",
     "start_time": "2020-10-07T13:24:44.758582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ...\n",
      "Load history successfully.\n",
      "Load model successfully.\n",
      "Load optimizer successfully.\n",
      "From epoch 62 by ckpt file {---- classification model\\20201006_151536 model for classification task\\20201007_202224 model for classification task epoch_62 val.pt}\n",
      "Start testing, test on 10401 samples.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                       Time used                 ETA         Loss     Acc                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[>...........................................................]  1/82   1s561ms (1s561ms/step)    2m6s        133.5303   0.9688 (124/128)\n",
      "[>...........................................................]  2/82   2s911ms (1s349ms/step)    2m3s        132.7309   0.9727 (249/256)\n",
      "[=>..........................................................]  3/82   4s331ms (1s420ms/step)    1m47s       133.1219   0.9740 (374/384)\n",
      "[=>..........................................................]  4/82   5s443ms (1s112ms/step)    1m48s       133.5245   0.9766 (500/512)\n",
      "[==>.........................................................]  5/82   6s530ms (1s87ms/step)     1m25s       133.6656   0.9797 (627/640)\n",
      "[===>........................................................]  6/82   7s728ms (1s197ms/step)    1m23s       133.4497   0.9792 (752/768)\n",
      "[====>.......................................................]  7/82   8s766ms (1s38ms/step)     1m28s       133.4637   0.9799 (878/896)\n",
      "[====>.......................................................]  8/82   10s202ms (1s436ms/step)   1m19s       133.3621   0.9795 (1003/1024)\n",
      "[=====>......................................................]  9/82   11s978ms (1s776ms/step)   1m47s       133.3060   0.9792 (1128/1152)\n",
      "[======>.....................................................] 10/82   13s284ms (1s305ms/step)   2m4s        133.3408   0.9797 (1254/1280)\n",
      "[=======>....................................................] 11/82   14s658ms (1s374ms/step)   1m33s       133.6744   0.9815 (1382/1408)\n",
      "[=======>....................................................] 12/82   16s81ms (1s423ms/step)    1m36s       133.6898   0.9811 (1507/1536)\n",
      "[========>...................................................] 13/82   18s266ms (2s184ms/step)   1m43s       133.6637   0.9796 (1630/1664)\n",
      "[=========>..................................................] 14/82   20s86ms (1s820ms/step)    2m26s       133.7371   0.9810 (1758/1792)\n",
      "[=========>..................................................] 15/82   21s783ms (1s697ms/step)   2m1s        133.6297   0.9807 (1883/1920)\n",
      "[==========>.................................................] 16/82   23s117ms (1s333ms/step)   1m49s       133.6068   0.9795 (2006/2048)\n",
      "[===========>................................................] 17/82   24s546ms (1s429ms/step)   1m27s       133.6050   0.9793 (2131/2176)\n",
      "[============>...............................................] 18/82   25s683ms (1s136ms/step)   1m29s       133.5035   0.9787 (2255/2304)\n",
      "[============>...............................................] 19/82   27s237ms (1s553ms/step)   1m14s       133.5842   0.9782 (2379/2432)\n",
      "[=============>..............................................] 20/82   28s905ms (1s668ms/step)   1m37s       133.5522   0.9777 (2503/2560)\n",
      "[==============>.............................................] 21/82   30s246ms (1s340ms/step)   1m39s       133.5198   0.9781 (2629/2688)\n",
      "[===============>............................................] 22/82   31s534ms (1s288ms/step)   1m20s       133.5295   0.9773 (2752/2816)\n",
      "[===============>............................................] 23/82   33s519ms (1s984ms/step)   1m20s       133.6316   0.9769 (2876/2944)\n",
      "[================>...........................................] 24/82   35s45ms (1s525ms/step)    1m52s       133.6501   0.9772 (3002/3072)\n",
      "[=================>..........................................] 25/82   36s579ms (1s533ms/step)   1m27s       133.5840   0.9769 (3126/3200)\n",
      "[==================>.........................................] 26/82   37s850ms (1s271ms/step)   1m24s       133.5677   0.9766 (3250/3328)\n",
      "[==================>.........................................] 27/82   39s35ms (1s184ms/step)    1m9s        133.5692   0.9766 (3375/3456)\n",
      "[===================>........................................] 28/82   40s402ms (1s367ms/step)   1m4s        133.5523   0.9763 (3499/3584)\n",
      "[====================>.......................................] 29/82   41s944ms (1s541ms/step)   1m13s       133.5387   0.9760 (3623/3712)\n",
      "[====================>.......................................] 30/82   43s381ms (1s437ms/step)   1m19s       133.5261   0.9763 (3749/3840)\n",
      "[=====================>......................................] 31/82   44s505ms (1s123ms/step)   1m11s       133.4555   0.9768 (3876/3968)\n",
      "[======================>.....................................] 32/82   46s46ms (1s540ms/step)    58s284ms    133.4703   0.9768 (4001/4096)\n",
      "[=======================>....................................] 33/82   47s528ms (1s482ms/step)   1m15s       133.4608   0.9773 (4128/4224)\n",
      "[=======================>....................................] 34/82   48s650ms (1s121ms/step)   1m9s        133.4471   0.9763 (4249/4352)\n",
      "[========================>...................................] 35/82   49s688ms (1s37ms/step)    52s335ms    133.4479   0.9761 (4373/4480)\n",
      "[=========================>..................................] 36/82   51s228ms (1s539ms/step)   50s24ms     133.4589   0.9761 (4498/4608)\n",
      "[==========================>.................................] 37/82   52s803ms (1s575ms/step)   1m9s        133.4531   0.9764 (4624/4736)\n",
      "[==========================>.................................] 38/82   53s975ms (1s171ms/step)   1m7s        133.3887   0.9764 (4749/4864)\n",
      "[===========================>................................] 39/82   55s268ms (1s292ms/step)   50s909ms    133.4034   0.9764 (4874/4992)\n",
      "[============================>...............................] 40/82   56s438ms (1s169ms/step)   53s771ms    133.4153   0.9764 (4999/5120)\n",
      "[=============================>..............................] 41/82   57s753ms (1s315ms/step)   48s561ms    133.4485   0.9768 (5126/5248)\n",
      "[=============================>..............................] 42/82   58s925ms (1s171ms/step)   52s44ms     133.4701   0.9767 (5251/5376)\n",
      "[==============================>.............................] 43/82   1m543ms (1s617ms/step)    47s441ms    133.4886   0.9769 (5377/5504)\n",
      "[===============================>............................] 44/82   1m2s (1s826ms/step)       1m2s        133.5195   0.9767 (5501/5632)\n",
      "[===============================>............................] 45/82   1m4s (1s658ms/step)       1m6s        133.6076   0.9767 (5626/5760)\n",
      "[================================>...........................] 46/82   1m5s (1s866ms/step)       1m455ms     133.5807   0.9767 (5751/5888)\n",
      "[=================================>..........................] 47/82   1m7s (1s458ms/step)       1m3s        133.5599   0.9769 (5877/6016)\n",
      "[==================================>.........................] 48/82   1m8s (1s443ms/step)       49s524ms    133.5388   0.9767 (6001/6144)\n",
      "[==================================>.........................] 49/82   1m10s (1s464ms/step)      47s692ms    133.5080   0.9766 (6125/6272)\n",
      "[===================================>........................] 50/82   1m11s (1s438ms/step)      46s767ms    133.5247   0.9764 (6249/6400)\n",
      "[====================================>.......................] 51/82   1m13s (1s355ms/step)      44s326ms    133.5356   0.9761 (6372/6528)\n",
      "[=====================================>......................] 52/82   1m14s (1s23ms/step)       39s664ms    133.5217   0.9758 (6495/6656)\n",
      "[=====================================>......................] 53/82   1m15s (1s262ms/step)      30s368ms    133.5150   0.9760 (6621/6784)\n",
      "[======================================>.....................] 54/82   1m16s (1s226ms/step)      35s252ms    133.5281   0.9757 (6744/6912)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================>....................] 55/82   1m18s (1s640ms/step)      34s238ms    133.5463   0.9756 (6868/7040)\n",
      "[=======================================>....................] 56/82   1m19s (1s600ms/step)      42s552ms    133.5288   0.9759 (6995/7168)\n",
      "[========================================>...................] 57/82   1m21s (1s383ms/step)      39s474ms    133.5412   0.9753 (7116/7296)\n",
      "[=========================================>..................] 58/82   1m22s (1s393ms/step)      33s223ms    133.5859   0.9749 (7238/7424)\n",
      "[==========================================>.................] 59/82   1m23s (1s306ms/step)      31s845ms    133.5751   0.9748 (7362/7552)\n",
      "[==========================================>.................] 60/82   1m24s (1s42ms/step)       28s161ms    133.5870   0.9750 (7488/7680)\n",
      "[===========================================>................] 61/82   1m26s (1s413ms/step)      22s665ms    133.6056   0.9748 (7611/7808)\n",
      "[============================================>...............] 62/82   1m27s (1s169ms/step)      27s777ms    133.6145   0.9751 (7738/7936)\n",
      "[=============================================>..............] 63/82   1m28s (1s233ms/step)      22s348ms    133.5766   0.9748 (7861/8064)\n",
      "[=============================================>..............] 64/82   1m30s (1s367ms/step)      22s447ms    133.5932   0.9749 (7986/8192)\n",
      "[==============================================>.............] 65/82   1m31s (1s445ms/step)      23s377ms    133.5698   0.9749 (8111/8320)\n",
      "[===============================================>............] 66/82   1m32s (1s404ms/step)      23s56ms     133.5652   0.9746 (8233/8448)\n",
      "[================================================>...........] 67/82   1m34s (1s232ms/step)      20s806ms    133.6177   0.9747 (8359/8576)\n",
      "[================================================>...........] 68/82   1m35s (1s185ms/step)      17s192ms    133.6408   0.9747 (8484/8704)\n",
      "[=================================================>..........] 69/82   1m37s (1s658ms/step)      16s30ms     133.6542   0.9746 (8608/8832)\n",
      "[==================================================>.........] 70/82   1m38s (1s427ms/step)      19s625ms    133.6560   0.9749 (8735/8960)\n",
      "[==================================================>.........] 71/82   1m39s (1s268ms/step)      15s524ms    133.6706   0.9750 (8861/9088)\n",
      "[===================================================>........] 72/82   1m40s (1s210ms/step)      12s628ms    133.6848   0.9752 (8987/9216)\n",
      "[====================================================>.......] 73/82   1m42s (1s461ms/step)      11s122ms    133.6675   0.9752 (9112/9344)\n",
      "[=====================================================>......] 74/82   1m43s (1s379ms/step)      11s623ms    133.6462   0.9754 (9239/9472)\n",
      "[=====================================================>......] 75/82   1m44s (1s170ms/step)      9s509ms     133.6682   0.9756 (9366/9600)\n",
      "[======================================================>.....] 76/82   1m45s (897ms/step)        6s861ms     133.6574   0.9757 (9492/9728)\n",
      "[=======================================================>....] 77/82   1m47s (1s274ms/step)      4s676ms     133.6570   0.9760 (9619/9856)\n",
      "[========================================================>...] 78/82   1m48s (1s531ms/step)      5s201ms     133.6529   0.9759 (9743/9984)\n",
      "[========================================================>...] 79/82   1m50s (1s599ms/step)      4s616ms     133.6745   0.9760 (9869/10112)\n",
      "[=========================================================>..] 80/82   1m51s (1s378ms/step)      3s155ms     133.6775   0.9760 (9994/10240)\n",
      "[==========================================================>.] 81/82   1m53s (1s994ms/step)      1s439ms     133.6955   0.9762 (10121/10368)\n",
      "[============================================================] 82/82   1m54s (452ms/step)        0ms         133.6890   0.9762 (10153/10401)\n"
     ]
    }
   ],
   "source": [
    "pipeline.load('pipeline', save_dir+'20201006_151536 model for classification task/20201007_202224 model for classification task epoch_62 val.pt')\n",
    "pipeline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-07T13:26:43.979Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model successfully.\n",
      "Start testing, test on 10401 samples.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                       Time used                 ETA         Loss     Acc                 \n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[>...........................................................]  1/82   1s465ms (1s465ms/step)    1m58s       137.9897   0.9766 (125/128)\n",
      "[>...........................................................]  2/82   2s810ms (1s345ms/step)    1m56s       137.1127   0.9805 (251/256)\n",
      "[=>..........................................................]  3/82   4s272ms (1s462ms/step)    1m47s       137.5592   0.9740 (374/384)\n",
      "[=>..........................................................]  4/82   5s381ms (1s109ms/step)    1m51s       137.9579   0.9746 (499/512)\n",
      "[==>.........................................................]  5/82   6s441ms (1s60ms/step)     1m25s       138.1233   0.9781 (626/640)\n",
      "[===>........................................................]  6/82   7s629ms (1s187ms/step)    1m21s       137.8950   0.9766 (750/768)\n",
      "[====>.......................................................]  7/82   8s652ms (1s23ms/step)     1m27s       137.9108   0.9799 (878/896)\n",
      "[====>.......................................................]  8/82   10s52ms (1s399ms/step)    1m18s       137.7913   0.9814 (1005/1024)\n",
      "[=====>......................................................]  9/82   11s883ms (1s831ms/step)   1m45s       137.7195   0.9818 (1131/1152)\n",
      "[======>.....................................................] 10/82   13s193ms (1s310ms/step)   2m8s        137.7660   0.9820 (1257/1280)\n",
      "[=======>....................................................] 11/82   14s580ms (1s386ms/step)   1m33s       138.1208   0.9837 (1385/1408)\n",
      "[=======>....................................................] 12/82   15s906ms (1s326ms/step)   1m36s       138.1329   0.9831 (1510/1536)\n",
      "[========>...................................................] 13/82   17s772ms (1s866ms/step)   1m35s       138.1047   0.9820 (1634/1664)\n",
      "[=========>..................................................] 14/82   19s449ms (1s677ms/step)   2m5s        138.1681   0.9827 (1761/1792)\n",
      "[=========>..................................................] 15/82   21s147ms (1s697ms/step)   1m52s       138.0677   0.9823 (1886/1920)\n",
      "[==========>.................................................] 16/82   22s415ms (1s267ms/step)   1m49s       138.0396   0.9814 (2010/2048)\n",
      "[===========>................................................] 17/82   23s935ms (1s520ms/step)   1m24s       138.0336   0.9807 (2134/2176)\n",
      "[============>...............................................] 18/82   25s138ms (1s202ms/step)   1m35s       137.9318   0.9800 (2258/2304)\n",
      "[============>...............................................] 19/82   26s869ms (1s730ms/step)   1m19s       138.0231   0.9794 (2382/2432)\n",
      "[=============>..............................................] 20/82   28s577ms (1s708ms/step)   1m47s       137.9839   0.9789 (2506/2560)\n",
      "[==============>.............................................] 21/82   29s954ms (1s377ms/step)   1m42s       137.9494   0.9792 (2632/2688)\n",
      "[===============>............................................] 22/82   31s263ms (1s308ms/step)   1m22s       137.9669   0.9783 (2755/2816)\n",
      "[===============>............................................] 23/82   33s321ms (2s58ms/step)    1m21s       138.0789   0.9779 (2879/2944)\n",
      "[================>...........................................] 24/82   34s809ms (1s488ms/step)   1m56s       138.0924   0.9782 (3005/3072)\n",
      "[=================>..........................................] 25/82   36s270ms (1s460ms/step)   1m24s       138.0217   0.9775 (3128/3200)\n",
      "[==================>.........................................] 26/82   37s445ms (1s175ms/step)   1m20s       138.0028   0.9775 (3253/3328)\n",
      "[==================>.........................................] 27/82   38s576ms (1s130ms/step)   1m4s        138.0052   0.9780 (3380/3456)\n",
      "[===================>........................................] 28/82   39s919ms (1s342ms/step)   1m2s        137.9912   0.9777 (3504/3584)\n",
      "[====================>.......................................] 29/82   41s418ms (1s498ms/step)   1m11s       137.9736   0.9774 (3628/3712)\n",
      "[====================>.......................................] 30/82   42s804ms (1s386ms/step)   1m17s       137.9626   0.9776 (3754/3840)\n",
      "[=====================>......................................] 31/82   43s889ms (1s85ms/step)    1m9s        137.8965   0.9781 (3881/3968)\n",
      "[======================>.....................................] 32/82   45s396ms (1s506ms/step)   56s364ms    137.9124   0.9783 (4007/4096)\n",
      "[=======================>....................................] 33/82   46s823ms (1s427ms/step)   1m13s       137.9024   0.9785 (4133/4224)\n",
      "[=======================>....................................] 34/82   47s976ms (1s152ms/step)   1m7s        137.8892   0.9773 (4253/4352)\n",
      "[========================>...................................] 35/82   49s30ms (1s54ms/step)     53s723ms    137.8900   0.9768 (4376/4480)\n",
      "[=========================>..................................] 36/82   50s558ms (1s527ms/step)   50s671ms    137.9001   0.9768 (4501/4608)\n",
      "[==========================>.................................] 37/82   52s155ms (1s596ms/step)   1m9s        137.8966   0.9770 (4627/4736)\n",
      "[==========================>.................................] 38/82   53s331ms (1s175ms/step)   1m8s        137.8358   0.9772 (4753/4864)\n",
      "[===========================>................................] 39/82   54s599ms (1s268ms/step)   50s960ms    137.8580   0.9770 (4877/4992)\n",
      "[============================>...............................] 40/82   55s787ms (1s187ms/step)   52s942ms    137.8715   0.9770 (5002/5120)\n",
      "[=============================>..............................] 41/82   57s98ms (1s310ms/step)    49s203ms    137.9067   0.9775 (5130/5248)\n",
      "[=============================>..............................] 42/82   58s271ms (1s172ms/step)   51s869ms    137.9286   0.9777 (5256/5376)\n",
      "[==============================>.............................] 43/82   59s923ms (1s652ms/step)   47s612ms    137.9495   0.9780 (5383/5504)\n",
      "[===============================>............................] 44/82   1m1s (1s806ms/step)       1m3s        137.9771   0.9780 (5508/5632)\n",
      "[===============================>............................] 45/82   1m3s (1s623ms/step)       1m6s        138.0665   0.9778 (5632/5760)\n",
      "[================================>...........................] 46/82   1m5s (1s880ms/step)       59s378ms    138.0388   0.9779 (5758/5888)\n",
      "[=================================>..........................] 47/82   1m6s (1s457ms/step)       1m4s        138.0143   0.9781 (5884/6016)\n",
      "[==================================>.........................] 48/82   1m8s (1s430ms/step)       49s449ms    137.9904   0.9779 (6008/6144)\n",
      "[==================================>.........................] 49/82   1m9s (1s532ms/step)       47s534ms    137.9593   0.9777 (6132/6272)\n",
      "[===================================>........................] 50/82   1m11s (1s375ms/step)      48s548ms    137.9754   0.9777 (6257/6400)\n",
      "[====================================>.......................] 51/82   1m12s (1s383ms/step)      42s659ms    137.9869   0.9775 (6381/6528)\n",
      "[=====================================>......................] 52/82   1m13s (1s35ms/step)       40s454ms    137.9757   0.9775 (6506/6656)\n",
      "[=====================================>......................] 53/82   1m14s (1s237ms/step)      30s608ms    137.9697   0.9774 (6631/6784)\n",
      "[======================================>.....................] 54/82   1m15s (1s271ms/step)      34s750ms    137.9804   0.9771 (6754/6912)\n",
      "[=======================================>....................] 55/82   1m17s (1s654ms/step)      35s367ms    137.9984   0.9770 (6878/7040)\n",
      "[=======================================>....................] 56/82   1m19s (1s556ms/step)      42s764ms    137.9807   0.9771 (7004/7168)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================>...................] 57/82   1m20s (1s371ms/step)      38s457ms    137.9951   0.9766 (7125/7296)\n",
      "[=========================================>..................] 58/82   1m21s (1s403ms/step)      32s988ms    138.0383   0.9762 (7247/7424)\n",
      "[==========================================>.................] 59/82   1m23s (1s300ms/step)      32s38ms     138.0300   0.9760 (7371/7552)\n",
      "[==========================================>.................] 60/82   1m24s (1s37ms/step)       28s32ms     138.0442   0.9762 (7497/7680)\n",
      "[===========================================>................] 61/82   1m25s (1s435ms/step)      22s617ms    138.0665   0.9762 (7622/7808)\n",
      "[============================================>...............] 62/82   1m26s (1s161ms/step)      28s156ms    138.0760   0.9763 (7748/7936)\n",
      "[=============================================>..............] 63/82   1m28s (1s246ms/step)      22s237ms    138.0354   0.9763 (7873/8064)\n",
      "[=============================================>..............] 64/82   1m29s (1s376ms/step)      22s673ms    138.0526   0.9763 (7998/8192)\n",
      "[==============================================>.............] 65/82   1m30s (1s425ms/step)      23s480ms    138.0300   0.9763 (8123/8320)\n",
      "[===============================================>............] 66/82   1m32s (1s402ms/step)      22s766ms    138.0278   0.9761 (8246/8448)\n",
      "[================================================>...........] 67/82   1m33s (1s243ms/step)      20s795ms    138.0800   0.9762 (8372/8576)\n",
      "[================================================>...........] 68/82   1m34s (1s157ms/step)      17s291ms    138.1055   0.9760 (8495/8704)\n"
     ]
    }
   ],
   "source": [
    "pipeline.load('model', save_dir+'20201006_151536 model for classification task/20201007_013732 model for classification task epoch_35 val.pt')\n",
    "pipeline(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T15:43:35.743572Z",
     "start_time": "2020-09-28T15:43:35.739557Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#T = 5\n",
    "#pipeline.load('history', save_dir + type_str[T-1] + '/history 20200928_193815 java ccd model for T4 epoch_32.hist')\n",
    "#pipeline.report(drop_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T14:12:13.884996Z",
     "start_time": "2020-09-26T14:12:13.881006Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pipeline.setup(reporter=Reporter_c2(labels = [0, 1], \n",
    "                                    init_hist = pipeline.reporter.history,\n",
    "                                    need_confusion_matrix = True,\n",
    "                                    output_to_score_fun = None, \n",
    "                                    report_interval = 2,\n",
    "                                    show_train_report = True,\n",
    "                                    summary_fun = None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
