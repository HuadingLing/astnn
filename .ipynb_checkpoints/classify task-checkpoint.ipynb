{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:58:19.742973Z",
     "start_time": "2020-08-01T12:58:19.713052Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.W_l = nn.Linear(encode_dim, encode_dim)\n",
    "        self.W_r = nn.Linear(encode_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, node, batch_index):\n",
    "        size = len(node)\n",
    "        if not size:\n",
    "            return None\n",
    "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "        for i in range(size):\n",
    "            if node[i][0] != -1:\n",
    "                index.append(i)\n",
    "                current_node.append(node[i][0])\n",
    "                temp = node[i][1:]\n",
    "                c_num = len(temp)\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] != -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            else:\n",
    "                batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
    "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_current = F.tanh(batch_current)\n",
    "        batch_index = [i for i in batch_index if i != -1]\n",
    "        b_in = Variable(self.th.LongTensor(batch_index))\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, x, bs):\n",
    "        self.batch_size = bs\n",
    "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(x, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]\n",
    "\n",
    "\n",
    "class BatchProgramClassifier(nn.Module):\n",
    "    # def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size, use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramClassifier, self).__init__()\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        #class \"BatchTreeEncoder\"\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def forward(self, x):\n",
    "        lens = [len(item) for item in x]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        encodes = []\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(lens[i]):\n",
    "                encodes.append(x[i][j])\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "            seq.append(encodes[start:end])\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "\n",
    "        # gru\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        # linear\n",
    "        y = self.hidden2label(gru_out)\n",
    "        return y\n",
    "    \n",
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    data, labels = [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        data.append(item[1])\n",
    "        labels.append(item[2]-1)  # from [1, 104] to [0, 103]\n",
    "    return data, torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:13:58.285886Z",
     "start_time": "2020-08-01T12:13:58.282893Z"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:47:18.186038Z",
     "start_time": "2020-08-01T12:47:17.871907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:102: UserWarning: \n",
      "    Found GPU0 GeForce GT 730 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n",
      "C:\\Users\\A1\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:125: UserWarning: \n",
      "GeForce GT 730 with CUDA capability sm_30 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the GeForce GT 730 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.get_device_name(0) == 'GeForce GT 730':\n",
    "            device = 'cpu'\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return torch.device(device)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:42:49.532182Z",
     "start_time": "2020-08-01T12:42:48.399120Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "root = 'data/'\n",
    "train_data = pd.read_pickle(root+'train/blocks.pkl')\n",
    "val_data = pd.read_pickle(root + 'dev/blocks.pkl')\n",
    "test_data = pd.read_pickle(root+'test/blocks.pkl')\n",
    "\n",
    "word2vec = Word2Vec.load(root+\"train/embedding/node_w2v_128\").wv\n",
    "\n",
    "embeddings = np.zeros((word2vec.vectors.shape[0] + 1, word2vec.vectors.shape[1]), dtype=\"float32\")\n",
    "embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T13:36:57.842033Z",
     "start_time": "2020-08-01T13:36:57.823084Z"
    }
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 104\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "\n",
    "model = BatchProgramClassifier(EMBEDDING_DIM,HIDDEN_DIM,MAX_TOKENS+1,ENCODE_DIM,LABELS,BATCH_SIZE,\n",
    "                               USE_GPU, embeddings)\n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "    \n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adamax(parameters)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-01T13:37:00.860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[Epoch:   1/ 15] [data: 128/31200] Training Loss: 4.6677\n",
      "[Epoch:   1/ 15] [data: 256/31200] Training Loss: 4.6522\n",
      "[Epoch:   1/ 15] [data: 384/31200] Training Loss: 4.6492\n",
      "[Epoch:   1/ 15] [data: 512/31200] Training Loss: 4.6613\n",
      "[Epoch:   1/ 15] [data: 640/31200] Training Loss: 4.6094\n",
      "[Epoch:   1/ 15] [data: 768/31200] Training Loss: 4.6266\n",
      "[Epoch:   1/ 15] [data: 896/31200] Training Loss: 4.6145\n",
      "[Epoch:   1/ 15] [data: 1024/31200] Training Loss: 4.5881\n",
      "[Epoch:   1/ 15] [data: 1152/31200] Training Loss: 4.6344\n",
      "[Epoch:   1/ 15] [data: 1280/31200] Training Loss: 4.6222\n",
      "[Epoch:   1/ 15] [data: 1408/31200] Training Loss: 4.5870\n",
      "[Epoch:   1/ 15] [data: 1536/31200] Training Loss: 4.5669\n",
      "[Epoch:   1/ 15] [data: 1664/31200] Training Loss: 4.5778\n",
      "[Epoch:   1/ 15] [data: 1792/31200] Training Loss: 4.5783\n",
      "[Epoch:   1/ 15] [data: 1920/31200] Training Loss: 4.5096\n",
      "[Epoch:   1/ 15] [data: 2048/31200] Training Loss: 4.5499\n",
      "[Epoch:   1/ 15] [data: 2176/31200] Training Loss: 4.5186\n",
      "[Epoch:   1/ 15] [data: 2304/31200] Training Loss: 4.5144\n",
      "[Epoch:   1/ 15] [data: 2432/31200] Training Loss: 4.5444\n",
      "[Epoch:   1/ 15] [data: 2560/31200] Training Loss: 4.5093\n",
      "[Epoch:   1/ 15] [data: 2688/31200] Training Loss: 4.4973\n",
      "[Epoch:   1/ 15] [data: 2816/31200] Training Loss: 4.4937\n",
      "[Epoch:   1/ 15] [data: 2944/31200] Training Loss: 4.4802\n",
      "[Epoch:   1/ 15] [data: 3072/31200] Training Loss: 4.4568\n",
      "[Epoch:   1/ 15] [data: 3200/31200] Training Loss: 4.3995\n",
      "[Epoch:   1/ 15] [data: 3328/31200] Training Loss: 4.4227\n",
      "[Epoch:   1/ 15] [data: 3456/31200] Training Loss: 4.4514\n",
      "[Epoch:   1/ 15] [data: 3584/31200] Training Loss: 4.4170\n",
      "[Epoch:   1/ 15] [data: 3712/31200] Training Loss: 4.4186\n",
      "[Epoch:   1/ 15] [data: 3840/31200] Training Loss: 4.4175\n",
      "[Epoch:   1/ 15] [data: 3968/31200] Training Loss: 4.4002\n",
      "[Epoch:   1/ 15] [data: 4096/31200] Training Loss: 4.2930\n",
      "[Epoch:   1/ 15] [data: 4224/31200] Training Loss: 4.3665\n",
      "[Epoch:   1/ 15] [data: 4352/31200] Training Loss: 4.3658\n",
      "[Epoch:   1/ 15] [data: 4480/31200] Training Loss: 4.2567\n",
      "[Epoch:   1/ 15] [data: 4608/31200] Training Loss: 4.3133\n",
      "[Epoch:   1/ 15] [data: 4736/31200] Training Loss: 4.3089\n",
      "[Epoch:   1/ 15] [data: 4864/31200] Training Loss: 4.3128\n",
      "[Epoch:   1/ 15] [data: 4992/31200] Training Loss: 4.2596\n",
      "[Epoch:   1/ 15] [data: 5120/31200] Training Loss: 4.3085\n",
      "[Epoch:   1/ 15] [data: 5248/31200] Training Loss: 4.2711\n",
      "[Epoch:   1/ 15] [data: 5376/31200] Training Loss: 4.2851\n",
      "[Epoch:   1/ 15] [data: 5504/31200] Training Loss: 4.2607\n",
      "[Epoch:   1/ 15] [data: 5632/31200] Training Loss: 4.2008\n",
      "[Epoch:   1/ 15] [data: 5760/31200] Training Loss: 4.2821\n",
      "[Epoch:   1/ 15] [data: 5888/31200] Training Loss: 4.2140\n",
      "[Epoch:   1/ 15] [data: 6016/31200] Training Loss: 4.1480\n",
      "[Epoch:   1/ 15] [data: 6144/31200] Training Loss: 4.1582\n",
      "[Epoch:   1/ 15] [data: 6272/31200] Training Loss: 4.1378\n",
      "[Epoch:   1/ 15] [data: 6400/31200] Training Loss: 4.0914\n",
      "[Epoch:   1/ 15] [data: 6528/31200] Training Loss: 4.1276\n",
      "[Epoch:   1/ 15] [data: 6656/31200] Training Loss: 4.0472\n",
      "[Epoch:   1/ 15] [data: 6784/31200] Training Loss: 4.0706\n",
      "[Epoch:   1/ 15] [data: 6912/31200] Training Loss: 4.0711\n",
      "[Epoch:   1/ 15] [data: 7040/31200] Training Loss: 3.9386\n",
      "[Epoch:   1/ 15] [data: 7168/31200] Training Loss: 4.1218\n",
      "[Epoch:   1/ 15] [data: 7296/31200] Training Loss: 3.9125\n",
      "[Epoch:   1/ 15] [data: 7424/31200] Training Loss: 3.9876\n",
      "[Epoch:   1/ 15] [data: 7552/31200] Training Loss: 3.8478\n",
      "[Epoch:   1/ 15] [data: 7680/31200] Training Loss: 3.9299\n",
      "[Epoch:   1/ 15] [data: 7808/31200] Training Loss: 3.9231\n",
      "[Epoch:   1/ 15] [data: 7936/31200] Training Loss: 3.9014\n",
      "[Epoch:   1/ 15] [data: 8064/31200] Training Loss: 3.8522\n",
      "[Epoch:   1/ 15] [data: 8192/31200] Training Loss: 3.8581\n",
      "[Epoch:   1/ 15] [data: 8320/31200] Training Loss: 3.8513\n",
      "[Epoch:   1/ 15] [data: 8448/31200] Training Loss: 3.8581\n",
      "[Epoch:   1/ 15] [data: 8576/31200] Training Loss: 3.7838\n",
      "[Epoch:   1/ 15] [data: 8704/31200] Training Loss: 3.8390\n",
      "[Epoch:   1/ 15] [data: 8832/31200] Training Loss: 3.7454\n",
      "[Epoch:   1/ 15] [data: 8960/31200] Training Loss: 3.7985\n",
      "[Epoch:   1/ 15] [data: 9088/31200] Training Loss: 3.7980\n",
      "[Epoch:   1/ 15] [data: 9216/31200] Training Loss: 3.7802\n",
      "[Epoch:   1/ 15] [data: 9344/31200] Training Loss: 3.6777\n",
      "[Epoch:   1/ 15] [data: 9472/31200] Training Loss: 3.7473\n",
      "[Epoch:   1/ 15] [data: 9600/31200] Training Loss: 3.6085\n",
      "[Epoch:   1/ 15] [data: 9728/31200] Training Loss: 3.6181\n",
      "[Epoch:   1/ 15] [data: 9856/31200] Training Loss: 3.5850\n",
      "[Epoch:   1/ 15] [data: 9984/31200] Training Loss: 3.5754\n",
      "[Epoch:   1/ 15] [data: 10112/31200] Training Loss: 3.5593\n",
      "[Epoch:   1/ 15] [data: 10240/31200] Training Loss: 3.5509\n",
      "[Epoch:   1/ 15] [data: 10368/31200] Training Loss: 3.4096\n",
      "[Epoch:   1/ 15] [data: 10496/31200] Training Loss: 3.4645\n",
      "[Epoch:   1/ 15] [data: 10624/31200] Training Loss: 3.4801\n",
      "[Epoch:   1/ 15] [data: 10752/31200] Training Loss: 3.4164\n",
      "[Epoch:   1/ 15] [data: 10880/31200] Training Loss: 3.4692\n",
      "[Epoch:   1/ 15] [data: 11008/31200] Training Loss: 3.4229\n",
      "[Epoch:   1/ 15] [data: 11136/31200] Training Loss: 3.5140\n",
      "[Epoch:   1/ 15] [data: 11264/31200] Training Loss: 3.4373\n",
      "[Epoch:   1/ 15] [data: 11392/31200] Training Loss: 3.3362\n",
      "[Epoch:   1/ 15] [data: 11520/31200] Training Loss: 3.3831\n",
      "[Epoch:   1/ 15] [data: 11648/31200] Training Loss: 3.3723\n",
      "[Epoch:   1/ 15] [data: 11776/31200] Training Loss: 3.4118\n",
      "[Epoch:   1/ 15] [data: 11904/31200] Training Loss: 3.3984\n",
      "[Epoch:   1/ 15] [data: 12032/31200] Training Loss: 3.2176\n",
      "[Epoch:   1/ 15] [data: 12160/31200] Training Loss: 3.2116\n",
      "[Epoch:   1/ 15] [data: 12288/31200] Training Loss: 3.2060\n",
      "[Epoch:   1/ 15] [data: 12416/31200] Training Loss: 3.2065\n",
      "[Epoch:   1/ 15] [data: 12544/31200] Training Loss: 3.2854\n",
      "[Epoch:   1/ 15] [data: 12672/31200] Training Loss: 3.2682\n",
      "[Epoch:   1/ 15] [data: 12800/31200] Training Loss: 3.2288\n",
      "[Epoch:   1/ 15] [data: 12928/31200] Training Loss: 3.1947\n",
      "[Epoch:   1/ 15] [data: 13056/31200] Training Loss: 3.0794\n",
      "[Epoch:   1/ 15] [data: 13184/31200] Training Loss: 3.1490\n",
      "[Epoch:   1/ 15] [data: 13312/31200] Training Loss: 3.1648\n",
      "[Epoch:   1/ 15] [data: 13440/31200] Training Loss: 3.0882\n",
      "[Epoch:   1/ 15] [data: 13568/31200] Training Loss: 3.1757\n",
      "[Epoch:   1/ 15] [data: 13696/31200] Training Loss: 3.0544\n",
      "[Epoch:   1/ 15] [data: 13824/31200] Training Loss: 3.0694\n",
      "[Epoch:   1/ 15] [data: 13952/31200] Training Loss: 3.0831\n",
      "[Epoch:   1/ 15] [data: 14080/31200] Training Loss: 2.9115\n",
      "[Epoch:   1/ 15] [data: 14208/31200] Training Loss: 2.9204\n",
      "[Epoch:   1/ 15] [data: 14336/31200] Training Loss: 2.8974\n",
      "[Epoch:   1/ 15] [data: 14464/31200] Training Loss: 2.9426\n",
      "[Epoch:   1/ 15] [data: 14592/31200] Training Loss: 2.9190\n",
      "[Epoch:   1/ 15] [data: 14720/31200] Training Loss: 2.9221\n",
      "[Epoch:   1/ 15] [data: 14848/31200] Training Loss: 2.8651\n",
      "[Epoch:   1/ 15] [data: 14976/31200] Training Loss: 2.7505\n",
      "[Epoch:   1/ 15] [data: 15104/31200] Training Loss: 2.8380\n",
      "[Epoch:   1/ 15] [data: 15232/31200] Training Loss: 2.8287\n",
      "[Epoch:   1/ 15] [data: 15360/31200] Training Loss: 2.7119\n",
      "[Epoch:   1/ 15] [data: 15488/31200] Training Loss: 2.6833\n",
      "[Epoch:   1/ 15] [data: 15616/31200] Training Loss: 2.8007\n",
      "[Epoch:   1/ 15] [data: 15744/31200] Training Loss: 2.7015\n",
      "[Epoch:   1/ 15] [data: 15872/31200] Training Loss: 2.6431\n",
      "[Epoch:   1/ 15] [data: 16000/31200] Training Loss: 2.7431\n",
      "[Epoch:   1/ 15] [data: 16128/31200] Training Loss: 2.8726\n",
      "[Epoch:   1/ 15] [data: 16256/31200] Training Loss: 2.6648\n",
      "[Epoch:   1/ 15] [data: 16384/31200] Training Loss: 2.5712\n",
      "[Epoch:   1/ 15] [data: 16512/31200] Training Loss: 2.7105\n",
      "[Epoch:   1/ 15] [data: 16640/31200] Training Loss: 2.5802\n",
      "[Epoch:   1/ 15] [data: 16768/31200] Training Loss: 2.6831\n",
      "[Epoch:   1/ 15] [data: 16896/31200] Training Loss: 2.5417\n",
      "[Epoch:   1/ 15] [data: 17024/31200] Training Loss: 2.5869\n",
      "[Epoch:   1/ 15] [data: 17152/31200] Training Loss: 2.4361\n",
      "[Epoch:   1/ 15] [data: 17280/31200] Training Loss: 2.4968\n",
      "[Epoch:   1/ 15] [data: 17408/31200] Training Loss: 2.4654\n",
      "[Epoch:   1/ 15] [data: 17536/31200] Training Loss: 2.3930\n",
      "[Epoch:   1/ 15] [data: 17664/31200] Training Loss: 2.4858\n",
      "[Epoch:   1/ 15] [data: 17792/31200] Training Loss: 2.5431\n",
      "[Epoch:   1/ 15] [data: 17920/31200] Training Loss: 2.4247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:   1/ 15] [data: 18048/31200] Training Loss: 2.4059\n",
      "[Epoch:   1/ 15] [data: 18176/31200] Training Loss: 2.2714\n",
      "[Epoch:   1/ 15] [data: 18304/31200] Training Loss: 2.2058\n",
      "[Epoch:   1/ 15] [data: 18432/31200] Training Loss: 2.3676\n",
      "[Epoch:   1/ 15] [data: 18560/31200] Training Loss: 2.2991\n",
      "[Epoch:   1/ 15] [data: 18688/31200] Training Loss: 2.4477\n",
      "[Epoch:   1/ 15] [data: 18816/31200] Training Loss: 2.2919\n",
      "[Epoch:   1/ 15] [data: 18944/31200] Training Loss: 2.3330\n",
      "[Epoch:   1/ 15] [data: 19072/31200] Training Loss: 2.3934\n",
      "[Epoch:   1/ 15] [data: 19200/31200] Training Loss: 2.3940\n",
      "[Epoch:   1/ 15] [data: 19328/31200] Training Loss: 2.2056\n",
      "[Epoch:   1/ 15] [data: 19456/31200] Training Loss: 2.2569\n",
      "[Epoch:   1/ 15] [data: 19584/31200] Training Loss: 2.2208\n",
      "[Epoch:   1/ 15] [data: 19712/31200] Training Loss: 2.1420\n",
      "[Epoch:   1/ 15] [data: 19840/31200] Training Loss: 2.2765\n",
      "[Epoch:   1/ 15] [data: 19968/31200] Training Loss: 2.2346\n",
      "[Epoch:   1/ 15] [data: 20096/31200] Training Loss: 2.1607\n",
      "[Epoch:   1/ 15] [data: 20224/31200] Training Loss: 2.1510\n",
      "[Epoch:   1/ 15] [data: 20352/31200] Training Loss: 2.2138\n",
      "[Epoch:   1/ 15] [data: 20480/31200] Training Loss: 2.1518\n",
      "[Epoch:   1/ 15] [data: 20608/31200] Training Loss: 2.1839\n",
      "[Epoch:   1/ 15] [data: 20736/31200] Training Loss: 2.0625\n",
      "[Epoch:   1/ 15] [data: 20864/31200] Training Loss: 2.3396\n",
      "[Epoch:   1/ 15] [data: 20992/31200] Training Loss: 2.1063\n",
      "[Epoch:   1/ 15] [data: 21120/31200] Training Loss: 2.0815\n",
      "[Epoch:   1/ 15] [data: 21248/31200] Training Loss: 2.0407\n",
      "[Epoch:   1/ 15] [data: 21376/31200] Training Loss: 2.1015\n",
      "[Epoch:   1/ 15] [data: 21504/31200] Training Loss: 1.9503\n",
      "[Epoch:   1/ 15] [data: 21632/31200] Training Loss: 1.9216\n",
      "[Epoch:   1/ 15] [data: 21760/31200] Training Loss: 2.0676\n",
      "[Epoch:   1/ 15] [data: 21888/31200] Training Loss: 1.9780\n",
      "[Epoch:   1/ 15] [data: 22016/31200] Training Loss: 1.9562\n",
      "[Epoch:   1/ 15] [data: 22144/31200] Training Loss: 1.9698\n",
      "[Epoch:   1/ 15] [data: 22272/31200] Training Loss: 1.8764\n",
      "[Epoch:   1/ 15] [data: 22400/31200] Training Loss: 1.9237\n",
      "[Epoch:   1/ 15] [data: 22528/31200] Training Loss: 2.1468\n",
      "[Epoch:   1/ 15] [data: 22656/31200] Training Loss: 1.8540\n",
      "[Epoch:   1/ 15] [data: 22784/31200] Training Loss: 1.9971\n",
      "[Epoch:   1/ 15] [data: 22912/31200] Training Loss: 1.9412\n",
      "[Epoch:   1/ 15] [data: 23040/31200] Training Loss: 1.8417\n",
      "[Epoch:   1/ 15] [data: 23168/31200] Training Loss: 1.8322\n",
      "[Epoch:   1/ 15] [data: 23296/31200] Training Loss: 1.8879\n",
      "[Epoch:   1/ 15] [data: 23424/31200] Training Loss: 1.8535\n",
      "[Epoch:   1/ 15] [data: 23552/31200] Training Loss: 1.9133\n",
      "[Epoch:   1/ 15] [data: 23680/31200] Training Loss: 1.7458\n",
      "[Epoch:   1/ 15] [data: 23808/31200] Training Loss: 1.8556\n",
      "[Epoch:   1/ 15] [data: 23936/31200] Training Loss: 1.8000\n",
      "[Epoch:   1/ 15] [data: 24064/31200] Training Loss: 1.8442\n",
      "[Epoch:   1/ 15] [data: 24192/31200] Training Loss: 1.7985\n",
      "[Epoch:   1/ 15] [data: 24320/31200] Training Loss: 1.7550\n",
      "[Epoch:   1/ 15] [data: 24448/31200] Training Loss: 1.7084\n",
      "[Epoch:   1/ 15] [data: 24576/31200] Training Loss: 1.7415\n",
      "[Epoch:   1/ 15] [data: 24704/31200] Training Loss: 1.7401\n",
      "[Epoch:   1/ 15] [data: 24832/31200] Training Loss: 1.6321\n",
      "[Epoch:   1/ 15] [data: 24960/31200] Training Loss: 1.8285\n",
      "[Epoch:   1/ 15] [data: 25088/31200] Training Loss: 1.6811\n",
      "[Epoch:   1/ 15] [data: 25216/31200] Training Loss: 1.8018\n",
      "[Epoch:   1/ 15] [data: 25344/31200] Training Loss: 1.6060\n",
      "[Epoch:   1/ 15] [data: 25472/31200] Training Loss: 1.7960\n",
      "[Epoch:   1/ 15] [data: 25600/31200] Training Loss: 1.6973\n",
      "[Epoch:   1/ 15] [data: 25728/31200] Training Loss: 1.5582\n",
      "[Epoch:   1/ 15] [data: 25856/31200] Training Loss: 1.7377\n",
      "[Epoch:   1/ 15] [data: 25984/31200] Training Loss: 1.7056\n",
      "[Epoch:   1/ 15] [data: 26112/31200] Training Loss: 1.6886\n",
      "[Epoch:   1/ 15] [data: 26240/31200] Training Loss: 1.7536\n",
      "[Epoch:   1/ 15] [data: 26368/31200] Training Loss: 1.6312\n",
      "[Epoch:   1/ 15] [data: 26496/31200] Training Loss: 1.5358\n",
      "[Epoch:   1/ 15] [data: 26624/31200] Training Loss: 1.6741\n",
      "[Epoch:   1/ 15] [data: 26752/31200] Training Loss: 1.5235\n",
      "[Epoch:   1/ 15] [data: 26880/31200] Training Loss: 1.4868\n",
      "[Epoch:   1/ 15] [data: 27008/31200] Training Loss: 1.5993\n",
      "[Epoch:   1/ 15] [data: 27136/31200] Training Loss: 1.4657\n",
      "[Epoch:   1/ 15] [data: 27264/31200] Training Loss: 1.5532\n",
      "[Epoch:   1/ 15] [data: 27392/31200] Training Loss: 1.5465\n",
      "[Epoch:   1/ 15] [data: 27520/31200] Training Loss: 1.4321\n",
      "[Epoch:   1/ 15] [data: 27648/31200] Training Loss: 1.4198\n",
      "[Epoch:   1/ 15] [data: 27776/31200] Training Loss: 1.5667\n",
      "[Epoch:   1/ 15] [data: 27904/31200] Training Loss: 1.3564\n",
      "[Epoch:   1/ 15] [data: 28032/31200] Training Loss: 1.5797\n",
      "[Epoch:   1/ 15] [data: 28160/31200] Training Loss: 1.5490\n",
      "[Epoch:   1/ 15] [data: 28288/31200] Training Loss: 1.4141\n",
      "[Epoch:   1/ 15] [data: 28416/31200] Training Loss: 1.3664\n",
      "[Epoch:   1/ 15] [data: 28544/31200] Training Loss: 1.4490\n",
      "[Epoch:   1/ 15] [data: 28672/31200] Training Loss: 1.3416\n",
      "[Epoch:   1/ 15] [data: 28800/31200] Training Loss: 1.4081\n",
      "[Epoch:   1/ 15] [data: 28928/31200] Training Loss: 1.4325\n",
      "[Epoch:   1/ 15] [data: 29056/31200] Training Loss: 1.3518\n",
      "[Epoch:   1/ 15] [data: 29184/31200] Training Loss: 1.5151\n",
      "[Epoch:   1/ 15] [data: 29312/31200] Training Loss: 1.3409\n",
      "[Epoch:   1/ 15] [data: 29440/31200] Training Loss: 1.5613\n",
      "[Epoch:   1/ 15] [data: 29568/31200] Training Loss: 1.2879\n",
      "[Epoch:   1/ 15] [data: 29696/31200] Training Loss: 1.3437\n",
      "[Epoch:   1/ 15] [data: 29824/31200] Training Loss: 1.3862\n"
     ]
    }
   ],
   "source": [
    "train_loss_ = []\n",
    "val_loss_ = []\n",
    "train_acc_ = []\n",
    "val_acc_ = []\n",
    "best_acc = 0.0\n",
    "print('Start training...')\n",
    "# training procedure\n",
    "best_model = model\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    model.train()\n",
    "    while i < len(train_data):\n",
    "        train_inputs, train_labels = get_batch(train_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        if USE_GPU:\n",
    "            train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train_inputs)\n",
    "\n",
    "        loss = loss_function(output, Variable(train_labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('[Epoch: %3d/%3d] [data: %d/%d] Training Loss: %.4f'\n",
    "          % (epoch + 1, EPOCHS, i, len(train_data), loss))\n",
    "\n",
    "        # calc training acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == train_labels).sum()\n",
    "        total += len(train_labels)\n",
    "        total_loss += loss.item()*len(train_inputs)\n",
    "\n",
    "    train_loss_.append(total_loss / total)\n",
    "    train_acc_.append(total_acc.item() / total)\n",
    "    \n",
    "    # validation epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while i < len(val_data):\n",
    "            val_inputs, val_labels = get_batch(val_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            if USE_GPU:\n",
    "                val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(val_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(val_inputs)\n",
    "\n",
    "            loss = loss_function(output, Variable(val_labels))\n",
    "\n",
    "            # calc valing acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == val_labels).sum()\n",
    "            total += len(val_labels)\n",
    "            total_loss += loss.item()*len(val_inputs)\n",
    "    val_loss_.append(total_loss / total)\n",
    "    val_acc_.append(total_acc.item() / total)\n",
    "    end_time = time.time()\n",
    "    if total_acc/total > best_acc:\n",
    "        best_model = model\n",
    "    print('[Epoch: %3d/%3d] Training Loss: %.4f, Validation Loss: %.4f,'\n",
    "          ' Training Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "          % (epoch + 1, EPOCHS, train_loss_[epoch], val_loss_[epoch],\n",
    "             train_acc_[epoch], val_acc_[epoch], end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T13:33:33.665656Z",
     "start_time": "2020-08-01T13:28:46.539Z"
    }
   },
   "outputs": [],
   "source": [
    "total_acc = 0.0\n",
    "total_loss = 0.0\n",
    "total = 0.0\n",
    "i = 0\n",
    "model = best_model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while i < len(test_data):\n",
    "        test_inputs, test_labels = get_batch(test_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        if USE_GPU:\n",
    "            test_inputs, test_labels = test_inputs.cuda(), test_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(test_inputs)\n",
    "\n",
    "        loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == test_labels).sum()\n",
    "        total += len(test_labels)\n",
    "        total_loss += loss.item() * len(test_inputs)\n",
    "    print(\"Testing results(Acc):\", total_acc.item() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramClassifier\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    data, labels = [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        data.append(item[1])\n",
    "        labels.append(item[2]-1)\n",
    "    return data, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "root = 'data/'\n",
    "train_data = pd.read_pickle(root+'train/blocks.pkl')\n",
    "val_data = pd.read_pickle(root + 'dev/blocks.pkl')\n",
    "test_data = pd.read_pickle(root+'test/blocks.pkl')\n",
    "\n",
    "word2vec = Word2Vec.load(root+\"train/embedding/node_w2v_128\").wv\n",
    "embeddings = np.zeros((word2vec.syn0.shape[0] + 1, word2vec.syn0.shape[1]), dtype=\"float32\")\n",
    "embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0\n",
    "\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 104\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "USE_GPU = True\n",
    "MAX_TOKENS = word2vec.syn0.shape[0]\n",
    "EMBEDDING_DIM = word2vec.syn0.shape[1]\n",
    "\n",
    "model = BatchProgramClassifier(EMBEDDING_DIM,HIDDEN_DIM,MAX_TOKENS+1,ENCODE_DIM,LABELS,BATCH_SIZE,\n",
    "                               USE_GPU, embeddings)\n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adamax(parameters)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_ = []\n",
    "val_loss_ = []\n",
    "train_acc_ = []\n",
    "val_acc_ = []\n",
    "best_acc = 0.0\n",
    "print('Start training...')\n",
    "# training procedure\n",
    "best_model = model\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(train_data):\n",
    "        batch = get_batch(train_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        train_inputs, train_labels = batch\n",
    "        if USE_GPU:\n",
    "            train_inputs, train_labels = train_inputs, train_labels.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.batch_size = len(train_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(train_inputs)\n",
    "\n",
    "        loss = loss_function(output, Variable(train_labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc training acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == train_labels).sum()\n",
    "        total += len(train_labels)\n",
    "        total_loss += loss.item()*len(train_inputs)\n",
    "\n",
    "    train_loss_.append(total_loss / total)\n",
    "    train_acc_.append(total_acc.item() / total)\n",
    "    # validation epoch\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(val_data):\n",
    "        batch = get_batch(val_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        val_inputs, val_labels = batch\n",
    "        if USE_GPU:\n",
    "            val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(val_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(val_inputs)\n",
    "\n",
    "        loss = loss_function(output, Variable(val_labels))\n",
    "\n",
    "        # calc valing acc\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_acc += (predicted == val_labels).sum()\n",
    "        total += len(val_labels)\n",
    "        total_loss += loss.item()*len(val_inputs)\n",
    "    val_loss_.append(total_loss / total)\n",
    "    val_acc_.append(total_acc.item() / total)\n",
    "    end_time = time.time()\n",
    "    if total_acc/total > best_acc:\n",
    "        best_model = model\n",
    "    print('[Epoch: %3d/%3d] Training Loss: %.4f, Validation Loss: %.4f,'\n",
    "          ' Training Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "          % (epoch + 1, EPOCHS, train_loss_[epoch], val_loss_[epoch],\n",
    "             train_acc_[epoch], val_acc_[epoch], end_time - start_time))\n",
    "\n",
    "total_acc = 0.0\n",
    "total_loss = 0.0\n",
    "total = 0.0\n",
    "i = 0\n",
    "model = best_model\n",
    "while i < len(test_data):\n",
    "    batch = get_batch(test_data, i, BATCH_SIZE)\n",
    "    i += BATCH_SIZE\n",
    "    test_inputs, test_labels = batch\n",
    "    if USE_GPU:\n",
    "        test_inputs, test_labels = test_inputs, test_labels.cuda()\n",
    "\n",
    "    model.batch_size = len(test_labels)\n",
    "    model.hidden = model.init_hidden()\n",
    "    output = model(test_inputs)\n",
    "\n",
    "    loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    total_acc += (predicted == test_labels).sum()\n",
    "    total += len(test_labels)\n",
    "    total_loss += loss.item() * len(test_inputs)\n",
    "print(\"Testing results(Acc):\", total_acc.item() / total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T12:04:43.965382Z",
     "start_time": "2020-07-27T11:56:14.576486Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def parse_source(root, output_file, option):\n",
    "    path = root+output_file\n",
    "    if os.path.exists(path) and option is 'existing':\n",
    "        source = pd.read_pickle(path)\n",
    "    else:\n",
    "        from pycparser import c_parser\n",
    "        parser = c_parser.CParser()\n",
    "        source = pd.read_pickle(root+'programs.pkl')\n",
    "\n",
    "        source.columns = ['id', 'code', 'label']\n",
    "        source['code'] = source['code'].apply(parser.parse)\n",
    "\n",
    "        source.to_pickle(path)\n",
    "    return source\n",
    "\n",
    "#s = parse_source(root='data/', output_file='ast.pkl',option='existing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T14:04:36.135512Z",
     "start_time": "2020-07-27T14:03:01.629158Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train/dev_.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-aef5fca83b82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mgenerate_block_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/train/train_.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mgenerate_block_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/train/dev_.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dev'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[0mgenerate_block_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/train/test_.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-aef5fca83b82>\u001b[0m in \u001b[0;36mgenerate_block_seqs\u001b[1;34m(root, data_path, part, size)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mtrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans2seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/blocks.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/dev_.pkl'"
     ]
    }
   ],
   "source": [
    "def split_data(root,ratio):\n",
    "    data = pd.read_pickle(root+'ast.pkl')\n",
    "    data_num = len(data)\n",
    "    ratios = [int(r) for r in ratio.split(':')]\n",
    "    train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "    val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "    data = data.sample(frac=1, random_state=666)\n",
    "    train = data.iloc[:train_split] \n",
    "    dev = data.iloc[train_split:val_split] \n",
    "    test = data.iloc[val_split:] \n",
    "\n",
    "    def check_or_create(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    train_path = root+'train/'\n",
    "    check_or_create(train_path)\n",
    "    train_file_path = train_path+'train_.pkl'\n",
    "    train.to_pickle(train_file_path)\n",
    "\n",
    "    dev_path = root+'dev/'\n",
    "    check_or_create(dev_path)\n",
    "    dev_file_path = dev_path+'dev_.pkl'\n",
    "    dev.to_pickle(dev_file_path)\n",
    "\n",
    "    test_path = root+'test/'\n",
    "    check_or_create(test_path)\n",
    "    test_file_path = test_path+'test_.pkl'\n",
    "    test.to_pickle(test_file_path)\n",
    "\n",
    "def dictionary_and_embedding(root, input_file, size):\n",
    "    if not input_file:\n",
    "        input_file = 'data/train/train_.pkl'\n",
    "    trees = pd.read_pickle(input_file)\n",
    "    if not os.path.exists(root+'train/embedding'):\n",
    "        os.mkdir(root+'train/embedding')\n",
    "    from prepare_data import get_sequences\n",
    "\n",
    "    def trans_to_sequences(ast):\n",
    "        sequence = []\n",
    "        get_sequences(ast, sequence)\n",
    "        return sequence\n",
    "    corpus = trees['code'].apply(trans_to_sequences)  # every row of corpus is list of str\n",
    "    str_corpus = [' '.join(c) for c in corpus]  # str_corpus is list of str\n",
    "    trees['code'] = pd.Series(str_corpus)  \n",
    "    # the first saveral ones are alway the same in every row, can we delete them ?\n",
    "    trees.to_csv(root+'train/programs_ns.tsv')  # tsv?\n",
    "\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    w2v = Word2Vec(corpus, size=size, workers=16, sg=1, min_count=3)\n",
    "    w2v.save(root+'train/embedding/node_w2v_' + str(size))    \n",
    "\n",
    "def generate_block_seqs(root,data_path,part,size=128):\n",
    "    from prepare_data import get_blocks as func\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "    word2vec = Word2Vec.load(root+'train/embedding/node_w2v_' + str(size)).wv\n",
    "    vocab = word2vec.vocab\n",
    "    max_token = word2vec.vectors.shape[0]\n",
    "    # Attribute `syn0` will be removed in 4.0.0, use self.vectors instead\n",
    "\n",
    "    def tree_to_index(node):\n",
    "        token = node.token\n",
    "        result = [vocab[token].index if token in vocab else max_token]\n",
    "        children = node.children\n",
    "        for child in children:\n",
    "            result.append(tree_to_index(child))\n",
    "        return result\n",
    "\n",
    "    def trans2seq(r):\n",
    "        blocks = []\n",
    "        func(r, blocks)\n",
    "        tree = []\n",
    "        for b in blocks:\n",
    "            btree = tree_to_index(b)\n",
    "            tree.append(btree)\n",
    "        return tree\n",
    "    trees = pd.read_pickle(data_path)\n",
    "    trees['code'] = trees['code'].apply(trans2seq)\n",
    "    trees.to_pickle(root+part+'/blocks.pkl')\n",
    "\n",
    "generate_block_seqs(root='data/', data_path='data/train/train_.pkl', part='train')\n",
    "generate_block_seqs(root='data/', data_path='data/dev/dev_.pkl', part='dev')\n",
    "generate_block_seqs(root='data/', data_path='data/test/test_.pkl', part='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self,  ratio, root):\n",
    "        self.ratio = ratio\n",
    "        self.root = root\n",
    "        self.sources = None\n",
    "        self.train_file_path = None\n",
    "        self.dev_file_path = None\n",
    "        self.test_file_path = None\n",
    "        self.size = None\n",
    "\n",
    "    # parse source code\n",
    "    def parse_source(self, output_file, option):\n",
    "        path = self.root+output_file\n",
    "        if os.path.exists(path) and option is 'existing':\n",
    "            source = pd.read_pickle(path)\n",
    "        else:\n",
    "            from pycparser import c_parser\n",
    "            parser = c_parser.CParser()\n",
    "            source = pd.read_pickle(self.root+'programs.pkl')\n",
    "\n",
    "            source.columns = ['id', 'code', 'label']\n",
    "            source['code'] = source['code'].apply(parser.parse)\n",
    "\n",
    "            source.to_pickle(path)\n",
    "        self.sources = source\n",
    "        return source\n",
    "\n",
    "    # split data for training, developing and testing\n",
    "    def split_data(self):\n",
    "        data = self.sources\n",
    "        data_num = len(data)\n",
    "        ratios = [int(r) for r in self.ratio.split(':')]\n",
    "        train_split = int(ratios[0]/sum(ratios)*data_num)\n",
    "        val_split = train_split + int(ratios[1]/sum(ratios)*data_num)\n",
    "        data = data.sample(frac=1, random_state=666)\n",
    "        train = data.iloc[:train_split] \n",
    "        dev = data.iloc[train_split:val_split] \n",
    "        test = data.iloc[val_split:] \n",
    "\n",
    "        def check_or_create(path):\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "        train_path = self.root+'train/'\n",
    "        check_or_create(train_path)\n",
    "        self.train_file_path = train_path+'train_.pkl'\n",
    "        train.to_pickle(self.train_file_path)\n",
    "\n",
    "        dev_path = self.root+'dev/'\n",
    "        check_or_create(dev_path)\n",
    "        self.dev_file_path = dev_path+'dev_.pkl'\n",
    "        dev.to_pickle(self.dev_file_path)\n",
    "\n",
    "        test_path = self.root+'test/'\n",
    "        check_or_create(test_path)\n",
    "        self.test_file_path = test_path+'test_.pkl'\n",
    "        test.to_pickle(self.test_file_path)\n",
    "\n",
    "    # construct dictionary and train word embedding\n",
    "    def dictionary_and_embedding(self, input_file, size):\n",
    "        self.size = size\n",
    "        if not input_file:\n",
    "            input_file = self.train_file_path\n",
    "        trees = pd.read_pickle(input_file)\n",
    "        if not os.path.exists(self.root+'train/embedding'):\n",
    "            os.mkdir(self.root+'train/embedding')\n",
    "        from prepare_data import get_sequences\n",
    "\n",
    "        def trans_to_sequences(ast):\n",
    "            sequence = []\n",
    "            get_sequences(ast, sequence)\n",
    "            return sequence\n",
    "        corpus = trees['code'].apply(trans_to_sequences)  # every row of corpus is list of str\n",
    "        str_corpus = [' '.join(c) for c in corpus]  # str_corpus is list of str\n",
    "        trees['code'] = pd.Series(str_corpus)  \n",
    "        # the first saveral ones are alway the same in every row, can we delete them ?\n",
    "        trees.to_csv(self.root+'train/programs_ns.tsv')\n",
    "\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, min_count=3)\n",
    "        w2v.save(self.root+'train/embedding/node_w2v_' + str(size))\n",
    "\n",
    "    # generate block sequences with index representations\n",
    "    def generate_block_seqs(self,data_path,part):\n",
    "        from prepare_data import get_blocks as func\n",
    "        from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "        word2vec = Word2Vec.load(self.root+'train/embedding/node_w2v_' + str(self.size)).wv\n",
    "        vocab = word2vec.vocab\n",
    "        max_token = word2vec.vectors.shape[0]\n",
    "        # Attribute `syn0` will be removed in 4.0.0, use self.vectors instead\n",
    "\n",
    "        def tree_to_index(node):\n",
    "            token = node.token\n",
    "            result = [vocab[token].index if token in vocab else max_token]\n",
    "            children = node.children\n",
    "            for child in children:\n",
    "                result.append(tree_to_index(child))\n",
    "            return result\n",
    "\n",
    "        def trans2seq(r):\n",
    "            blocks = []\n",
    "            func(r, blocks)\n",
    "            tree = []\n",
    "            for b in blocks:\n",
    "                btree = tree_to_index(b)\n",
    "                tree.append(btree)\n",
    "            return tree\n",
    "        trees = pd.read_pickle(data_path)\n",
    "        trees['code'] = trees['code'].apply(trans2seq)\n",
    "        trees.to_pickle(self.root+part+'/blocks.pkl')\n",
    "\n",
    "    # run for processing data to train\n",
    "    def run(self):\n",
    "        print('parse source code...')\n",
    "        self.parse_source(output_file='ast.pkl',option='existing')\n",
    "        print('split data...')\n",
    "        self.split_data()\n",
    "        print('train word embedding...')\n",
    "        self.dictionary_and_embedding(None,128)\n",
    "        print('generate block sequences...')\n",
    "        self.generate_block_seqs(self.train_file_path, 'train')\n",
    "        self.generate_block_seqs(self.dev_file_path, 'dev')\n",
    "        self.generate_block_seqs(self.test_file_path, 'test')\n",
    "\n",
    "\n",
    "ppl = Pipeline('3:1:1', 'data/')\n",
    "ppl.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
